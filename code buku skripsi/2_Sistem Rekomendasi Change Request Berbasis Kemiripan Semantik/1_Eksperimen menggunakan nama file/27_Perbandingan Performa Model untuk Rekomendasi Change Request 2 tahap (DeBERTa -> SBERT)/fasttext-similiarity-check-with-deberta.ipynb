{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10981401,"sourceType":"datasetVersion","datasetId":6834080},{"sourceId":351367,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":293215,"modelId":313850}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport argparse\nimport os\nimport json\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom scipy.spatial.distance import jaccard # Although jaccard similarity calculation is different for embeddings\nfrom transformers import DebertaTokenizer, DebertaModel\nfrom torch import nn\nimport fasttext\nimport fasttext.util\nimport re\nfrom typing import List, Dict, Any, Tuple\nimport urllib.request\nimport sys\nimport gzip\nimport shutil\nfrom scipy.spatial.distance import jaccard # Although jaccard similarity calculation is different for embeddings\n\nclass DeBERTaClassifier(nn.Module):\n    \"\"\"\n    A classifier model based on DeBERTa for multi-label classification.\n    \n    This model uses a pre-trained DeBERTa model as the encoder and adds a \n    classification head on top with sigmoid activation for multi-label output.\n    \n    Args:\n        num_labels (int): Number of classes in the multi-label classification task.\n    \"\"\"\n    def __init__(self, num_labels):\n        super().__init__()\n        self.deberta = DebertaModel.from_pretrained('microsoft/deberta-base')\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(768, num_labels) # 768 is the hidden size for deberta-base\n        # Freeze all parameters in DeBERTa\n        for param in self.deberta.parameters():\n            param.requires_grad = False\n        # Unfreeze encoder parameters for fine-tuning\n        # Note: DeBERTa has a different architecture than BERT/RoBERTa\n        # We'll unfreeze the last 3 encoder layers\n        for layer in self.deberta.encoder.layer[-3:]:\n            for param in layer.parameters():\n                param.requires_grad = True\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        # Unlike BERT, DeBERTa doesn't have a pooler, so we need to take the last hidden state\n        # and either use the [CLS] token (first token) or do mean pooling\n        # Here we'll use the [CLS] token (first token) representation\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        cls_output = self.dropout(cls_output)\n        # Return raw logits for BCEWithLogitsLoss (sigmoid will be applied in the loss function)\n        return self.classifier(cls_output)\n\nclass DeBERTaIssueDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_len):\n        self.texts = texts.tolist() if isinstance(texts, pd.Series) else texts # Ensure texts is a list\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten()\n        }\n\ndef prepare_data(df, text_column='all_text', min_label_freq=0, max_label_len=100):\n    if text_column in df.columns:\n        df = df[[text_column, 'labels']]\n        df = df[~df[text_column].apply(lambda x: x.startswith('nan') if isinstance(x, str) else False)]\n    else:\n        raise ValueError(f\"Text column '{text_column}' not found in the DataFrame\")\n    \n    df = df.dropna()\n    texts = df[text_column]\n    labels = df['labels'].apply(lambda x: x if isinstance(x, list) else [])\n\n    label_distribution = Counter([label for labels in labels for label in labels])\n    frequent_labels = [label for label, count in label_distribution.items() if count >= min_label_freq]\n    \n    filtered_labels = labels.apply(lambda x: [label for label in x if label in frequent_labels])\n    label_length = filtered_labels.apply(len)\n    length_mask = (label_length > 0) & (label_length <= max_label_len)\n    \n    texts = texts[length_mask].reset_index(drop=True)\n    filtered_labels = filtered_labels[length_mask].reset_index(drop=True)\n    \n    return texts, filtered_labels\n\ndef preprocess_text(text: str) -> str:\n    \"\"\"\n    Preprocess text for FastText embedding.\n    \"\"\"\n    if not isinstance(text, str): # Handle potential non-string inputs\n        text = str(text)\n    # Convert to lowercase\n    text = text.lower()\n    # Replace newlines and tabs with spaces\n    text = re.sub(r'[\\n\\t]', ' ', text)\n    # Remove special characters but keep letters, numbers and spaces\n    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ndef get_text_embedding(text: str, model: fasttext.FastText._FastText) -> np.ndarray:\n    \"\"\"\n    Get the sentence embedding for a text using the trained FastText model.\n    Uses get_sentence_vector which is suitable after supervised training.\n    \"\"\"\n    processed_text = preprocess_text(text)\n    # If text is empty after preprocessing, return zero vector\n    if not processed_text:\n        return np.zeros(model.get_dimension())\n    return model.get_sentence_vector(processed_text)\n\ndef get_embeddings(texts: pd.Series, model: fasttext.FastText._FastText) -> np.ndarray:\n    \"\"\"\n    Get embeddings for a list of texts using the loaded FastText model.\n    Uses get_sentence_vector for each text.\n    \"\"\"\n    embeddings = []\n    # Note: fasttext.get_sentence_vector doesn't benefit from explicit batching like transformers.\n    # We process text by text.\n    for i in tqdm(range(0, len(texts)), desc=\"Generating FastText embeddings\"):\n        embedding = get_text_embedding(str(texts.iloc[i]), model)\n        embeddings.append(embedding)\n\n    return np.array(embeddings).astype(np.float32) # Ensure float32 for similarity calculations\n\ndef download_with_progress(url: str, output_path: str) -> None:\n    \"\"\"\n    Download a file with a tqdm progress bar.\n    \"\"\"\n    try:\n        print(f\"Downloading from {url}\")\n        # Get content length from URL\n        u = urllib.request.urlopen(url)\n        total_size = int(u.info().get('Content-Length', 0))\n\n        # Set up the tqdm progress bar\n        with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading FastText Model\") as pbar:\n            def report_hook(count, block_size, total_size):\n                pbar.update(block_size)\n\n            # Download the file\n            urllib.request.urlretrieve(url, output_path, reporthook=report_hook)\n    except Exception as e:\n        print(f\"Error during download: {e}\")\n        # Try direct download without progress in case of error\n        print(\"Attempting direct download...\")\n        urllib.request.urlretrieve(url, output_path)\n\ndef ensure_pretrained_model_downloaded(model_name: str = 'cc.en.300.bin') -> str:\n    \"\"\"\n    Ensures the FastText pre-trained model (.bin file) is downloaded and returns its path.\n    Uses tqdm for download progress.\n    \"\"\"\n    home_dir = os.path.expanduser(\"~\")\n    fasttext_dir = os.path.join(home_dir, \".fasttext\")\n    local_model_path = os.path.join(os.getcwd(), model_name)\n    default_model_path = os.path.join(fasttext_dir, model_name)\n\n    # Check various locations for the .bin file\n    if os.path.exists(local_model_path):\n        print(f\"Found pre-trained FastText model at {local_model_path}\")\n        return local_model_path\n    elif os.path.exists(default_model_path):\n        print(f\"Found pre-trained FastText model at {default_model_path}\")\n        return default_model_path\n    else:\n        print(f\"Pre-trained FastText model ({model_name}) not found, downloading...\")\n\n        # Ensure the FastText directory exists\n        os.makedirs(fasttext_dir, exist_ok=True)\n\n        # FastText download URL for English model (.bin.gz)\n        # Assuming 'cc.en.300.bin.gz' for the 'en' model.\n        gz_file_name = f\"{model_name}.gz\"\n        url = f\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/{gz_file_name}\"\n        gz_path = os.path.join(fasttext_dir, gz_file_name)\n\n        # Download with progress bar\n        try:\n            download_with_progress(url, gz_path)\n        except Exception as e:\n            print(f\"Error during download with progress: {e}\")\n            print(\"Could not download the pre-trained model.\")\n            sys.exit(1) # Exit if download fails\n\n        # Extract the gz file\n        print(f\"Extracting model file to {default_model_path}...\")\n        try:\n            with gzip.open(gz_path, 'rb') as f_in:\n                with open(default_model_path, 'wb') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n        except Exception as e:\n            print(f\"Error during extraction: {e}\")\n            if os.path.exists(gz_path):\n                os.remove(gz_path) # Clean up downloaded archive on extraction error\n            sys.exit(1)\n\n        # Remove the gz file\n        os.remove(gz_path)\n        print(f\"Model extracted to {default_model_path}\")\n\n        # Try to create a symlink in current directory for convenience\n        try:\n            if not os.path.exists(local_model_path):\n                os.symlink(default_model_path, local_model_path)\n                print(f\"Created symlink to model in current directory\")\n        except (OSError, AttributeError) as e:\n            print(f\"Note: Could not create symlink in current directory: {e}\")\n\n        return default_model_path\n\ndef calculate_similarities(test_embeddings, train_embeddings, similarity_metric='cosine'):\n    \"\"\"\n    Calculate similarities between test and train embeddings using different metrics.\n    \n    Args:\n        test_embeddings: numpy array of shape (n_test, embedding_dim)\n        train_embeddings: numpy array of shape (n_train, embedding_dim)\n        similarity_metric: one of 'cosine', 'euclidean', 'jaccard'\n        \n    Returns:\n        similarities: numpy array of shape (n_test, n_train)\n    \"\"\"\n    if similarity_metric == 'cosine':\n        return cosine_similarity(test_embeddings, train_embeddings)\n    \n    elif similarity_metric == 'euclidean':\n        # Convert distances to similarities (higher is more similar)\n        distances = euclidean_distances(test_embeddings, train_embeddings)\n        max_dist = np.max(distances)\n        if max_dist == 0:\n            return np.ones_like(distances)\n        return 1 - (distances / max_dist)\n    \n    elif similarity_metric == 'jaccard':\n        # Jaccard similarity for continuous embeddings is not standard.\n        # The version from 2_a binarized based on median. We'll replicate that here.\n        # Be aware this is an approximation for Jaccard on continuous data.\n        # We'll use the median of each dimension as a threshold\n        binary_test = test_embeddings > np.median(test_embeddings, axis=0)\n        binary_train = train_embeddings > np.median(train_embeddings, axis=0)\n        \n        similarities = np.zeros((len(test_embeddings), len(train_embeddings)))\n        \n        for i in range(len(test_embeddings)):\n            for j in range(len(train_embeddings)):\n                intersection = np.logical_and(binary_test[i], binary_train[j]).sum()\n                union = np.logical_or(binary_test[i], binary_train[j]).sum()\n                similarities[i, j] = intersection / union if union > 0 else 0\n                \n        return similarities\n    \n    else:\n        raise ValueError(f\"Unknown similarity metric: {similarity_metric}\")\n\ndef calculate_label_based_metrics(test_labels, train_labels, similar_indices, k_values=[1, 3, 5]):\n    \"\"\"\n    Calculate precision@k, recall@k, F1@k and other metrics based on label matching.\n    \n    Args:\n        test_labels: list of label lists\n        train_labels: list of label lists\n        similar_indices: list of arrays containing indices of similar items\n        k_values: list of k values for metrics\n    \"\"\"\n    metrics = {}\n    for k in k_values:\n        metrics.update({\n            f'precision@{k}': [],\n            f'recall@{k}': [],\n            f'f1@{k}': []\n        })\n    \n    metrics.update({\n        'avg_label_overlap': [],\n        'total_matches': 0,\n        'total_test_samples': len(test_labels)\n    })\n    \n    for i, test_label_set in enumerate(test_labels):\n        test_labels_set = set(test_label_set)\n        if not test_labels_set:  # Skip empty test label sets\n            continue\n            \n        retrieved_indices = similar_indices[i]\n        \n        for k in k_values:\n            # Consider only top-k results\n            top_k_indices = retrieved_indices[:k]\n            \n            # Calculate true positives across all top-k recommendations\n            retrieved_labels_set = set()\n            for idx in top_k_indices:\n                retrieved_labels_set.update(train_labels[idx])\n            \n            true_positives = len(test_labels_set.intersection(retrieved_labels_set))\n            \n            # Calculate metrics\n            precision = true_positives / len(retrieved_labels_set) if retrieved_labels_set else 0\n            recall = true_positives / len(test_labels_set) if test_labels_set else 0\n            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n            \n            metrics[f'precision@{k}'].append(precision)\n            metrics[f'recall@{k}'].append(recall)\n            metrics[f'f1@{k}'].append(f1)\n        \n        # Calculate label overlap for all top-k results\n        label_overlaps = []\n        for j, idx in enumerate(retrieved_indices):\n            train_labels_set = set(train_labels[idx])\n            if test_labels_set & train_labels_set:  # If there's an intersection\n                overlap = len(test_labels_set & train_labels_set) / len(test_labels_set | train_labels_set)\n                label_overlaps.append(overlap)\n                \n                # Count if there's at least one match\n                if j == 0:  # Only count once per test sample\n                    metrics['total_matches'] += 1\n        \n        avg_overlap = np.mean(label_overlaps) if label_overlaps else 0\n        metrics['avg_label_overlap'].append(avg_overlap)\n    \n    # Calculate averages\n    for k in k_values:\n        metrics[f'avg_precision@{k}'] = np.mean(metrics[f'precision@{k}'])\n        metrics[f'avg_recall@{k}'] = np.mean(metrics[f'recall@{k}'])\n        metrics[f'avg_f1@{k}'] = np.mean(metrics[f'f1@{k}'])\n    \n    metrics['avg_label_overlap'] = np.mean(metrics['avg_label_overlap'])\n    metrics['match_rate'] = metrics['total_matches'] / metrics['total_test_samples']\n    \n    return metrics\n\ndef create_stratification_labels(labels_list, min_samples_per_label=2):\n    \"\"\"\n    Create stratification labels that ensure each label has enough samples.\n    Only considers labels that appear frequently enough for stratification.\n    \"\"\"\n    # Count label occurrences\n    label_counts = Counter([label for labels in labels_list for label in labels])\n    \n    # Keep only labels that appear frequently enough\n    frequent_labels = {label for label, count in label_counts.items() if count >= min_samples_per_label}\n    \n    # Create binary indicators only for frequent labels\n    stratification_indicators = []\n    for labels in labels_list:\n        # Create indicator only for frequent labels\n        indicator = tuple(sorted(label for label in labels if label in frequent_labels))\n        # If no frequent labels, use a special category\n        if not indicator:\n            indicator = ('rare_combination',)\n        stratification_indicators.append(indicator)\n    \n    return stratification_indicators\n\ndef calculate_and_evaluate_similarity(test_embeddings, filtered_train_embeddings, test_labels, original_train_labels, original_index_map, k_values, run_dir):\n    \"\"\"\n    Calculates similarities between test and filtered train embeddings,\n    maps indices back to original, evaluates metrics, and saves plots.\n    \n    Args:\n        test_embeddings: numpy array (n_test, dim)\n        filtered_train_embeddings: numpy array (n_filtered_train, dim)\n        test_labels: list of lists (ground truth labels for test set)\n        original_train_labels: list of lists (ground truth labels for the *original* full training set)\n        original_index_map: dict mapping filtered train index -> original train index\n        k_values: list of k values for evaluation\n        run_dir: directory to save plots and results\n\n    Returns:\n        tuple: (results_dict, all_similar_indices_mapped, all_similarity_scores_mapped)\n            results_dict: Dictionary containing aggregated metrics for each similarity type.\n            all_similar_indices_mapped: Dict mapping metric -> list of lists of *original* train indices.\n            all_similarity_scores_mapped: Dict mapping metric -> list of lists of similarity scores.\n    \"\"\"\n    similarity_metrics = ['cosine', 'euclidean', 'jaccard']\n    results = {} # Store aggregated metrics per similarity type\n    all_similar_indices_mapped = {} # Store top-k original indices per metric\n    all_similarity_scores_mapped = {} # Store corresponding top-k scores per metric\n    max_k = max(k_values)\n\n    if filtered_train_embeddings.shape[0] == 0:\n        print(\"Warning: No filtered training embeddings to compare against. Skipping similarity calculation and evaluation.\")\n        # Return empty results\n        for metric in similarity_metrics:\n            results[metric] = {f'avg_precision@{k}': 0 for k in k_values}\n            results[metric].update({\n                f'avg_recall@{k}': 0 for k in k_values\n            })\n            results[metric].update({\n                f'avg_f1@{k}': 0 for k in k_values\n            })\n            results[metric]['match_rate'] = 0\n            results[metric]['avg_label_overlap'] = 0\n            all_similar_indices_mapped[metric] = [[] for _ in range(len(test_embeddings))]\n            all_similarity_scores_mapped[metric] = [[] for _ in range(len(test_embeddings))]\n        return results, all_similar_indices_mapped, all_similarity_scores_mapped\n\n    for metric in similarity_metrics:\n        print(f\"\\nCalculating {metric} similarities...\")\n        # Calculate full similarity matrix between test and the filtered training set\n        similarities = calculate_similarities(\n            test_embeddings, filtered_train_embeddings, metric\n        )\n        \n        # Get top-k indices and scores *within the filtered set*\n        # Argsort gives indices of the smallest values first, use [::-1] for descending order\n        num_filtered_train = filtered_train_embeddings.shape[0]\n        current_max_k = min(max_k, num_filtered_train)\n\n        # Get indices of top k scores for each test sample\n        top_k_indices_filtered = np.argsort(similarities, axis=1)[:, -current_max_k:][:, ::-1] \n        # Get the actual scores corresponding to these indices\n        top_k_scores = np.array([similarities[i, top_k_indices_filtered[i]] for i in range(len(test_embeddings))])\n\n        # Map filtered indices back to original training set indices\n        similar_original_indices_list = []\n        for filtered_indices_row in top_k_indices_filtered:\n            original_indices_row = [original_index_map[filt_idx] for filt_idx in filtered_indices_row]\n            similar_original_indices_list.append(original_indices_row)\n            \n        # Store mapped indices and scores for this metric\n        all_similar_indices_mapped[metric] = similar_original_indices_list\n        all_similarity_scores_mapped[metric] = top_k_scores.tolist() # Store as list\n\n        # Evaluate using original labels\n        print(f\"Evaluating metrics for {metric} similarity...\")\n        metrics = calculate_label_based_metrics(\n            test_labels, \n            original_train_labels, # Pass the original full set of train labels\n            similar_original_indices_list, # Pass the list of lists of mapped original indices\n            k_values\n        )\n\n        # Store aggregated metrics\n        results[metric] = metrics\n\n    # Create comparison plots (similar to before)\n    print(\"\\nGenerating comparison plots...\")\n    for k in k_values:\n        plt.figure(figsize=(12, 8))\n        metrics_to_plot = [f'avg_precision@{k}', f'avg_recall@{k}', f'avg_f1@{k}']\n        x = np.arange(len(metrics_to_plot))\n        width = 0.25\n\n        for i, metric in enumerate(similarity_metrics):\n            values = [results[metric].get(m, 0) for m in metrics_to_plot] # Use .get for safety if metric calculation failed\n            plt.bar(x + i*width, values, width, label=f'{metric.capitalize()} Similarity')\n\n        plt.ylabel('Score')\n        plt.title(f'Comparison of Similarity Metrics at k={k} (Filtered Train Set)')\n        plt.xticks(x + width, [m.split('@')[0].replace(\"avg_\", \"\").capitalize() for m in metrics_to_plot])\n        plt.legend()\n        plt.grid(True, axis='y')\n        plot_path = os.path.join(run_dir, f'similarity_metrics_comparison_k{k}_filtered_train.png')\n        plt.savefig(plot_path)\n        print(f\"Saved comparison plot to {plot_path}\")\n        plt.close()\n\n    # Save comparison results dictionary\n    comparison_results_path = os.path.join(run_dir, 'similarity_metrics_comparison_filtered_train.json')\n    with open(comparison_results_path, 'w') as f:\n        json.dump(results, f, indent=4)\n    print(f\"Saved comparison metrics to {comparison_results_path}\")\n\n    return results, all_similar_indices_mapped, all_similarity_scores_mapped\n\ndef predict_labels_with_deberta(texts, model, tokenizer, mlb_deberta, device, batch_size, max_length, threshold=0.5):\n    \"\"\"\n    Predict multi-labels for given texts using a trained DeBERTa model.\n\n    Args:\n        texts (pd.Series): Series of input texts.\n        model (nn.Module): Trained DeBERTaClassifier model.\n        tokenizer (transformers.PreTrainedTokenizer): DeBERTa tokenizer.\n        mlb_deberta (MultiLabelBinarizer): Fitted MultiLabelBinarizer corresponding to the DeBERTa model's labels.\n        device: Device to perform inference on.\n        batch_size (int): Batch size for prediction.\n        max_length (int): Maximum token length for tokenizer.\n        threshold (float): Threshold for converting sigmoid outputs to binary labels.\n\n    Returns:\n        list[list[str]]: List of predicted label lists for each input text.\n    \"\"\"\n    model.eval() # Set model to evaluation mode\n    dataset = DeBERTaIssueDataset(texts, tokenizer, max_length)\n    loader = DataLoader(dataset, batch_size=batch_size)\n\n    all_preds_binary = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Predicting labels with DeBERTa\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n\n            outputs = model(input_ids, attention_mask)\n            # Apply sigmoid and threshold\n            predictions_binary = (torch.sigmoid(outputs) >= threshold).cpu().numpy()\n            all_preds_binary.append(predictions_binary)\n\n    all_preds_binary = np.vstack(all_preds_binary)\n\n    # Convert binary predictions back to label names\n    predicted_labels = mlb_deberta.inverse_transform(all_preds_binary)\n    # Convert tuples back to lists\n    predicted_labels_list = [list(labels) for labels in predicted_labels]\n\n    return predicted_labels_list\n\ndef main(args):\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n    \n    np.random.seed(42)\n    torch.manual_seed(42)\n    \n    results_dir = args.results_dir\n    os.makedirs(results_dir, exist_ok=True)\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_dir = os.path.join(results_dir, f\"run_{timestamp}_{args.text_column}\")\n    os.makedirs(run_dir, exist_ok=True)\n    \n    print(f\"Loading data from {args.data_path}...\")\n    df = pd.read_json(args.data_path)\n    \n    if args.text_column not in df.columns:\n        available_columns = [col for col in df.columns if col.startswith('all_text')]\n        print(f\"Text column '{args.text_column}' not found. Available text columns: {available_columns}\")\n        if len(available_columns) == 0:\n            raise ValueError(\"No text columns found in the data\")\n        args.text_column = available_columns[0]\n        print(f\"Using '{args.text_column}' instead\")\n    \n    texts, filtered_labels_all = prepare_data(\n        df, \n        text_column=args.text_column,\n        min_label_freq=args.min_label_freq, \n        max_label_len=args.max_label_len\n    )\n    \n    # Create stratification indicators *before* split\n    print(\"\\nPreparing stratified split...\")\n    stratification_indicators = create_stratification_labels(filtered_labels_all)\n    \n    try:\n        # Try stratified split and convert to lists\n        train_indices, test_indices = train_test_split(\n            range(len(texts)),\n            test_size=0.1,\n            random_state=42,\n            stratify=stratification_indicators\n        )\n        \n        # Use indices to split both texts and labels\n        original_train_texts = texts.iloc[train_indices].reset_index(drop=True) # Keep original training data\n        test_texts = texts.iloc[test_indices].reset_index(drop=True)\n        original_train_labels = [filtered_labels_all[i] for i in train_indices] # Keep original training labels\n        test_labels = [filtered_labels_all[i] for i in test_indices]\n        \n        print(\"Successfully performed stratified split\")\n    except ValueError as e:\n        print(f\"Warning: Could not perform stratified split ({str(e)})\")\n        print(\"Falling back to random split\")\n        \n        # Random split with indices\n        train_indices, test_indices = train_test_split(\n            range(len(texts)),\n            test_size=0.1,\n            random_state=42\n        )\n        \n        # Use indices to split both texts and labels\n        original_train_texts = texts.iloc[train_indices].reset_index(drop=True) # Keep original training data\n        test_texts = texts.iloc[test_indices].reset_index(drop=True)\n        original_train_labels = [filtered_labels_all[i] for i in train_indices] # Keep original training labels\n        test_labels = [filtered_labels_all[i] for i in test_indices]\n\n    print(f\"\\nOriginal Training Samples: {len(original_train_texts)}\")\n    print(f\"Test Samples: {len(test_texts)}\")\n\n    # --- DeBERTa Prediction FIRST ---\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"\\nUsing device: {device}\")\n\n    print(\"\\nLoading DeBERTa model and resources for prediction...\")\n    deberta_tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n\n    with open(args.deberta_label_encoder_path, 'r') as f:\n        encoder_data = json.load(f)\n    mlb_deberta = MultiLabelBinarizer()\n    mlb_deberta.classes_ = np.array(encoder_data['classes'])\n    num_deberta_labels = len(mlb_deberta.classes_)\n\n    selected_deberta_labels = None\n    if args.deberta_selected_labels_path:\n        print(f\"Loading selected labels from {args.deberta_selected_labels_path}\")\n        with open(args.deberta_selected_labels_path, 'r') as f:\n            selected_data = json.load(f)\n        selected_deberta_labels = selected_data['selected_labels']\n        mlb_deberta.classes_ = np.array([lbl for lbl in selected_deberta_labels if lbl in mlb_deberta.classes_])\n        num_deberta_labels = len(mlb_deberta.classes_)\n        print(f\"Using {num_deberta_labels} selected labels for DeBERTa prediction.\")\n    else:\n        print(f\"Using all {num_deberta_labels} labels from the DeBERTa encoder.\")\n\n    deberta_model = DeBERTaClassifier(num_labels=num_deberta_labels).to(device)\n    deberta_model.load_state_dict(torch.load(args.deberta_model_path, map_location=device))\n    deberta_model.eval() \n\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs for DeBERTa prediction!\")\n        deberta_model = torch.nn.DataParallel(deberta_model)\n\n    print(\"\\nPredicting labels for the test set using DeBERTa...\")\n    predicted_test_labels = predict_labels_with_deberta(\n        texts=test_texts, \n        model=deberta_model,\n        tokenizer=deberta_tokenizer,\n        mlb_deberta=mlb_deberta,\n        device=device,\n        batch_size=args.batch_size,\n        max_length=512,\n        threshold=args.deberta_threshold\n    )\n    print(f\"Finished predicting labels for {len(predicted_test_labels)} test samples.\")\n    \n    # Cleanup DeBERTa model to free memory if possible\n    del deberta_model\n    del deberta_tokenizer\n    del mlb_deberta\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    # --- End DeBERTa Prediction ---\n\n    # --- Filter Training Data based on ALL Predicted Test Labels ---\n    print(\"\\nFiltering training data based on globally predicted test labels...\")\n    all_predicted_labels_set = set(label for labels in predicted_test_labels for label in labels)\n    print(f\"Total unique labels predicted across test set: {len(all_predicted_labels_set)}\")\n\n    filtered_train_indices_original = [] # Store original indices\n    filtered_train_texts_list = []\n    filtered_train_labels_list = []\n\n    for i, (text, labels) in enumerate(zip(original_train_texts, original_train_labels)):\n        if any(label in all_predicted_labels_set for label in labels):\n            filtered_train_indices_original.append(i) # Store original index\n            filtered_train_texts_list.append(text)\n            filtered_train_labels_list.append(labels)\n\n    # Create filtered Series/lists\n    filtered_train_texts = pd.Series(filtered_train_texts_list)\n    filtered_train_labels = filtered_train_labels_list # Already a list\n    \n    # Map from filtered index (0 to N-1) back to original index\n    original_index_map = {filtered_idx: original_idx for filtered_idx, original_idx in enumerate(filtered_train_indices_original)}\n\n    print(f\"Filtered Training Samples: {len(filtered_train_texts)}\")\n    if len(filtered_train_texts) == 0:\n        print(\"Warning: Filtering resulted in zero training samples. Similarity search will be skipped.\")\n        # Optionally exit or handle this case appropriately\n        return {} \n    # --- End Training Data Filtering ---\n\n    # --- Initialize and Prepare FastText Model ---\n    print(\"Loading/Downloading FastText model...\")\n    fasttext_model_path = None\n    if args.fasttext_model_path:\n        if os.path.exists(args.fasttext_model_path):\n            print(f\"Loading pre-existing FastText model from: {args.fasttext_model_path}\")\n            fasttext_model_path = args.fasttext_model_path\n        else:\n            print(f\"Warning: Specified FastText model path '{args.fasttext_model_path}' not found.\")\n            print(\"Falling back to downloading the standard English model.\")\n\n    if not fasttext_model_path:\n        # Training skipped (or path not provided/found), download standard model\n        print(\"Downloading standard pre-trained English model (cc.en.300.bin) with progress...\")\n        try:\n            fasttext_model_path = ensure_pretrained_model_downloaded('cc.en.300.bin')\n            print(f\"Using downloaded model: {fasttext_model_path}\")\n        except Exception as e: # Catch potential errors from download/extraction\n            print(f\"Error ensuring standard FastText model is available: {e}\")\n            print(\"Please check your internet connection or try providing a model path using --fasttext_model_path.\")\n            sys.exit(1) # Exit if download fails\n\n    print(f\"Loading FastText model from {fasttext_model_path}...\")\n    try:\n        model = fasttext.load_model(fasttext_model_path)\n        loaded_model_dim = model.get_dimension()\n        print(f\"FastText model loaded. Dimension: {loaded_model_dim}\")\n    except ValueError as e:\n         print(f\"Error loading FastText model: {e}\")\n         print(\"Ensure the path points to a valid FastText .bin file.\")\n         sys.exit(1)\n    # --- End FastText Model Prep ---\n\n    # --- Generate Embeddings (Test and Filtered Train) ---\n    print(\"Generating embeddings with loaded FastText model...\")\n    # Embed the original test texts\n    test_embeddings = get_embeddings(test_texts, model)\n    # Embed ONLY the filtered training texts\n    filtered_train_embeddings = get_embeddings(filtered_train_texts, model)\n    print(f\"Generated {test_embeddings.shape[0]} test embeddings.\")\n    print(f\"Generated {filtered_train_embeddings.shape[0]} filtered training embeddings.\")\n    # --- End Embedding Generation ---\n\n    # --- Compare Similarity Metrics (using filtered embeddings) ---\n    print(\"Calculating similarities and evaluating metrics...\")\n    k_values = [1, 3, 5, 10] \n    \n    # Call the modified comparison function (needs implementation below)\n    # Pass original_train_labels and the original_index_map\n    similarity_comparison_results, all_similar_indices_mapped, all_similarity_scores_mapped = calculate_and_evaluate_similarity(\n        test_embeddings, filtered_train_embeddings, \n        test_labels, original_train_labels, # Pass ORIGINAL train labels for evaluation\n        original_index_map, # Pass the map\n        k_values, run_dir\n    )\n\n    # --- Save Detailed Results ---\n    all_similarity_details = {}\n    similarity_metrics = ['cosine', 'euclidean', 'jaccard'] \n\n    for metric in similarity_metrics:\n        print(f\"Processing results for {metric} similarity...\")\n        # Retrieve pre-calculated *mapped* indices and scores\n        # Note: calculate_and_evaluate_similarity should return indices mapped back to original\n        similar_original_indices = all_similar_indices_mapped[metric] \n        similarity_scores = all_similarity_scores_mapped[metric] # These scores correspond to the mapped indices\n\n        # Metrics are already calculated in calculate_and_evaluate_similarity\n        label_metrics = similarity_comparison_results[metric]\n\n        # Print metrics \n        print(f\"{metric.capitalize()} Similarity Metrics (using filtered train set):\")\n        print(f\"Match Rate (at least one match): {label_metrics.get('match_rate', 0.0):.4f}\") # Use .get for safety\n        for k in k_values:\n            print(f\"Average Precision@{k}: {label_metrics.get(f'avg_precision@{k}', 0.0):.4f}\")\n            print(f\"Average Recall@{k}: {label_metrics.get(f'avg_recall@{k}', 0.0):.4f}\")\n            print(f\"Average F1@{k}: {label_metrics.get(f'avg_f1@{k}', 0.0):.4f}\")\n        print(f\"Average Label Overlap: {label_metrics.get('avg_label_overlap', 0.0):.4f}\")\n\n        # Generate and save detailed similarity results per test sample\n        metric_similarity_results = []\n        # Use args.top_k for the detailed results file length\n        for i, (original_indices_for_test, scores_for_test) in enumerate(zip(similar_original_indices, similarity_scores)):\n            test_sample = {\n                'test_text': test_texts.iloc[i],\n                'test_labels': test_labels[i],\n                'predicted_test_labels': predicted_test_labels[i], # Add predicted labels for context\n                'similar_requests': []\n            }\n            test_labels_set = set(test_labels[i])\n            \n            # Ensure we don't try to access more items than available\n            num_retrieved = min(args.top_k, len(original_indices_for_test))\n\n            for j in range(num_retrieved):\n                original_idx = original_indices_for_test[j]\n                score = scores_for_test[j]\n                \n                # Use original_idx to fetch from original training data\n                train_text = original_train_texts.iloc[original_idx]\n                train_label_list = original_train_labels[original_idx] \n                train_labels_set = set(train_label_list)\n                \n                matching_labels = list(test_labels_set & train_labels_set)\n                similar_request = {\n                    'rank': j + 1,\n                    'original_train_index': int(original_idx), # Store original index\n                    'text': train_text,\n                    'labels': train_label_list,\n                    'similarity_score': float(score),\n                    'matching_labels': matching_labels,\n                    'has_matching_label': len(matching_labels) > 0\n                }\n                test_sample['similar_requests'].append(similar_request)\n            metric_similarity_results.append(test_sample)\n\n        all_similarity_details[metric] = metric_similarity_results\n        # Use a NpEncoder for saving JSON to handle potential numpy types\n        class NpEncoder(json.JSONEncoder):\n            def default(self, obj):\n                if isinstance(obj, np.integer): return int(obj)\n                elif isinstance(obj, np.floating): return float(obj)\n                elif isinstance(obj, np.ndarray): return obj.tolist()\n                return super(NpEncoder, self).default(obj)\n\n        with open(os.path.join(run_dir, f'{metric}_similarity_results_filtered_train.json'), 'w') as f:\n            try:\n                json.dump(metric_similarity_results, f, indent=4, cls=NpEncoder)\n            except TypeError as e:\n                print(f\"Error saving JSON for {metric}: {e}. Check data types.\")\n                # Fallback or simplified saving if needed\n                # json.dump([str(item) for item in metric_similarity_results], f, indent=4) \n\n    # Prepare combined results dictionary\n    results = {\n        'text_column': args.text_column,\n        'filtering_info': {\n            'total_unique_predicted_labels': len(all_predicted_labels_set),\n            'original_training_samples': len(original_train_texts),\n            'filtered_training_samples': len(filtered_train_texts),\n        },\n        'similarity_comparison': similarity_comparison_results, # Metrics calculated by calculate_and_evaluate_similarity\n        'fasttext_info': {\n            'model_path': fasttext_model_path,\n        },\n    }\n\n    with open(os.path.join(run_dir, 'all_metrics_results_filtered_train.json'), 'w') as f:\n        def convert_numpy(obj):\n            if isinstance(obj, np.integer): return int(obj)\n            elif isinstance(obj, np.floating): return float(obj)\n            elif isinstance(obj, np.ndarray): return obj.tolist()\n            elif isinstance(obj, dict): return {k: convert_numpy(v) for k, v in obj.items()}\n            elif isinstance(obj, list): return [convert_numpy(i) for i in obj]\n            return obj\n        results_serializable = convert_numpy(results)\n        json.dump(results_serializable, f, indent=4)\n\n    print(f\"\\nAnalysis completed! Results saved to {run_dir}\")\n\n    # Return relevant data if needed downstream (adjust as necessary)\n    return {\n        'model': model,\n        'test_embeddings': test_embeddings,\n        'filtered_train_embeddings': filtered_train_embeddings,\n        'original_index_map': original_index_map,\n        'all_similar_indices_mapped': all_similar_indices_mapped,\n        'all_similarity_scores_mapped': all_similarity_scores_mapped,\n        'similarity_details': all_similarity_details,\n        'results_dir': run_dir\n    }","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T15:29:47.157670Z","iopub.execute_input":"2025-04-22T15:29:47.158144Z","iopub.status.idle":"2025-04-22T15:29:47.239496Z","shell.execute_reply.started":"2025-04-22T15:29:47.158116Z","shell.execute_reply":"2025-04-22T15:29:47.238752Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Compare change requests using SBERT embeddings and similarity metrics, with optional DeBERTa label filtering.')\n    \n    parser.add_argument('--data_path', type=str, \n                        default=\"/kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json\",\n                        help='Path to the JSON data file')\n    parser.add_argument('--text_column', type=str, default='all_text_0.5',\n                        help='Column name with the text data to use for training')\n    parser.add_argument('--results_dir', type=str, default='./results',\n                        help='Directory to save results')\n    \n    parser.add_argument('--min_label_freq', type=int, default=5,\n                        help='Minimum frequency for a label to be considered')\n    parser.add_argument('--max_label_len', type=int, default=5,\n                        help='Maximum number of labels per sample')\n    \n    parser.add_argument('--batch_size', type=int, default=32,\n                        help='Batch size for DeBERTa prediction (FastText embedding is not batched)')\n    parser.add_argument('--top_k', type=int, default=5,\n                        help='Number of similar requests to find for each test sample')\n    \n    # FastText Model Loading (Optional)\n    parser.add_argument('--fasttext_model_path', type=str, default=None,\n                        help='Optional path to load a pre-existing FastText model (.bin file). If not provided, downloads default cc.en.300.bin.')\n\n    # DeBERTa Filtering Settings\n    parser.add_argument('--deberta_model_path', type=str,\n                        default=\"/kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/best_model_all_text_0.5.pt\",\n                        help='Path to the trained DeBERTa model state_dict (.pt file). Uses default if not specified.')\n    parser.add_argument('--deberta_label_encoder_path', type=str,\n                        default=\"/kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/label_encoder.json\",\n                        help='Path to the DeBERTa label encoder (.json file). Uses default if not specified.')\n    parser.add_argument('--deberta_selected_labels_path', type=str,\n                        default=\"/kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/selected_labels.json\",\n                        help='Optional path to the selected labels JSON file if feature selection was used for DeBERTa. Uses default if not specified.')\n    parser.add_argument('--deberta_threshold', type=float, default=0.5,\n                        help='Threshold for DeBERTa label prediction')\n\n    args, unknown = parser.parse_known_args()\n    main(args)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T15:31:13.607342Z","iopub.execute_input":"2025-04-22T15:31:13.607926Z","iopub.status.idle":"2025-04-22T15:33:01.315378Z","shell.execute_reply.started":"2025-04-22T15:31:13.607904Z","shell.execute_reply":"2025-04-22T15:33:01.314634Z"}},"outputs":[{"name":"stdout","text":"Loading data from /kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json...\n\nPreparing stratified split...\nWarning: Could not perform stratified split (setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1222,) + inhomogeneous part.)\nFalling back to random split\n\nOriginal Training Samples: 1099\nTest Samples: 123\n\nUsing device: cuda\n\nLoading DeBERTa model and resources for prediction...\nLoading selected labels from /kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/selected_labels.json\nUsing 10 selected labels for DeBERTa prediction.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15378dfc375d479daefea1aa95578410"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/3830460644.py:636: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  deberta_model.load_state_dict(torch.load(args.deberta_model_path, map_location=device))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f76a34537aaf4d8cb0136567cffbb052"}},"metadata":{}},{"name":"stdout","text":"Using 2 GPUs for DeBERTa prediction!\n\nPredicting labels for the test set using DeBERTa...\n","output_type":"stream"},{"name":"stderr","text":"\nPredicting labels with DeBERTa:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\nPredicting labels with DeBERTa:  25%|██▌       | 1/4 [00:02<00:06,  2.32s/it]\u001b[A\nPredicting labels with DeBERTa:  50%|█████     | 2/4 [00:03<00:03,  1.50s/it]\u001b[A\nPredicting labels with DeBERTa:  75%|███████▌  | 3/4 [00:04<00:01,  1.24s/it]\u001b[A\nPredicting labels with DeBERTa: 100%|██████████| 4/4 [00:04<00:00,  1.25s/it]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"Finished predicting labels for 123 test samples.\n\nFiltering training data based on globally predicted test labels...\nTotal unique labels predicted across test set: 9\nFiltered Training Samples: 860\nLoading/Downloading FastText model...\nDownloading standard pre-trained English model (cc.en.300.bin) with progress...\nPre-trained FastText model (cc.en.300.bin) not found, downloading...\nDownloading from https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n","output_type":"stream"},{"name":"stderr","text":"Downloading FastText Model: 4.50GB [00:32, 139MB/s]                                \n","output_type":"stream"},{"name":"stdout","text":"Extracting model file to /root/.fasttext/cc.en.300.bin...\nModel extracted to /root/.fasttext/cc.en.300.bin\nCreated symlink to model in current directory\nUsing downloaded model: /root/.fasttext/cc.en.300.bin\nLoading FastText model from /root/.fasttext/cc.en.300.bin...\nFastText model loaded. Dimension: 300\nGenerating embeddings with loaded FastText model...\n","output_type":"stream"},{"name":"stderr","text":"Generating FastText embeddings: 100%|██████████| 123/123 [00:00<00:00, 1425.91it/s]\nGenerating FastText embeddings: 100%|██████████| 860/860 [00:00<00:00, 1333.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generated 123 test embeddings.\nGenerated 860 filtered training embeddings.\nCalculating similarities and evaluating metrics...\n\nCalculating cosine similarities...\nEvaluating metrics for cosine similarity...\n\nCalculating euclidean similarities...\nEvaluating metrics for euclidean similarity...\n\nCalculating jaccard similarities...\nEvaluating metrics for jaccard similarity...\n\nGenerating comparison plots...\nSaved comparison plot to ./results/run_20250422_153113_all_text_0.5/similarity_metrics_comparison_k1_filtered_train.png\nSaved comparison plot to ./results/run_20250422_153113_all_text_0.5/similarity_metrics_comparison_k3_filtered_train.png\nSaved comparison plot to ./results/run_20250422_153113_all_text_0.5/similarity_metrics_comparison_k5_filtered_train.png\nSaved comparison plot to ./results/run_20250422_153113_all_text_0.5/similarity_metrics_comparison_k10_filtered_train.png\nSaved comparison metrics to ./results/run_20250422_153113_all_text_0.5/similarity_metrics_comparison_filtered_train.json\nProcessing results for cosine similarity...\nCosine Similarity Metrics (using filtered train set):\nMatch Rate (at least one match): 0.8293\nAverage Precision@1: 0.6226\nAverage Recall@1: 0.7606\nAverage F1@1: 0.6478\nAverage Precision@3: 0.4944\nAverage Recall@3: 0.8862\nAverage F1@3: 0.5932\nAverage Precision@5: 0.4065\nAverage Recall@5: 0.9268\nAverage F1@5: 0.5261\nAverage Precision@10: 0.3233\nAverage Recall@10: 0.9587\nAverage F1@10: 0.4409\nAverage Label Overlap: 0.6561\nProcessing results for euclidean similarity...\nEuclidean Similarity Metrics (using filtered train set):\nMatch Rate (at least one match): 0.8130\nAverage Precision@1: 0.6159\nAverage Recall@1: 0.7421\nAverage F1@1: 0.6392\nAverage Precision@3: 0.4832\nAverage Recall@3: 0.8726\nAverage F1@3: 0.5833\nAverage Precision@5: 0.4055\nAverage Recall@5: 0.9160\nAverage F1@5: 0.5191\nAverage Precision@10: 0.3213\nAverage Recall@10: 0.9519\nAverage F1@10: 0.4386\nAverage Label Overlap: 0.6527\nProcessing results for jaccard similarity...\nJaccard Similarity Metrics (using filtered train set):\nMatch Rate (at least one match): 0.7317\nAverage Precision@1: 0.6079\nAverage Recall@1: 0.6569\nAverage F1@1: 0.6014\nAverage Precision@3: 0.4771\nAverage Recall@3: 0.8039\nAverage F1@3: 0.5604\nAverage Precision@5: 0.4173\nAverage Recall@5: 0.8818\nAverage F1@5: 0.5271\nAverage Precision@10: 0.3258\nAverage Recall@10: 0.9313\nAverage F1@10: 0.4454\nAverage Label Overlap: 0.6715\n\nAnalysis completed! Results saved to ./results/run_20250422_153113_all_text_0.5\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Compare change requests using SBERT embeddings and similarity metrics, with optional DeBERTa label filtering.')\n    \n    parser.add_argument('--data_path', type=str, \n                        default=\"/kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json\",\n                        help='Path to the JSON data file')\n    parser.add_argument('--text_column', type=str, default='all_text_0.5',\n                        help='Column name with the text data to use for training')\n    parser.add_argument('--results_dir', type=str, default='./results',\n                        help='Directory to save results')\n    \n    parser.add_argument('--min_label_freq', type=int, default=5,\n                        help='Minimum frequency for a label to be considered')\n    parser.add_argument('--max_label_len', type=int, default=5,\n                        help='Maximum number of labels per sample')\n    \n    parser.add_argument('--batch_size', type=int, default=32,\n                        help='Batch size for DeBERTa prediction (FastText embedding is not batched)')\n    parser.add_argument('--top_k', type=int, default=5,\n                        help='Number of similar requests to find for each test sample')\n    \n    # FastText Model Loading (Optional)\n    parser.add_argument('--fasttext_model_path', type=str, default=None,\n                        help='Optional path to load a pre-existing FastText model (.bin file). Used only if --training_epochs is 0.')\n\n    # DeBERTa Filtering Settings\n    parser.add_argument('--deberta_model_path', type=str,\n                        default=\"/kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/best_model_all_text_0.5.pt\",\n                        help='Path to the trained DeBERTa model state_dict (.pt file). Uses default if not specified.')\n    parser.add_argument('--deberta_label_encoder_path', type=str,\n                        default='/kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/label_encoder.json',\n                        help='Path to the DeBERTa label encoder (.json file). Uses default if not specified.')\n    parser.add_argument('--deberta_selected_labels_path', type=str,\n                        default=\"/kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/selected_labels.json\",\n                        help='Optional path to the selected labels JSON file if feature selection was used for DeBERTa. Uses default if not specified.')\n    parser.add_argument('--deberta_threshold', type=float, default=0.5,\n                        help='Threshold for DeBERTa label prediction')\n\n    # FastText Training Arguments (Copied from 2_a)\n    parser.add_argument('--training_epochs', type=int, default=20, # Default to 0 (no training)\n                        help='Number of epochs for FastText supervised training. Set > 0 to enable training.')\n    parser.add_argument('--learning_rate', type=float, default=0.1,\n                        help='Learning rate for FastText training (used only if training_epochs > 0)')\n    parser.add_argument('--embedding_dim', type=int, default=300,\n                        help='Dimension of embeddings. If training, this defines output dim. If loading, it should match the loaded model.')\n    parser.add_argument('--word_ngrams', type=int, default=2,\n                        help='Max length of word n-grams for FastText training (used only if training_epochs > 0)')\n    parser.add_argument('--loss_function', type=str, default='softmax', choices=['softmax', 'ns', 'hs'],\n                        help='Loss function for FastText training (used only if training_epochs > 0)')\n\n    args, unknown = parser.parse_known_args()\n\n    # Validate arguments related to training/loading\n    if args.training_epochs <= 0 and args.fasttext_model_path and not os.path.exists(args.fasttext_model_path):\n         print(f\"Warning: --training_epochs is not > 0, and the specified --fasttext_model_path '{args.fasttext_model_path}' does not exist. Will attempt to download the default model.\")\n         args.fasttext_model_path = None # Reset path so download is triggered\n\n    main(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T16:09:48.824551Z","iopub.execute_input":"2025-04-22T16:09:48.825117Z","iopub.status.idle":"2025-04-22T16:14:15.308232Z","shell.execute_reply.started":"2025-04-22T16:09:48.825095Z","shell.execute_reply":"2025-04-22T16:14:15.307316Z"}},"outputs":[{"name":"stdout","text":"Loading data from /kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json...\n\nPreparing stratified split...\nWarning: Could not perform stratified split (setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1222,) + inhomogeneous part.)\nFalling back to random split\n\nOriginal Training Samples: 1099\nTest Samples: 123\n\nUsing device: cuda\n\nLoading DeBERTa model and resources for prediction...\nLoading selected labels from /kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/selected_labels.json\nUsing 10 selected labels for DeBERTa prediction.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/690421907.py:784: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  deberta_model.load_state_dict(torch.load(args.deberta_model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs for DeBERTa prediction!\n\nPredicting labels for the test set using DeBERTa...\n","output_type":"stream"},{"name":"stderr","text":"Predicting labels with DeBERTa: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Finished predicting labels for 123 test samples.\n\nFiltering training data based on globally predicted test labels...\nTotal unique labels predicted across test set: 9\nFiltered Training Samples: 860\nLoading/Downloading FastText model...\n\n--- Starting FastText Fine-Tuning ---\nEnsuring pre-trained FastText vectors (.vec) are available...\nPre-trained FastText vectors (cc.en.300.vec) not found, downloading...\nDownloading from https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n","output_type":"stream"},{"name":"stderr","text":"Downloading FastText Model: 1.33GB [00:37, 35.5MB/s]                            \n","output_type":"stream"},{"name":"stdout","text":"Extracting vector file to /root/.fasttext/cc.en.300.vec...\nVectors extracted to /root/.fasttext/cc.en.300.vec\nCreated symlink to vectors in current directory\nFormatting data for FastText training, saving to ./results/run_20250422_160948_all_text_0.5/train_fasttext_for_finetuning.txt...\n","output_type":"stream"},{"name":"stderr","text":"Formatting data: 100%|██████████| 1099/1099 [00:00<00:00, 7054.09it/s]","output_type":"stream"},{"name":"stdout","text":"Data formatting complete.\n\nStarting FastText supervised training...\n  Train data: ./results/run_20250422_160948_all_text_0.5/train_fasttext_for_finetuning.txt\n  Pretrained vectors: /root/.fasttext/cc.en.300.vec\n  Output model: ./results/run_20250422_160948_all_text_0.5/finetuned_fasttext_model.bin\n  Epochs: 20, LR: 0.1, Dim: 300, Ngrams: 2, Loss: softmax\n","output_type":"stream"},{"name":"stderr","text":"\nRead 0M words\nNumber of words:  13093\nNumber of labels: 17\nProgress: 100.0% words/sec/thread:  458555 lr:  0.000000 avg.loss:  2.831707 ETA:   0h 0m 0s% words/sec/thread:  434120 lr:  0.061884 avg.loss:  2.840755 ETA:   0h 0m15s 40.6% words/sec/thread:  435739 lr:  0.059359 avg.loss:  2.837286 ETA:   0h 0m15s 452216 lr:  0.042699 avg.loss:  2.831589 ETA:   0h 0m10s 74.5% words/sec/thread:  457188 lr:  0.025533 avg.loss:  2.833015 ETA:   0h 0m 6s avg.loss:  2.831601 ETA:   0h 0m 0s\n","output_type":"stream"},{"name":"stdout","text":"Training complete. Model saved to ./results/run_20250422_160948_all_text_0.5/finetuned_fasttext_model.bin\nFine-tuning complete. Model saved to ./results/run_20250422_160948_all_text_0.5/finetuned_fasttext_model.bin\n\nLoading final FastText model from ./results/run_20250422_160948_all_text_0.5/finetuned_fasttext_model.bin...\nFastText model loaded. Dimension: 300\nGenerating embeddings with loaded FastText model...\n","output_type":"stream"},{"name":"stderr","text":"Generating FastText embeddings: 100%|██████████| 123/123 [00:00<00:00, 2936.67it/s]\nGenerating FastText embeddings: 100%|██████████| 860/860 [00:00<00:00, 3312.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generated 123 test embeddings.\nGenerated 860 filtered training embeddings.\nCalculating similarities and evaluating metrics...\n\nCalculating cosine similarities...\nEvaluating metrics for cosine similarity...\n\nCalculating euclidean similarities...\nEvaluating metrics for euclidean similarity...\n\nCalculating jaccard similarities...\nEvaluating metrics for jaccard similarity...\n\nGenerating comparison plots...\nSaved comparison plot to ./results/run_20250422_160948_all_text_0.5/similarity_metrics_comparison_k1_filtered_train.png\nSaved comparison plot to ./results/run_20250422_160948_all_text_0.5/similarity_metrics_comparison_k3_filtered_train.png\nSaved comparison plot to ./results/run_20250422_160948_all_text_0.5/similarity_metrics_comparison_k5_filtered_train.png\nSaved comparison plot to ./results/run_20250422_160948_all_text_0.5/similarity_metrics_comparison_k10_filtered_train.png\nSaved comparison metrics to ./results/run_20250422_160948_all_text_0.5/similarity_metrics_comparison_filtered_train.json\nProcessing results for cosine similarity...\nCosine Similarity Metrics (using filtered train set):\nMatch Rate (at least one match): 0.7236\nAverage Precision@1: 0.6100\nAverage Recall@1: 0.6691\nAverage F1@1: 0.6129\nAverage Precision@3: 0.4710\nAverage Recall@3: 0.8516\nAverage F1@3: 0.5725\nAverage Precision@5: 0.3858\nAverage Recall@5: 0.8930\nAverage F1@5: 0.5084\nAverage Precision@10: 0.3148\nAverage Recall@10: 0.9417\nAverage F1@10: 0.4381\nAverage Label Overlap: 0.6742\nProcessing results for euclidean similarity...\nEuclidean Similarity Metrics (using filtered train set):\nMatch Rate (at least one match): 0.7317\nAverage Precision@1: 0.6220\nAverage Recall@1: 0.6711\nAverage F1@1: 0.6179\nAverage Precision@3: 0.4759\nAverage Recall@3: 0.8489\nAverage F1@3: 0.5773\nAverage Precision@5: 0.3928\nAverage Recall@5: 0.8984\nAverage F1@5: 0.5128\nAverage Precision@10: 0.2988\nAverage Recall@10: 0.9431\nAverage F1@10: 0.4212\nAverage Label Overlap: 0.6786\nProcessing results for jaccard similarity...\nJaccard Similarity Metrics (using filtered train set):\nMatch Rate (at least one match): 0.6992\nAverage Precision@1: 0.6125\nAverage Recall@1: 0.6270\nAverage F1@1: 0.5913\nAverage Precision@3: 0.4725\nAverage Recall@3: 0.8184\nAverage F1@3: 0.5668\nAverage Precision@5: 0.3969\nAverage Recall@5: 0.8753\nAverage F1@5: 0.5127\nAverage Precision@10: 0.3044\nAverage Recall@10: 0.8984\nAverage F1@10: 0.4194\nAverage Label Overlap: 0.6758\n\nAnalysis completed! Results saved to ./results/run_20250422_160948_all_text_0.5\n","output_type":"stream"}],"execution_count":8}]}