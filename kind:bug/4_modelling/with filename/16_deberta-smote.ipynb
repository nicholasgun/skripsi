{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10981401,"sourceType":"datasetVersion","datasetId":6834080}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -U scikit-learn imbalanced-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import DebertaTokenizer, DebertaModel\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm import tqdm\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    multilabel_confusion_matrix\n)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport argparse\nimport os\nimport json\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE\n\n# Suppress expected warnings\nimport warnings\nwarnings.filterwarnings('ignore', category=UserWarning, module='sklearn.metrics.cluster._supervised')\nwarnings.filterwarnings('ignore', category=UserWarning, module='sklearn.feature_selection')\n\n# Add GPU count check at the top level\ndef get_available_gpus():\n    \"\"\"Get the number of available GPUs and their IDs\"\"\"\n    if not torch.cuda.is_available():\n        return 0, []\n    \n    n_gpus = torch.cuda.device_count()\n    gpu_ids = list(range(n_gpus))\n    return n_gpus, gpu_ids\n\ndef reduce_tokens_simple_truncation(text, tokenizer, max_length=512):\n    \"\"\"\n    Simply truncate text to the maximum allowed token length.\n    \n    Args:\n        text (str): Input text\n        tokenizer: Tokenizer to use\n        max_length (int): Maximum token length\n        \n    Returns:\n        str: Truncated text\n    \"\"\"\n    tokens = tokenizer(text, truncation=True, max_length=max_length)\n    return tokenizer.decode(tokens['input_ids'], skip_special_tokens=True)\n\ndef reduce_tokens_smart_truncation(text, tokenizer, max_length=512):\n    \"\"\"\n    Intelligently truncate text by keeping the beginning and end portions.\n    \n    Args:\n        text (str): Input text\n        tokenizer: Tokenizer to use\n        max_length (int): Maximum token length\n        \n    Returns:\n        str: Truncated text with beginning and end portions\n    \"\"\"\n    tokens = tokenizer(text, truncation=False, return_tensors=\"pt\")[\"input_ids\"][0]\n    \n    if len(tokens) <= max_length:\n        return text\n    \n    # Keep beginning and end portions (prioritize beginning slightly)\n    beginning_length = max_length // 2 + 50  # Keep slightly more from beginning\n    end_length = max_length - beginning_length - 1  # Reserve 1 for separator\n    \n    beginning_tokens = tokens[:beginning_length]\n    end_tokens = tokens[-end_length:]\n    \n    # Combine with a separator token\n    beginning_text = tokenizer.decode(beginning_tokens, skip_special_tokens=True)\n    end_text = tokenizer.decode(end_tokens, skip_special_tokens=True)\n    \n    return f\"{beginning_text} [...] {end_text}\"\n\ndef reduce_tokens_extractive_summarization(text, tokenizer, max_length=512):\n    \"\"\"\n    Reduce text length using extractive summarization techniques.\n    \n    Args:\n        text (str): Input text\n        tokenizer: Tokenizer to use\n        max_length (int): Maximum token length\n        \n    Returns:\n        str: Summarized text\n    \"\"\"\n    tokens = tokenizer(text, truncation=False, return_tensors=\"pt\")[\"input_ids\"][0]\n    \n    if len(tokens) <= max_length:\n        return text\n    \n    # Import NLTK for sentence tokenization\n    try:\n        import nltk\n        from nltk.tokenize import sent_tokenize\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n    except ImportError:\n        # If NLTK is not available, fall back to smart truncation\n        return reduce_tokens_smart_truncation(text, tokenizer, max_length)\n    \n    # Split text into sentences\n    sentences = sent_tokenize(text)\n    \n    if len(sentences) <= 3:\n        # Not enough sentences to summarize meaningfully, use smart truncation\n        return reduce_tokens_smart_truncation(text, tokenizer, max_length)\n    \n    # Get sentence token lengths\n    sentence_tokens = []\n    for sentence in sentences:\n        tokens = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"][0]\n        sentence_tokens.append((sentence, len(tokens)))\n    \n    # Calculate target ratio based on max length vs total length\n    tokens = tokenizer(text, truncation=False, return_tensors=\"pt\")[\"input_ids\"][0]\n    reduction_ratio = max_length / len(tokens)\n    \n    # Always keep first and last sentences\n    first_sentence, first_len = sentence_tokens[0]\n    last_sentence, last_len = sentence_tokens[-1]\n    \n    remaining_length = max_length - first_len - last_len - 10  # Reserve some tokens for separators\n    \n    # If can't even fit first and last sentences, use smart truncation\n    if remaining_length <= 0:\n        return reduce_tokens_smart_truncation(text, tokenizer, max_length)\n    \n    # Choose middle sentences based on importance (for now, just choose evenly distributed sentences)\n    middle_sentences = sentence_tokens[1:-1]\n    \n    # Calculate how many middle sentences we can include\n    middle_sentences_to_keep = []\n    current_length = 0\n    \n    # Select sentences in a distributed manner\n    if len(middle_sentences) > 0:\n        # Fix: Add a check to prevent division by zero\n        sentences_to_keep = int(reduction_ratio * len(middle_sentences))\n        if sentences_to_keep <= 0:\n            step = len(middle_sentences) + 1  # This will select only the first sentence if any\n        else:\n            step = max(1, len(middle_sentences) // sentences_to_keep)\n            \n        for i in range(0, len(middle_sentences), step):\n            sentence, length = middle_sentences[i]\n            if current_length + length <= remaining_length:\n                middle_sentences_to_keep.append(sentence)\n                current_length += length\n            else:\n                break\n    \n    # Combine sentences\n    summarized_text = first_sentence\n    \n    if middle_sentences_to_keep:\n        summarized_text += \" \" + \" \".join(middle_sentences_to_keep)\n    \n    summarized_text += \" \" + last_sentence\n    \n    # Verify final length is within limit\n    final_tokens = tokenizer(summarized_text, truncation=False, return_tensors=\"pt\")[\"input_ids\"][0]\n    if len(final_tokens) > max_length:\n        # Fall back to smart truncation if still too long\n        return reduce_tokens_smart_truncation(summarized_text, tokenizer, max_length)\n    \n    return summarized_text\n\ndef reduce_tokens_hybrid(text, tokenizer, max_length=512):\n    \"\"\"\n    Use a hybrid approach combining extractive summarization and smart truncation.\n    \n    Args:\n        text (str): Input text\n        tokenizer: Tokenizer to use\n        max_length (int): Maximum token length\n        \n    Returns:\n        str: Processed text\n    \"\"\"\n    tokens = tokenizer(text, truncation=False, return_tensors=\"pt\")[\"input_ids\"][0]\n    \n    if len(tokens) <= max_length:\n        return text\n    \n    # For very long documents, use extractive summarization first\n    if len(tokens) > max_length * 2:\n        summarized = reduce_tokens_extractive_summarization(text, tokenizer, max_length)\n        summarized_tokens = tokenizer(summarized, truncation=False, return_tensors=\"pt\")[\"input_ids\"][0]\n        \n        # If still too long, apply smart truncation\n        if len(summarized_tokens) > max_length:\n            return reduce_tokens_smart_truncation(summarized, tokenizer, max_length)\n        return summarized\n    \n    # For moderately long documents, use smart truncation directly\n    return reduce_tokens_smart_truncation(text, tokenizer, max_length)\n\ndef process_with_token_reduction(texts, tokenizer, max_length=512, strategy=\"smart_truncation\"):\n    \"\"\"\n    Process a series of texts by applying token reduction where necessary.\n    \n    Args:\n        texts (pd.Series): Series of input texts\n        tokenizer: Tokenizer to use for tokenization\n        max_length (int): Maximum token length (default: 512)\n        strategy (str): Token reduction strategy, one of:\n            - \"simple\": Simple truncation at max_length\n            - \"smart_truncation\": Keep beginning and end portions\n            - \"extractive_summarization\": Use extractive summarization\n            - \"hybrid\": Combine summarization and smart truncation\n            \n    Returns:\n        pd.Series: Series with processed texts\n    \"\"\"\n    processed_texts = []\n    token_lengths_before = []\n    token_lengths_after = []\n    \n    for text in tqdm(texts, desc=f\"Applying token reduction ({strategy})\"):\n        # Calculate original token length\n        tokens = tokenizer(text, truncation=False, return_tensors=\"pt\")[\"input_ids\"][0]\n        token_lengths_before.append(len(tokens))\n        \n        # Only process if longer than max_length\n        if len(tokens) <= max_length:\n            processed_texts.append(text)\n            token_lengths_after.append(len(tokens))\n            continue\n        \n        # Apply selected strategy\n        if strategy == \"simple\":\n            processed_text = reduce_tokens_simple_truncation(text, tokenizer, max_length)\n        elif strategy == \"smart_truncation\":\n            processed_text = reduce_tokens_smart_truncation(text, tokenizer, max_length)\n        elif strategy == \"extractive_summarization\":\n            processed_text = reduce_tokens_extractive_summarization(text, tokenizer, max_length)\n        elif strategy == \"hybrid\":\n            processed_text = reduce_tokens_hybrid(text, tokenizer, max_length)\n        else:\n            # Default to smart truncation\n            processed_text = reduce_tokens_smart_truncation(text, tokenizer, max_length)\n        \n        processed_texts.append(processed_text)\n        \n        # Calculate new token length\n        new_tokens = tokenizer(processed_text, truncation=False, return_tensors=\"pt\")[\"input_ids\"][0]\n        token_lengths_after.append(len(new_tokens))\n    \n    # Print statistics\n    print(f\"\\nToken reduction statistics using {strategy} strategy:\")\n    print(f\"  Before:\")\n    print(f\"    Mean length: {np.mean(token_lengths_before):.2f}\")\n    print(f\"    Median length: {np.median(token_lengths_before):.2f}\")\n    print(f\"    Max length: {max(token_lengths_before)}\")\n    print(f\"    Docs exceeding {max_length} tokens: {sum(1 for l in token_lengths_before if l > max_length)} ({sum(1 for l in token_lengths_before if l > max_length)/len(token_lengths_before)*100:.2f}%)\")\n    \n    print(f\"  After:\")\n    print(f\"    Mean length: {np.mean(token_lengths_after):.2f}\")\n    print(f\"    Median length: {np.median(token_lengths_after):.2f}\")\n    print(f\"    Max length: {max(token_lengths_after)}\")\n    print(f\"    Docs exceeding {max_length} tokens: {sum(1 for l in token_lengths_after if l > max_length)} ({sum(1 for l in token_lengths_after if l > max_length)/len(token_lengths_after)*100:.2f}%)\")\n    \n    # Optional: Create histogram plot\n    try:\n        plt.figure(figsize=(10, 6))\n        plt.hist([token_lengths_before, token_lengths_after], bins=30, \n                 label=['Before reduction', 'After reduction'], alpha=0.7)\n        plt.axvline(x=max_length, color='r', linestyle='--', label=f'Max length ({max_length})')\n        plt.title(f'Token Length Distribution Before and After {strategy}')\n        plt.xlabel('Number of Tokens')\n        plt.ylabel('Frequency')\n        plt.legend()\n        plt.savefig(f'token_reduction_{strategy}.png')\n        plt.close()\n        print(f\"  Distribution plot saved as token_reduction_{strategy}.png\")\n    except Exception as e:\n        print(f\"  Could not create distribution plot: {str(e)}\")\n    \n    return pd.Series(processed_texts, index=texts.index)\n\ndef calculate_token_lengths(texts, tokenizer):\n    \"\"\"\n    Calculate the token length for each text sample using the specified tokenizer.\n    \n    Args:\n        texts (pd.Series): Series of input texts\n        tokenizer: Tokenizer to use for tokenization\n        \n    Returns:\n        pd.Series: Series containing the token length of each text\n    \"\"\"\n    token_lengths = []\n    for text in tqdm(texts, desc=\"Calculating token lengths\"):\n        tokens = tokenizer(str(text), truncation=False, return_tensors=\"pt\")\n        token_lengths.append(len(tokens['input_ids'][0]))\n    \n    return pd.Series(token_lengths, index=texts.index)\n\ndef filter_outliers_by_token_length(texts, token_lengths, std_threshold=3.0, min_token_threshold=None):\n    \"\"\"\n    Filter out text samples with token lengths beyond a certain standard deviation threshold.\n    \n    Args:\n        texts (pd.Series): Series of input texts\n        token_lengths (pd.Series): Series containing token length of each text\n        std_threshold (float): Standard deviation threshold (default: 3.0)\n        min_token_threshold (int, optional): Minimum number of tokens required (default: None)\n        \n    Returns:\n        tuple: Filtered texts and boolean mask to apply to original data\n    \"\"\"\n    mean_length = token_lengths.mean()\n    std_length = token_lengths.std()\n    \n    # Print original token statistics\n    print(f\"Token length statistics before filtering:\")\n    print(f\"  Mean: {mean_length:.2f}, Std Dev: {std_length:.2f}\")\n    print(f\"  Min: {token_lengths.min()}, Max: {token_lengths.max()}\")\n    print(f\"  25th percentile: {token_lengths.quantile(0.25):.2f}\")\n    print(f\"  50th percentile (median): {token_lengths.quantile(0.5):.2f}\")\n    print(f\"  75th percentile: {token_lengths.quantile(0.75):.2f}\")\n    \n    # Original data size\n    original_size = len(texts)\n    \n    # Start with all True mask for the original data\n    final_mask = pd.Series(True, index=texts.index)\n    \n    # Step 1: Apply standard deviation filtering if std_threshold is provided\n    if std_threshold < float('inf'):\n        # Define upper and lower bounds\n        upper_bound = mean_length + std_threshold * std_length\n        lower_bound = mean_length - std_threshold * std_length\n        lower_bound = max(1, lower_bound)  # Ensure lower bound is at least 1\n        \n        # Create mask for samples within bounds\n        std_mask = (token_lengths >= lower_bound) & (token_lengths <= upper_bound)\n        \n        # Update final mask with standard deviation condition\n        final_mask = final_mask & std_mask\n        \n        std_removed = (~std_mask).sum()\n        print(f\"Applied {std_threshold} std dev threshold: ({lower_bound:.2f}, {upper_bound:.2f})\")\n        print(f\"Removed {std_removed} samples by std dev filtering ({std_removed/original_size*100:.2f}% of data)\")\n    \n    # Step 2: Apply minimum token threshold if specified\n    if min_token_threshold is not None:\n        # Create mask for minimum token threshold\n        min_token_mask = token_lengths >= min_token_threshold\n        \n        # Track how many would be removed by this filter\n        min_token_removed = (~min_token_mask).sum()\n        \n        # Track how many would be removed by this filter that weren't already filtered by std\n        additional_removed = ((~min_token_mask) & final_mask).sum()\n        \n        # Update final mask with minimum token threshold condition\n        final_mask = final_mask & min_token_mask\n        \n        print(f\"Applied minimum token threshold of {min_token_threshold}\")\n        print(f\"Removed {min_token_removed} samples below minimum token threshold ({min_token_removed/original_size*100:.2f}% of original data)\")\n        print(f\"Of which {additional_removed} weren't already filtered by std deviation ({additional_removed/original_size*100:.2f}% of original data)\")\n    \n    # Apply final mask to get filtered data\n    filtered_texts = texts[final_mask]\n    filtered_token_lengths = token_lengths[final_mask]\n    \n    # Calculate total removed\n    total_removed = (~final_mask).sum()\n    print(f\"Total removed: {total_removed} samples ({total_removed/original_size*100:.2f}% of original data)\")\n    print(f\"Remaining: {final_mask.sum()} samples ({final_mask.sum()/original_size*100:.2f}% of original data)\")\n\n    # Print final statistics\n    print(f\"\\nToken length statistics after all filtering:\")\n    print(f\"  Mean: {filtered_token_lengths.mean():.2f}, Std Dev: {filtered_token_lengths.std():.2f}\")\n    print(f\"  Min: {filtered_token_lengths.min()}, Max: {filtered_token_lengths.max()}\")\n    print(f\"  25th percentile: {filtered_token_lengths.quantile(0.25):.2f}\")\n    print(f\"  50th percentile (median): {filtered_token_lengths.quantile(0.5):.2f}\")\n    print(f\"  75th percentile: {filtered_token_lengths.quantile(0.75):.2f}\")\n    \n    return filtered_texts, final_mask\n\nclass IssueDataset(Dataset):\n    \"\"\"\n    Dataset for processing text data and multi-label classification.\n\n    Args:\n        texts (pd.Series): Series of input texts.\n        labels (list or pd.Series): Corresponding labels (one-hot encoded for multi-label).\n        tokenizer (transformers.PreTrainedTokenizer): Tokenizer for converting text to tokens.\n        max_length (int): Maximum length of tokenized sequences (default is 512).\n    \"\"\"\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts.reset_index(drop=True)\n        # Reset index for labels if it's a pandas Series.\n        if isinstance(labels, pd.Series):\n            self.labels = labels.reset_index(drop=True)\n        else:\n            self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts.iloc[idx])\n        encodings = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        # For multi-label classification, ensure we're passing the full label array\n        # and not just a single value\n        label = self.labels[idx]\n        \n        # Make sure we're getting a proper multi-dimensional label array\n        # and not flattening it incorrectly\n        if isinstance(label, (list, np.ndarray)):\n            # Convert directly to tensor without modifying shape\n            label = torch.tensor(label, dtype=torch.float)\n        else:\n            # If it's not already an array-like structure, this is likely a mistake\n            # as we expect multi-label one-hot encoded data\n            raise ValueError(f\"Expected multi-dimensional label array but got {type(label)}\")\n        \n        return {\n            'input_ids': encodings['input_ids'].flatten(),\n            'attention_mask': encodings['attention_mask'].flatten(),\n            'labels': label\n        }\n    \nclass DeBERTaClassifier(nn.Module):\n    \"\"\"\n    A classifier model based on DeBERTa for multi-label classification.\n    \n    This model uses a pre-trained DeBERTa model as the encoder and adds a \n    classification head on top with sigmoid activation for multi-label output.\n    The DeBERTa model is completely frozen, only the classification layer is trained.\n    \n    Args:\n        num_labels (int): Number of classes in the multi-label classification task.\n    \"\"\"\n    def __init__(self, num_labels):\n        super().__init__()\n        self.deberta = DebertaModel.from_pretrained('microsoft/deberta-base')\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(768, num_labels)\n        \n        # Freeze all parameters in DeBERTa\n        for param in self.deberta.parameters():\n            param.requires_grad = False\n\n    def forward(self, input_ids, attention_mask):\n        with torch.no_grad():  # Ensure no gradients flow through DeBERTa\n            outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        \n        # Get the [CLS] token representation\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        cls_output = self.dropout(cls_output)\n        # Return raw logits for BCEWithLogitsLoss\n        return self.classifier(cls_output)\n        \n    def get_embeddings(self, input_ids, attention_mask):\n        \"\"\"\n        Extract embeddings from the DeBERTa model without computing gradients.\n        \n        Args:\n            input_ids: Input token IDs\n            attention_mask: Attention mask\n            \n        Returns:\n            torch.Tensor: CLS token embeddings for each input\n        \"\"\"\n        with torch.no_grad():\n            outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n            # Get the [CLS] token representation\n            embeddings = outputs.last_hidden_state[:, 0, :]\n        return embeddings\n\nclass EarlyStopping:\n    \"\"\"\n    Early stopping to stop training when the validation loss does not improve sufficiently.\n    \n    For multi-label classification, we consider a loss improvement when \n    the validation loss decreases by at least min_delta.\n    \n    Args:\n        patience (int): Number of epochs to wait for an improvement before stopping.\n        min_delta (float): Minimum decrease in the monitored loss to qualify as an improvement.\n    \"\"\"\n    def __init__(self, patience=3, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n    def reset(self):\n        \"\"\"Reset the early stopping state.\"\"\"\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n\ndef train_epoch(model, loader, criterion, optimizer, device, threshold=0.5, early_stopping=None):\n    \"\"\"\n    Train the model for one epoch, computing loss and metrics for multi-label classification.\n\n    Args:\n        model (nn.Module): The multi-label classification model.\n        loader (DataLoader): Training DataLoader.\n        criterion: Loss function (BCEWithLogitsLoss).\n        optimizer: Optimization algorithm.\n        device: Device to perform training (CPU or GPU).\n        threshold (float): Threshold for binary predictions (default is 0.5).\n        early_stopping (EarlyStopping, optional): Instance to monitor improvement in loss.\n\n    Returns:\n        tuple: Average loss, Hamming accuracy, and a flag indicating if early stopping was triggered.\n    \"\"\"\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    for batch in tqdm(loader, desc=\"Training\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        # Apply sigmoid and threshold for predictions\n        predictions = torch.sigmoid(outputs) >= threshold\n        all_preds.append(predictions.cpu().detach().numpy())\n        all_labels.append(labels.cpu().detach().numpy())\n    \n    # Calculate metrics for multi-label classification\n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n    \n    # Use subset accuracy (exact match) for a strict measure\n    exact_match = (all_preds == all_labels).all(axis=1).mean()\n    \n    avg_loss = total_loss / len(loader)\n    \n    if early_stopping:\n        early_stopping(avg_loss)\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered\")\n            return avg_loss, exact_match, True\n            \n    return avg_loss, exact_match, False\n    \n\ndef validate(model, loader, criterion, device, threshold=0.5):\n    \"\"\"\n    Evaluate the model on provided validation data for multi-label classification.\n\n    Args:\n        model (nn.Module): The multi-label classification model.\n        loader (DataLoader): Validation DataLoader.\n        criterion: Loss function (BCEWithLogitsLoss).\n        device: Device to perform evaluation.\n        threshold (float): Threshold for binary predictions (default is 0.5).\n\n    Returns:\n        tuple: Average loss, various accuracy metrics, precision, recall, and F1 score.\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            \n            # Apply sigmoid and threshold for predictions\n            predictions = (torch.sigmoid(outputs) >= threshold).float()\n            all_preds.append(predictions.cpu().numpy())\n            all_labels.append(labels.cpu().numpy())\n    \n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n    \n    # Calculate different multi-label metrics\n    \n    # 1. Exact Match / Subset Accuracy (all labels must be correct)\n    exact_match = (all_preds == all_labels).all(axis=1).mean()\n    \n    # 2. Partial Match Accuracy (only count correctly predicted 1s, ignore 0s)\n    # Calculate true positives per sample\n    true_positives = np.logical_and(all_preds == 1, all_labels == 1).sum(axis=1)\n    # Calculate total actual positives per sample\n    total_positives = (all_labels == 1).sum(axis=1)\n    # Handle division by zero - samples with no positive labels get a score of 0\n    partial_match = np.zeros_like(true_positives, dtype=float)\n    # Only calculate ratio for samples with at least one positive label\n    mask = total_positives > 0\n    partial_match[mask] = true_positives[mask] / total_positives[mask]\n    partial_match_accuracy = partial_match.mean()\n    \n    # 3. Jaccard Similarity (intersection over union)\n    def jaccard_score(y_true, y_pred):\n        intersection = np.logical_and(y_true, y_pred).sum(axis=1)\n        union = np.logical_or(y_true, y_pred).sum(axis=1)\n        # Create a float array for output to avoid type casting error\n        result = np.zeros_like(intersection, dtype=float)\n        # Avoid division by zero\n        np.divide(intersection, union, out=result, where=union!=0)\n        return np.mean(result)\n    \n    jaccard_sim = jaccard_score(all_labels.astype(bool), all_preds.astype(bool))\n    \n    # Add Hamming metric - this is the same as partial_match_accuracy\n    hamming_sim = partial_match_accuracy\n    \n    # Sample-based metrics - Each sample contributes equally regardless of number of labels\n    precision = precision_score(all_labels, all_preds, average='samples', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='samples', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='samples', zero_division=0)\n    \n    return (total_loss / len(loader), \n            {\"exact_match\": exact_match, \n             \"partial_match\": partial_match_accuracy,\n             \"hamming\": hamming_sim,\n             \"jaccard\": jaccard_sim}, \n            precision, recall, f1)\n\ndef plot_multilabel_confusion_matrix(y_true, y_pred, class_names):\n    \"\"\"\n    Plot confusion matrices for each label in a multi-label classification problem.\n    \n    Args:\n        y_true (numpy.ndarray): True binary labels.\n        y_pred (numpy.ndarray): Predicted binary labels.\n        class_names (list): Names of the classes/labels.\n    \"\"\"\n    confusion_matrices = multilabel_confusion_matrix(y_true, y_pred)\n    \n    num_classes = len(class_names)\n    fig, axes = plt.subplots(nrows=(num_classes + 3) // 4, ncols=min(4, num_classes), \n                             figsize=(20, 5 * ((num_classes + 3) // 4)))\n    if num_classes == 1:\n        axes = np.array([axes])  # Make it indexable for single class\n    axes = axes.flatten()\n    \n    for i, matrix in enumerate(confusion_matrices):\n        if i < num_classes:  # Ensure we don't exceed the number of classes\n            ax = axes[i]\n            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', ax=ax)\n            ax.set_title(f'Label: {class_names[i]}')\n            ax.set_xlabel('Predicted')\n            ax.set_ylabel('True')\n            ax.set_xticklabels(['Negative', 'Positive'])\n            ax.set_yticklabels(['Negative', 'Positive'])\n    \n    # Hide any unused subplots\n    for i in range(num_classes, len(axes)):\n        fig.delaxes(axes[i])\n        \n    plt.tight_layout()\n    return fig\n\ndef prepare_data(df, text_column='all_text', min_label_freq=0, max_label_len=100, min_label_comb_freq=0, tokenizer=None, token_std_threshold=None, min_token_threshold=None):\n    \"\"\"\n    Filter out infrequent labels, samples with too many labels, and token length outliers.\n    \n    Args:\n        df (pd.DataFrame): DataFrame with text column and 'labels'\n        text_column (str): Name of the column containing the text data to use\n        min_label_freq (int): Minimum frequency for a label to be considered frequent.\n        max_label_len (int): Maximum number of labels per sample.\n        min_label_comb_freq (int): Minimum frequency for a label combination to be kept.\n        tokenizer: Tokenizer to use for token length calculation (required if token_std_threshold is provided)\n        token_std_threshold (float, optional): Standard deviation threshold for filtering token length outliers.\n            If None, no token length filtering is applied. Common values are 2.0 or 3.0.\n        min_token_threshold (int, optional): Minimum number of tokens required for a sample.\n            If None, no minimum token threshold is applied.\n\n    Returns:\n        tuple: Filtered texts and labels.\n    \"\"\"\n    # Print initial dataset size\n    initial_size = len(df)\n    print(f\"\\n=== DATA PREPROCESSING STATISTICS ===\")\n    print(f\"Initial dataset size: {initial_size}\")\n    \n    # Only keep text column and 'labels' columns\n    if text_column in df.columns:\n        df = df[[text_column, 'labels']]\n        # Filter out rows with 'nan' text\n        before_nan_filter = len(df)\n        df = df[~df[text_column].apply(lambda x: x.startswith('nan') if isinstance(x, str) else False)]\n        nan_removed = before_nan_filter - len(df)\n        if nan_removed > 0:\n            print(f\"Step 1: Removed {nan_removed} rows with 'nan' text ({nan_removed/before_nan_filter*100:.2f}% of data)\")\n    else:\n        raise ValueError(f\"Text column '{text_column}' not found in the DataFrame. Available columns: {df.columns.tolist()}\")\n    \n    # Drop rows with missing labels\n    before_na_drop = len(df)\n    df = df.dropna()\n    na_removed = before_na_drop - len(df)\n    if na_removed > 0:\n        print(f\"Step 2: Removed {na_removed} rows with missing labels ({na_removed/before_na_drop*100:.2f}% of data)\")\n    \n    # Extract issue texts and labels\n    texts = df[text_column]\n    labels = df['labels'].apply(lambda x: x if isinstance(x, list) else [])  # Ensure labels are lists\n    current_size = len(texts)\n    print(f\"Dataset size after basic cleaning: {current_size} ({current_size/initial_size*100:.2f}% of original data)\")\n\n    # Filter by token length if requested\n    if (token_std_threshold is not None or min_token_threshold is not None) and tokenizer is not None:\n        print(f\"\\nStep 3: Filtering by token length...\")\n        if token_std_threshold is not None:\n            print(f\"Using {token_std_threshold} standard deviation threshold\")\n        if min_token_threshold is not None:\n            print(f\"Using minimum token threshold of {min_token_threshold}\")\n        \n        # Calculate token lengths\n        token_lengths = calculate_token_lengths(texts, tokenizer)\n        \n        # Apply token length filtering\n        before_token_filter = len(texts)\n        filtered_texts, token_mask = filter_outliers_by_token_length(\n            texts, \n            token_lengths, \n            std_threshold=token_std_threshold if token_std_threshold is not None else float('inf'),\n            min_token_threshold=min_token_threshold\n        )\n        # Apply same filter to labels\n        filtered_labels = labels[token_mask].reset_index(drop=True)\n        token_removed = before_token_filter - len(filtered_texts)\n        print(f\"Removed {token_removed} samples by token length filtering ({token_removed/before_token_filter*100:.2f}% of data)\")\n        print(f\"Texts after token length filtering: {len(filtered_texts)} ({len(filtered_texts)/initial_size*100:.2f}% of original data)\")\n\n    # Get labels count distribution\n    label_distribution = Counter([label for labels in labels for label in labels])\n    total_labels_before = len(label_distribution)\n    print(f\"\\nStep 4: Filtering infrequent labels (min frequency: {min_label_freq})\")\n    print(f\"Total unique labels before filtering: {total_labels_before}\")\n\n    # Labels to keep based on frequency\n    frequent_labels = [label for label, count in label_distribution.items() if count >= min_label_freq]\n    labels_removed = total_labels_before - len(frequent_labels)\n    print(f\"Removed {labels_removed} infrequent labels ({labels_removed/total_labels_before*100:.2f}% of labels)\")\n    print(f\"Number of labels remaining: {len(frequent_labels)} ({len(frequent_labels)/total_labels_before*100:.2f}% of labels)\")\n\n    # Filter out infrequent labels\n    before_label_filter = len(labels)\n    filtered_labels = labels.apply(lambda x: [label for label in x if label in frequent_labels])\n    \n    # Count samples that have no labels after filtering\n    empty_labels_mask = filtered_labels.apply(len) > 0\n    empty_labels_count = (~empty_labels_mask).sum()\n    if empty_labels_count > 0:\n        print(f\"Warning: {empty_labels_count} samples ({empty_labels_count/before_label_filter*100:.2f}%) now have no labels due to label frequency filtering\")\n        # Remove samples with no labels\n        filtered_labels = filtered_labels[empty_labels_mask]\n        texts = texts[empty_labels_mask]\n        print(f\"Removed {empty_labels_count} samples with no labels\")\n    \n    print(f\"Samples remaining after label filtering: {len(filtered_labels)} ({len(filtered_labels)/before_label_filter*100:.2f}% of data)\")\n\n    # Get label combinations distribution\n    label_combinations = Counter([tuple(sorted(labels)) for labels in filtered_labels])\n    total_combinations_before = len(label_combinations)\n    \n    print(f\"\\nStep 5: Filtering infrequent label combinations (min frequency: {min_label_comb_freq})\")\n    print(f\"Total unique label combinations before filtering: {total_combinations_before}\")\n    \n    frequent_combinations = {labels: count for labels, count in label_combinations.items() if count >= min_label_comb_freq}\n    combinations_removed = total_combinations_before - len(frequent_combinations)\n    print(f\"Removed {combinations_removed} infrequent label combinations ({combinations_removed/total_combinations_before*100:.2f}% of combinations)\")\n    print(f\"Number of label combinations remaining: {len(frequent_combinations)} ({len(frequent_combinations)/total_combinations_before*100:.2f}% of combinations)\")\n    \n    # Create mask for samples with frequent label combinations (if min_label_comb_freq > 0)\n    if min_label_comb_freq > 0:\n        before_comb_filter = len(filtered_labels)\n        comb_mask = filtered_labels.apply(lambda x: tuple(sorted(x)) in frequent_combinations)\n        samples_removed_by_comb = before_comb_filter - comb_mask.sum()\n        print(f\"Removed {samples_removed_by_comb} samples with infrequent label combinations ({samples_removed_by_comb/before_comb_filter*100:.2f}% of data)\")\n        print(f\"Samples remaining after combination filtering: {comb_mask.sum()} ({comb_mask.sum()/before_comb_filter*100:.2f}% of data)\")\n    else:\n        comb_mask = pd.Series([True] * len(filtered_labels))\n    \n    # Filter by label length\n    print(f\"\\nStep 6: Filtering samples with too many labels (max labels per sample: {max_label_len})\")\n    before_length_filter = len(filtered_labels)\n    label_length = filtered_labels.apply(len)\n    length_mask = (label_length > 0) & (label_length <= max_label_len)\n    samples_removed_by_length = before_length_filter - length_mask.sum()\n    print(f\"Removed {samples_removed_by_length} samples with too many or zero labels ({samples_removed_by_length/before_length_filter*100:.2f}% of data)\")\n    \n    # Combine both masks\n    final_mask = comb_mask & length_mask\n    \n    # Now get the final filtered texts and labels\n    texts = texts[final_mask].reset_index(drop=True)\n    filtered_labels = filtered_labels[final_mask].reset_index(drop=True)\n    \n    print(f\"\\n=== FINAL PREPROCESSING RESULTS ===\")\n    print(f\"Original dataset size: {initial_size}\")\n    print(f\"Final dataset size: {len(filtered_labels)} ({len(filtered_labels)/initial_size*100:.2f}% of original data)\")\n    print(f\"Total samples removed: {initial_size - len(filtered_labels)} ({(initial_size - len(filtered_labels))/initial_size*100:.2f}% of original data)\")\n    \n    return texts, filtered_labels\n\n# Add hybrid feature selection function\ndef hybrid_feature_selection(texts, labels_encoded, mlb, top_k_filter=20, top_k_final=10, vectorizer=None, random_seed=42, wrapper_method='rf'):\n    \"\"\"\n    Perform hybrid feature selection using both filter and wrapper methods.\n    \n    Args:\n        texts (pd.Series): Series of text data\n        labels_encoded (np.array): One-hot encoded labels\n        mlb (MultiLabelBinarizer): Label encoder used for transforming labels\n        top_k_filter (int): Number of labels to retain after filter stage\n        top_k_final (int): Final number of labels to select\n        vectorizer (object): Text vectorizer with fit_transform method. If None, uses simple word count\n        random_seed (int): Random seed for reproducibility\n        wrapper_method (str): Wrapper method to use ('rf' for Random Forest or 'lr' for Logistic Regression)\n        \n    Returns:\n        tuple: Selected indices, selected label names, and feature importance scores\n    \"\"\"\n    print(f\"Starting hybrid feature selection to select {top_k_final} out of {labels_encoded.shape[1]} labels...\")\n    \n    # If no vectorizer provided, create a simple one using sklearn's CountVectorizer\n    if vectorizer is None:\n        from sklearn.feature_extraction.text import CountVectorizer\n        vectorizer = CountVectorizer(max_features=5000)\n    \n    # Transform texts to feature vectors\n    print(\"Vectorizing text data...\")\n    X_vec = vectorizer.fit_transform(texts)\n    \n    # STEP 1: Filter Method - Use chi-square test and mutual information\n    print(\"Applying filter methods...\")\n    \n    # Store scores from multiple filter methods\n    feature_scores = np.zeros(labels_encoded.shape[1])\n    \n    # Chi-square test for each label\n    for i in range(labels_encoded.shape[1]):\n        chi_scores = chi2(X_vec, labels_encoded[:, i])\n        feature_scores[i] += chi_scores[0].mean()  # Add chi-square statistic\n    \n    # Mutual information for each label\n    for i in range(labels_encoded.shape[1]):\n        mi_score = mutual_info_classif(X_vec, labels_encoded[:, i], random_state=random_seed)\n        feature_scores[i] += mi_score.mean() * 10  # Scale and add MI score\n    \n    # Get top-k features from filter methods\n    filter_selected_indices = np.argsort(-feature_scores)[:top_k_filter]\n    filter_selected_labels = np.array(mlb.classes_)[filter_selected_indices]\n    \n    print(f\"Filter stage selected {len(filter_selected_indices)} labels\")\n    \n    # STEP 2: Wrapper Method - Use specified model to evaluate feature subsets\n    print(f\"Applying wrapper method using {wrapper_method.upper()}...\")\n    \n    # Initialize the appropriate model based on wrapper_method\n    if wrapper_method.lower() == 'rf':\n        model = RandomForestClassifier(n_estimators=100, random_state=random_seed, n_jobs=-1)\n    elif wrapper_method.lower() == 'lr':\n        model = LogisticRegression(random_state=random_seed, max_iter=1000)\n    else:\n        raise ValueError(f\"Unsupported wrapper method: {wrapper_method}. Use 'rf' or 'lr'.\")\n    \n    X_filtered = labels_encoded[:, filter_selected_indices]\n    \n    # For wrapper method, we'll create a matrix where each sample is label presence/absence\n    # and the target is other labels - a proxy for how well each label predicts others\n    importance_scores = np.zeros(len(filter_selected_indices))\n    \n    # For each label, train a model to predict it using the other labels\n    for i in tqdm(range(len(filter_selected_indices)), desc=\"Wrapper evaluation\"):\n        # Current target label\n        y = X_filtered[:, i]\n        \n        # Features (other labels)\n        X_others = np.delete(X_filtered, i, axis=1)\n        \n        # Train model\n        model.fit(X_others, y)\n        \n        # Score based on model performance\n        accuracy = model.score(X_others, y)\n        importance_scores[i] = accuracy\n    \n    # STEP 3: Combine scores to select final features\n    final_scores = 0.6 * feature_scores[filter_selected_indices] + 0.4 * importance_scores\n    final_selected_indices = filter_selected_indices[np.argsort(-final_scores)[:top_k_final]]\n    final_selected_labels = np.array(mlb.classes_)[final_selected_indices]\n    \n    print(f\"Final selection: {len(final_selected_labels)} labels\")\n    print(\"Selected labels:\", final_selected_labels)\n    \n    return final_selected_indices, final_selected_labels, final_scores\n\ndef extract_embeddings_and_apply_smote(model, dataloader, device, k_neighbors=5, random_state=42):\n    \"\"\"\n    Extract embeddings from the DeBERTa model and apply SMOTE for data augmentation.\n    Focuses on balancing specific area labels based on their frequencies.\n    \n    Args:\n        model (DeBERTaClassifier): The model to extract embeddings from\n        dataloader (DataLoader): DataLoader containing the training data\n        device (torch.device): Device to run the model on\n        k_neighbors (int): Number of neighbors to use for SMOTE\n        random_state (int): Random seed for reproducibility\n        \n    Returns:\n        tuple: (augmented_embeddings, augmented_labels) - the balanced dataset after SMOTE\n    \"\"\"\n    print(\"Extracting embeddings for SMOTE augmentation...\")\n    all_embeddings = []\n    all_labels = []\n    \n    model.eval()  # Set model to evaluation mode\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels']\n            \n            # Extract embeddings\n            embeddings = model.get_embeddings(input_ids, attention_mask)\n            \n            all_embeddings.append(embeddings.cpu().numpy())\n            all_labels.append(labels.numpy())\n    \n    # Concatenate all batches\n    embeddings_array = np.vstack(all_embeddings)\n    labels_array = np.vstack(all_labels)\n    \n    print(f\"Extracted embeddings shape: {embeddings_array.shape}\")\n    print(f\"Labels shape: {labels_array.shape}\")\n    \n    # Define the specific area labels and their frequencies\n    area_labels = {\n        \"area/kubelet\": 352,\n        \"area/test\": 297,\n        \"area/apiserver\": 204,\n        \"area/cloudprovider\": 178,\n        \"area/kubectl\": 134,\n        \"area/provider/azure\": 66,\n        \"area/dependency\": 63,\n        \"area/code-generation\": 47,\n        \"area/ipvs\": 41,\n        \"area/kubeadm\": 39,\n        \"area/kube-proxy\": 27,\n        \"area/provider/gcp\": 22,\n        \"area/e2e-test-framework\": 17,\n        \"area/conformance\": 16,\n        \"area/custom-resources\": 15,\n        \"area/release-eng\": 14,\n        \"area/security\": 10,\n        \"area/etcd\": 5,\n        \"area/provider/openstack\": 5,\n        \"area/provider/vmware\": 2\n    }\n    \n    # Get the class indices from the label encoder\n    label_encoder_classes = dataloader.dataset.labels[0].shape[0]\n    \n    # For multi-label data, we'll apply SMOTE for each label separately\n    # This approach handles class imbalance for each label independently\n    augmented_embeddings = embeddings_array.copy()\n    augmented_labels = labels_array.copy()\n    \n    # Calculate class distribution before augmentation\n    class_counts_before = labels_array.sum(axis=0)\n    \n    # Match area labels to their indices\n    if hasattr(dataloader.dataset, 'mlb') and hasattr(dataloader.dataset.mlb, 'classes_'):\n        mlb_classes = dataloader.dataset.mlb.classes_\n    else:\n        # If we don't have direct access to classes, try to infer from labels\n        print(\"Warning: Could not access label encoder classes directly.\")\n        mlb_classes = [f\"class_{i}\" for i in range(labels_array.shape[1])]\n    \n    # Map area labels to their indices and filter to only include these specific labels\n    target_indices = []\n    for i, class_name in enumerate(mlb_classes):\n        if class_name in area_labels:\n            target_indices.append((i, class_name, area_labels[class_name]))\n    \n    if not target_indices:\n        print(\"Warning: None of the specified area labels were found in the dataset. Falling back to all labels.\")\n        # Fall back to all labels\n        target_indices = [(i, f\"class_{i}\", class_counts_before[i]) for i in range(labels_array.shape[1])]\n    \n    # Sort by frequency to handle rare classes first\n    target_indices.sort(key=lambda x: x[2])\n    \n    # Get the frequency of the most common class\n    max_frequency = max(item[2] for item in target_indices)\n    \n    print(\"\\nClass distribution before augmentation:\")\n    for idx, class_name, freq in target_indices:\n        print(f\"  {class_name}: {int(class_counts_before[idx])} samples ({class_counts_before[idx]/len(labels_array)*100:.2f}%)\")\n    \n    print(\"\\nApplying SMOTE augmentation for target labels...\")\n    \n    for idx, class_name, orig_freq in target_indices:\n        # Skip the most frequent classes\n        if orig_freq > max_frequency * 0.5:\n            print(f\"  Skipping {class_name}: Already has {int(class_counts_before[idx])} samples (>50% of max frequency)\")\n            continue\n            \n        # Get current label column\n        y = labels_array[:, idx]\n        \n        # Check if label is imbalanced (fewer positives than negatives)\n        pos_count = y.sum()\n        neg_count = len(y) - pos_count\n        \n        # Only apply SMOTE if positive class is minority\n        if pos_count < neg_count:\n            print(f\"  Processing {class_name}: Positive samples {int(pos_count)}/{len(y)} ({pos_count/len(y)*100:.2f}%)\")\n            \n            # Calculate target ratio based on frequency\n            # For very rare classes (< 10% of max), aim for 40% of max frequency\n            # For rare classes (10-30% of max), aim for 30% of max frequency\n            # For less rare classes (30-50% of max), aim for 20% of max frequency\n            if orig_freq < max_frequency * 0.1:\n                target_ratio = 0.4  # Very rare classes\n            elif orig_freq < max_frequency * 0.3:\n                target_ratio = 0.3  # Rare classes\n            else:\n                target_ratio = 0.2  # Less rare classes\n                \n            target_samples = int(max_frequency * target_ratio)\n            print(f\"    Target: {target_samples} samples ({target_ratio*100:.0f}% of max frequency)\")\n            \n            try:\n                # Apply SMOTE to generate synthetic samples\n                # Ensure k_neighbors is less than the minority class count\n                k = min(k_neighbors, int(pos_count) - 1)\n                k = max(1, k)  # Ensure k is at least 1\n                \n                # Use sampling_strategy as a ratio to control how many samples to generate\n                # Higher ratio = more synthetic samples\n                sampling_ratio = min(1.0, target_samples / neg_count)\n                \n                smote = SMOTE(sampling_strategy=sampling_ratio,\n                             k_neighbors=k,\n                             random_state=random_state)\n                \n                # Use embeddings as features, the current label as target\n                X_resampled, y_resampled = smote.fit_resample(embeddings_array, y)\n                \n                # Get only the newly generated samples (they come after the original samples)\n                new_samples_mask = len(embeddings_array) < np.arange(len(X_resampled))\n                new_embeddings = X_resampled[new_samples_mask]\n                new_y = y_resampled[new_samples_mask]\n                \n                if len(new_embeddings) > 0:\n                    # Create labels for new samples (initially all zeros)\n                    new_labels = np.zeros((len(new_embeddings), labels_array.shape[1]))\n                    # Set current label to 1 for all new samples\n                    new_labels[:, idx] = 1\n                    \n                    # Add new samples to augmented dataset\n                    augmented_embeddings = np.vstack([augmented_embeddings, new_embeddings])\n                    augmented_labels = np.vstack([augmented_labels, new_labels])\n                    \n                    print(f\"    Added {len(new_embeddings)} synthetic samples\")\n            except ValueError as e:\n                print(f\"    Error applying SMOTE: {str(e)}\")\n                if \"Expected n_neighbors <= n_samples\" in str(e):\n                    print(f\"    Not enough positive samples for SMOTE (need at least k+1={k+1})\")\n    \n    # Calculate class distribution after augmentation\n    class_counts_after = augmented_labels.sum(axis=0)\n    print(\"\\nClass distribution after augmentation:\")\n    for idx, class_name, _ in target_indices:\n        before = class_counts_before[idx]\n        after = class_counts_after[idx]\n        print(f\"  {class_name}: {int(before)}  {int(after)} samples ({int(after-before)} added, {after/len(augmented_labels)*100:.2f}%)\")\n    \n    print(f\"Final augmented dataset size: {len(augmented_embeddings)} samples \" +\n          f\"({len(augmented_embeddings)-len(embeddings_array)} synthetic samples added)\")\n    \n    return augmented_embeddings, augmented_labels\n\nclass EmbeddingDataset(Dataset):\n    \"\"\"\n    Dataset for handling pre-extracted embeddings and labels.\n    \n    Args:\n        embeddings (np.ndarray): Pre-extracted embeddings\n        labels (np.ndarray): Corresponding labels\n    \"\"\"\n    def __init__(self, embeddings, labels):\n        self.embeddings = embeddings\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.embeddings)\n    \n    def __getitem__(self, idx):\n        embedding = torch.tensor(self.embeddings[idx], dtype=torch.float)\n        label = torch.tensor(self.labels[idx], dtype=torch.float)\n        \n        return {\n            'embedding': embedding,\n            'labels': label\n        }\n\ndef train_epoch_with_embeddings(model, dataloader, criterion, optimizer, device, threshold=0.5, early_stopping=None, epoch=1):\n    \"\"\"\n    Train the model for one epoch using pre-computed embeddings.\n    This training only updates the classification layer weights.\n    \n    Args:\n        model (nn.Module): The multi-label classification model\n        dataloader (DataLoader): Training DataLoader with embeddings\n        criterion: Loss function (BCEWithLogitsLoss)\n        optimizer: Optimization algorithm\n        device: Device to perform training (CPU or GPU)\n        threshold (float): Threshold for binary predictions (default is 0.5)\n        early_stopping (EarlyStopping, optional): Instance to monitor improvement in loss\n        epoch (int): Current epoch number for adaptive weighting\n        \n    Returns:\n        tuple: Average loss, Hamming accuracy, and a flag indicating if early stopping was triggered\n    \"\"\"\n    model.train()\n    \n    # Explicitly set classifier to training mode and ensure gradients are enabled\n    model.classifier.train()\n    for param in model.classifier.parameters():\n        param.requires_grad = True\n        \n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    # Track positive predictions to monitor class balance\n    pos_pred_rate = 0\n    \n    # Add dynamic weighting based on epoch\n    # First few epochs - boost positives more to prevent all-zero predictions\n    # Later epochs - gradually reduce weighting for more balanced predictions\n    pos_weight_factor = max(5.0 - 0.3 * epoch, 2.0)  # Starts at 5.0, decreases to minimum of 2.0\n    neg_weight_factor = min(0.5 + 0.025 * epoch, 0.8)  # Starts at 0.5, increases to maximum of 0.8\n    \n    print(f\"Using dynamic weighting: positive={pos_weight_factor:.2f}x, negative={neg_weight_factor:.2f}x\")\n    \n    for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training (embeddings)\")):\n        embeddings = batch['embedding'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        \n        # Apply dropout to embeddings (no gradient tracking here)\n        with torch.no_grad():\n            embeddings_with_dropout = model.dropout(embeddings)\n        \n        # Forward pass through classifier - WITH gradient tracking\n        outputs = model.classifier(embeddings_with_dropout)\n        \n        # Apply focal loss modifier to upweight rare positives\n        # This helps prevent the model from converging to all zeros\n        pos_weight = (labels == 0).float() * neg_weight_factor + (labels == 1).float() * pos_weight_factor\n        weighted_loss = criterion(outputs, labels) * pos_weight\n        loss = weighted_loss.mean()\n        \n        loss.backward()\n        \n        # Verify gradients are flowing (only on first batch)\n        if batch_idx == 0:\n            has_grad = any(p.grad is not None and p.grad.abs().sum().item() > 0 for p in model.classifier.parameters())\n            if not has_grad:\n                print(\"WARNING: No gradients flowing to classifier!\")\n            else:\n                print(\" Gradients are flowing to classifier\")\n        \n        # Add noise to gradients (helps escape local minima)\n        if epoch < 5:  # Only in early epochs\n            for p in model.classifier.parameters():\n                if p.grad is not None:\n                    noise = 0.01 * torch.randn_like(p.grad) * p.grad.std()\n                    p.grad += noise\n        \n        # Calculate positive prediction rate for monitoring\n        with torch.no_grad():\n            pos_preds = (torch.sigmoid(outputs) > threshold).float()\n            pos_pred_rate += pos_preds.mean().item()\n                \n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Apply sigmoid and threshold for predictions\n        predictions = torch.sigmoid(outputs) >= threshold\n        all_preds.append(predictions.cpu().detach().numpy())\n        all_labels.append(labels.cpu().detach().numpy())\n    \n    # Print classifier gradient magnitudes to verify training\n    grad_norms = [p.grad.norm().item() if p.grad is not None else 0 \n                  for p in model.classifier.parameters()]\n    if len(grad_norms) > 0:\n        print(f\"Classifier gradient norms: mean={np.mean(grad_norms):.6f}, max={max(grad_norms):.6f}\")\n    else:\n        print(\"WARNING: No gradients in classifier parameters!\")\n    \n    # Print positive prediction rate\n    avg_pos_pred_rate = pos_pred_rate / len(dataloader)\n    print(f\"Positive prediction rate: {avg_pos_pred_rate:.4f}\")\n    if avg_pos_pred_rate < 0.01:\n        print(\"WARNING: Very low positive prediction rate - model may be converging to all zeros\")\n    \n    # Calculate metrics for multi-label classification\n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n    \n    # Use subset accuracy (exact match) for a strict measure\n    exact_match = (all_preds == all_labels).all(axis=1).mean()\n    \n    avg_loss = total_loss / len(dataloader)\n    \n    if early_stopping:\n        early_stopping(avg_loss)\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered\")\n            return avg_loss, exact_match, True\n            \n    return avg_loss, exact_match, False\n\ndef validate_with_embeddings(model, dataloader, criterion, device, threshold=0.5):\n    \"\"\"\n    Evaluate the model using pre-computed embeddings.\n    \n    Args:\n        model (nn.Module): The multi-label classification model\n        dataloader (DataLoader): Validation DataLoader with embeddings\n        criterion: Loss function (BCEWithLogitsLoss)\n        device: Device to perform evaluation\n        threshold (float): Threshold for binary predictions (default is 0.5)\n        \n    Returns:\n        tuple: Average loss, various accuracy metrics, precision, recall, and F1 score\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            embeddings = batch['embedding'].to(device)\n            labels = batch['labels'].to(device)\n            \n            # Apply dropout to embeddings (same as in forward pass)\n            embeddings = model.dropout(embeddings)\n            \n            # Get outputs from classification layer\n            outputs = model.classifier(embeddings)\n            \n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            \n            # Apply sigmoid and threshold for predictions\n            predictions = (torch.sigmoid(outputs) >= threshold).float()\n            all_preds.append(predictions.cpu().numpy())\n            all_labels.append(labels.cpu().numpy())\n    \n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n    \n    # Calculate different multi-label metrics\n    \n    # 1. Exact Match / Subset Accuracy (all labels must be correct)\n    exact_match = (all_preds == all_labels).all(axis=1).mean()\n    \n    # 2. Partial Match Accuracy (only count correctly predicted 1s, ignore 0s)\n    # Calculate true positives per sample\n    true_positives = np.logical_and(all_preds == 1, all_labels == 1).sum(axis=1)\n    # Calculate total actual positives per sample\n    total_positives = (all_labels == 1).sum(axis=1)\n    # Handle division by zero - samples with no positive labels get a score of 0\n    partial_match = np.zeros_like(true_positives, dtype=float)\n    # Only calculate ratio for samples with at least one positive label\n    mask = total_positives > 0\n    partial_match[mask] = true_positives[mask] / total_positives[mask]\n    partial_match_accuracy = partial_match.mean()\n    \n    # 3. Jaccard Similarity (intersection over union)\n    def jaccard_score(y_true, y_pred):\n        intersection = np.logical_and(y_true, y_pred).sum(axis=1)\n        union = np.logical_or(y_true, y_pred).sum(axis=1)\n        # Create a float array for output to avoid type casting error\n        result = np.zeros_like(intersection, dtype=float)\n        # Avoid division by zero\n        np.divide(intersection, union, out=result, where=union!=0)\n        return np.mean(result)\n    \n    jaccard_sim = jaccard_score(all_labels.astype(bool), all_preds.astype(bool))\n    \n    # Add Hamming metric - this is the same as partial_match_accuracy\n    hamming_sim = partial_match_accuracy\n    \n    # Sample-based metrics - Each sample contributes equally regardless of number of labels\n    precision = precision_score(all_labels, all_preds, average='samples', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='samples', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='samples', zero_division=0)\n    \n    return (total_loss / len(dataloader), \n            {\"exact_match\": exact_match, \n             \"partial_match\": partial_match_accuracy,\n             \"hamming\": hamming_sim,\n             \"jaccard\": jaccard_sim}, \n            precision, recall, f1)\n\ndef main(args):\n    \"\"\"\n    Main function to run the multi-label classification pipeline with DeBERTa.\n    This function loads data, preprocesses it, trains the model, and evaluates performance.\n    \n    Includes data augmentation with SMOTE to balance class distribution.\n    \"\"\"\n    # Set random seeds for reproducibility\n    torch.manual_seed(42)\n    np.random.seed(42)\n    \n    # Check for GPU availability\n    n_gpus, gpu_ids = get_available_gpus()\n    if n_gpus >= 2:\n        print(f\"Using {n_gpus} GPUs: {gpu_ids}\")\n        device = torch.device(\"cuda\")\n        use_multi_gpu = True\n    elif n_gpus == 1:\n        print(\"Using 1 GPU\")\n        device = torch.device(\"cuda\")\n        use_multi_gpu = False\n    else:\n        print(\"No GPUs available, using CPU\")\n        device = torch.device(\"cpu\")\n        use_multi_gpu = False\n    \n    # Make results directory if it doesn't exist\n    results_dir = args.results_dir\n    os.makedirs(results_dir, exist_ok=True)\n    \n    # Create a timestamped directory for this run\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_dir = os.path.join(results_dir, f\"run_{timestamp}_{args.text_column}_augmented\")\n    os.makedirs(run_dir, exist_ok=True)\n    \n    # Load data\n    print(f\"Loading data from {args.data_path}...\")\n    df = pd.read_json(args.data_path)\n    \n    # Check if the text column exists\n    if args.text_column not in df.columns:\n        available_columns = [col for col in df.columns if col.startswith('all_text')]\n        print(f\"Text column '{args.text_column}' not found. Available text columns: {available_columns}\")\n        if len(available_columns) == 0:\n            raise ValueError(\"No text columns found in the data\")\n        args.text_column = available_columns[0]\n        print(f\"Using '{args.text_column}' instead\")\n    \n    # Load the tokenizer for token length calculations\n    print(\"Loading tokenizer...\")\n    tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n    \n    # Extract issue texts and labels\n    texts = df[args.text_column]\n    labels = df['labels'].apply(lambda x: x if isinstance(x, list) else [])  # Ensure labels are lists\n    \n    # Determine token length filtering threshold based on args\n    token_std_threshold = None\n    if args.token_length_filter == '3std':\n        token_std_threshold = 3.0\n    elif args.token_length_filter == '2std':\n        token_std_threshold = 2.0\n    \n    # Apply token length filtering first if requested\n    if token_std_threshold is not None or args.min_token_threshold is not None:\n        print(f\"\\nApplying token length filtering...\")\n        token_lengths = calculate_token_lengths(texts, tokenizer)\n        \n        # First filter by standard deviation, then by min threshold (in sequence)\n        filtered_texts, token_mask = filter_outliers_by_token_length(\n            texts, \n            token_lengths, \n            std_threshold=token_std_threshold if token_std_threshold is not None else float('inf'),\n            min_token_threshold=args.min_token_threshold\n        )\n        \n        # Apply same filter to labels and dataframe - keep original indices\n        filtered_labels = labels[token_mask]\n        filtered_df = df[token_mask]\n        \n        # Now reset indices for further processing\n        texts = filtered_texts.reset_index(drop=True)\n        labels = filtered_labels.reset_index(drop=True)\n        filtered_df = filtered_df.reset_index(drop=True)\n    else:\n        filtered_df = df\n    \n    # Apply token reduction if requested (after outlier removal)\n    if args.token_reduction_strategy:\n        print(f\"\\nApplying token reduction strategy: {args.token_reduction_strategy}\")\n        texts = process_with_token_reduction(\n            texts, \n            tokenizer, \n            max_length=args.max_length, \n            strategy=args.token_reduction_strategy\n        )\n        # Update filtered_df with the reduced texts\n        filtered_df[args.text_column] = texts\n    \n    # Use prepare_data function to filter and prepare data, but skip token length filtering since we've done it\n    texts, filtered_labels = prepare_data(\n        filtered_df,\n        text_column=args.text_column,\n        min_label_freq=args.min_label_freq, \n        max_label_len=args.max_label_len, \n        min_label_comb_freq=args.min_label_comb_freq,\n        tokenizer=tokenizer,\n        token_std_threshold=None,  # Set to None to skip the token filtering in prepare_data\n        min_token_threshold=args.min_token_threshold\n    )\n    \n    # Print final dataset statistics\n    print(\"\\n=== FINAL DATASET STATISTICS ===\")\n    print(f\"Initial dataset size: {len(df)}\")\n    print(f\"Final dataset size: {len(texts)}\")\n    print(f\"Total samples removed: {len(df) - len(texts)} ({(len(df) - len(texts))/len(df)*100:.2f}% of original data)\")\n    \n    # Count the number of labels distribution\n    label_distribution = Counter([label for labels in filtered_labels for label in labels])\n    print('\\nLabel Distribution:')\n    for i, (label, count) in enumerate(sorted(label_distribution.items(), key=lambda x: x[1], reverse=True)):\n        print(f'{i}. {label}: {count}')\n    \n    # Count the label length distribution\n    label_length_distribution = Counter([len(labels) for labels in filtered_labels])\n    print('\\nLabel count per row distribution:')\n    for label in sorted(label_length_distribution.keys()):\n        print(f'Label: {label}, count: {label_length_distribution[label]}')\n    \n    # Save preprocessing metadata\n    preprocessing_metadata = {\n        'initial_dataset_size': len(df),\n        'final_dataset_size': len(texts),\n        'token_reduction': {\n            'applied': args.token_reduction_strategy is not None,\n            'strategy': args.token_reduction_strategy if args.token_reduction_strategy else None,\n            'max_length': args.max_length\n        },\n        'token_length_filtering': {\n            'applied': token_std_threshold is not None,\n            'threshold': token_std_threshold\n        },\n        'label_filtering': {\n            'min_label_freq': args.min_label_freq,\n            'max_label_len': args.max_label_len,\n            'min_label_comb_freq': args.min_label_comb_freq\n        },\n        'min_token_threshold': {\n            'applied': args.min_token_threshold is not None,\n            'threshold': args.min_token_threshold\n        },\n        'data_augmentation': {\n            'enabled': args.use_data_augmentation,\n            'augmentation_method': 'SMOTE'\n        }\n    }\n    \n    # Calculate and add max token length to metadata\n    if tokenizer is not None:\n        token_lengths = calculate_token_lengths(texts, tokenizer)\n        max_token_length = int(token_lengths.max())\n        preprocessing_metadata['token_stats'] = {\n            'max_token_length': max_token_length,\n            'mean_token_length': float(token_lengths.mean()),\n            'median_token_length': float(token_lengths.median())\n        }\n        print(f\"\\n=== TOKEN LENGTH SUMMARY ===\")\n        print(f\"Maximum token length: {max_token_length}\")\n        print(f\"Mean token length: {token_lengths.mean():.2f}\")\n        print(f\"Median token length: {token_lengths.median():.2f}\")\n    \n    with open(os.path.join(run_dir, 'preprocessing_metadata.json'), 'w') as f:\n        json.dump(preprocessing_metadata, f, indent=4)\n    \n    # Encode multi-labels using MultiLabelBinarizer\n    print(\"Encoding labels...\")\n    mlb = MultiLabelBinarizer()\n    labels_encoded = mlb.fit_transform(filtered_labels)\n    \n    # Save all original label classes\n    all_classes = mlb.classes_.tolist()\n    \n    # Save label encoder for future use\n    with open(os.path.join(run_dir, 'label_encoder.json'), 'w') as f:\n        json.dump({\n            'classes': all_classes\n        }, f)\n    \n    # Calculate label distribution\n    label_counts = labels_encoded.sum(axis=0)\n    \n    # Log class imbalance metrics\n    label_density = label_counts.sum() / (labels_encoded.shape[0] * labels_encoded.shape[1])\n    print(f\"Label density: {label_density:.4f}\")\n    print(f\"Average labels per sample: {label_counts.sum() / labels_encoded.shape[0]:.2f}\")\n    \n    # Print hybrid feature selection args\n    print(f\"Feature selection enabled: {args.feature_selection}\")\n    if args.feature_selection:\n        print(f\"Filter top-k: {args.filter_k}, Final top-k: {args.final_k}\")\n        print(f\"Wrapper method: {args.wrapper_method.upper()}\")\n    else:\n        print(\"Feature selection disabled\")\n        \n    # Perform hybrid feature selection if enabled\n    if args.feature_selection:\n        print(f\"\\nPerforming hybrid feature selection...\")\n        \n        # Create appropriate vectorizer based on argument\n        if args.vectorizer == 'tfidf':\n            from sklearn.feature_extraction.text import TfidfVectorizer\n            vectorizer = TfidfVectorizer(max_features=5000)\n            print(\"Using TF-IDF vectorizer for feature selection\")\n        else:  # default to count\n            from sklearn.feature_extraction.text import CountVectorizer\n            vectorizer = CountVectorizer(max_features=5000)\n            print(\"Using Count vectorizer for feature selection\")\n        \n        selected_indices, selected_labels, feature_scores = hybrid_feature_selection(\n            texts, labels_encoded, mlb, \n            top_k_filter=args.filter_k,\n            top_k_final=args.final_k,\n            vectorizer=vectorizer,\n            random_seed=42,\n            wrapper_method=args.wrapper_method\n        )\n        \n        # Filter labels_encoded to keep only selected labels\n        labels_encoded = labels_encoded[:, selected_indices]\n        \n        # Save selected labels to file\n        with open(os.path.join(run_dir, 'selected_labels.json'), 'w') as f:\n            json.dump({\n                'selected_labels': selected_labels.tolist(),\n                'feature_scores': feature_scores.tolist(),\n                'selected_indices': selected_indices.tolist(),\n                'vectorizer_type': args.vectorizer,\n                'wrapper_method': args.wrapper_method\n            }, f)\n        \n        # Update mlb.classes_ to only contain selected classes\n        mlb.classes_ = np.array(selected_labels)\n        \n        # Recalculate label counts with selected labels\n        label_counts = labels_encoded.sum(axis=0)\n        print(f\"Training with {len(selected_labels)} selected labels: {selected_labels}\")\n    else:\n        print(\"Feature selection disabled, using all labels\")\n    \n    # Split data into training and validation sets (80% training, 20% validation)\n    split_idx = int(len(texts) * 0.8)\n    train_texts, val_texts = texts[:split_idx], texts[split_idx:]\n    train_labels, val_labels = labels_encoded[:split_idx], labels_encoded[split_idx:]\n    \n    print(f\"Training samples: {len(train_texts)}, Validation samples: {len(val_texts)}\")\n    \n    # Initialize tokenizer\n    print(\"Loading tokenizer...\")\n    tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n    \n    # Implement class weights for loss function to handle imbalance\n    pos_weights = None\n    if args.use_class_weights and label_counts.min() < label_counts.max() / 5:  # If there's significant imbalance\n        print(\"Computing class weights for imbalanced labels...\")\n        pos_weights = torch.FloatTensor(\n            (labels_encoded.shape[0] - label_counts) / label_counts\n        ).clamp(0.5, 10).to(device)  # Limit range to prevent extreme weights\n    \n    # Create datasets and dataloaders\n    batch_size = args.batch_size\n    \n    # Create original datasets for getting embeddings\n    train_dataset = IssueDataset(train_texts, train_labels, tokenizer, max_length=args.max_length)\n    val_dataset = IssueDataset(val_texts, val_labels, tokenizer, max_length=args.max_length)\n    \n    # Increase batch size for DataParallel if multiple GPUs\n    if use_multi_gpu:\n        batch_size = batch_size * n_gpus\n        print(f\"Using larger batch size of {batch_size} for {n_gpus} GPUs\")\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)  # Don't shuffle yet\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    \n    # Initialize model\n    print(\"Initializing model...\")\n    model = DeBERTaClassifier(num_labels=len(mlb.classes_)).to(device)\n    \n    # Use DataParallel for multiple GPUs\n    if use_multi_gpu:\n        model = nn.DataParallel(model)\n        print(\"Model wrapped in DataParallel\")\n    \n    # Extract embeddings and apply SMOTE augmentation if enabled\n    if args.use_data_augmentation:\n        print(\"\\n=== APPLYING DATA AUGMENTATION WITH SMOTE ===\")\n        \n        # Extract embeddings from training set and apply SMOTE\n        augmented_embeddings, augmented_labels = extract_embeddings_and_apply_smote(\n            model.module if use_multi_gpu else model,\n            train_loader,\n            device,\n            k_neighbors=5,\n            random_state=42\n        )\n        \n        # Create a new dataset with the augmented data\n        train_embedding_dataset = EmbeddingDataset(augmented_embeddings, augmented_labels)\n        augmented_train_loader = DataLoader(train_embedding_dataset, batch_size=batch_size, shuffle=True)\n        \n        # Also extract embeddings for validation set (no augmentation)\n        print(\"\\nExtracting embeddings for validation set...\")\n        val_embeddings = []\n        val_labels_list = []\n        \n        model.eval()\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Extracting validation embeddings\"):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                \n                # Extract embeddings\n                if use_multi_gpu:\n                    embeddings = model.module.get_embeddings(input_ids, attention_mask)\n                else:\n                    embeddings = model.get_embeddings(input_ids, attention_mask)\n                \n                val_embeddings.append(embeddings.cpu().numpy())\n                val_labels_list.append(batch['labels'].numpy())\n        \n        val_embeddings = np.vstack(val_embeddings)\n        val_labels_np = np.vstack(val_labels_list)\n        \n        val_embedding_dataset = EmbeddingDataset(val_embeddings, val_labels_np)\n        val_embedding_loader = DataLoader(val_embedding_dataset, batch_size=batch_size)\n        \n        # Set flags to use embedding-based training\n        use_embeddings = True\n        \n        # Save augmentation statistics to metadata\n        with open(os.path.join(run_dir, 'augmentation_stats.json'), 'w') as f:\n            json.dump({\n                'original_train_samples': len(train_texts),\n                'augmented_train_samples': len(augmented_embeddings),\n                'synthetic_samples_added': len(augmented_embeddings) - len(train_texts),\n                'augmentation_method': 'SMOTE'\n            }, f)\n    else:\n        print(\"Data augmentation disabled\")\n        use_embeddings = False\n    \n    # Use weighted loss if we have weights\n    if pos_weights is not None:\n        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n        print(\"Using weighted BCE loss\")\n    else:\n        criterion = nn.BCEWithLogitsLoss()\n    \n    # Using optimizer that only updates classifier parameters\n    if use_multi_gpu:\n        optimizer = torch.optim.AdamW(model.module.classifier.parameters(), \n                                    lr=args.learning_rate * 0.5,  # Higher learning rate (0.5x instead of 0.1x)\n                                    weight_decay=0.01)\n    else:\n        optimizer = torch.optim.AdamW(model.classifier.parameters(), \n                                    lr=args.learning_rate * 0.5,  # Higher learning rate (0.5x instead of 0.1x)\n                                    weight_decay=0.01)\n    \n    # Add learning rate scheduler for better convergence\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=2, verbose=True\n    )\n    \n    # Initialize early stopping\n    early_stopping = EarlyStopping(patience=args.patience, min_delta=0.01)\n    \n    # Training loop\n    num_epochs = args.epochs\n    print(f\"Starting training for {num_epochs} epochs...\")\n    print(f\"Training mode: {'Using pre-computed embeddings with augmentation' if use_embeddings else 'Standard training'}\")\n    \n    train_losses = []\n    val_losses = []\n    best_f1 = 0.0\n    best_model_saved = False  # Flag to track if we've saved at least one model\n    stuck_epochs = 0  # Counter for epochs with no improvement\n    \n    # Define model path\n    model_path = os.path.join(run_dir, f'best_model_{args.text_column}_augmented.pt')\n    \n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        \n        # Check for stuck training and reinitialize if needed\n        if epoch >= 5 and stuck_epochs >= 3:\n            print(\"Model seems stuck. Reinitializing classifier layer...\")\n            # Reinitialize the classifier layer with different initialization\n            if use_multi_gpu:\n                nn.init.xavier_normal_(model.module.classifier.weight)\n                if model.module.classifier.bias is not None:\n                    nn.init.zeros_(model.module.classifier.bias)\n            else:\n                nn.init.xavier_normal_(model.classifier.weight)\n                if model.classifier.bias is not None:\n                    nn.init.zeros_(model.classifier.bias)\n            \n            # Reset optimizer with higher learning rate\n            if use_multi_gpu:\n                optimizer = torch.optim.AdamW(\n                    model.module.classifier.parameters(),\n                    lr=args.learning_rate * 1.0,  # Full learning rate for reinitialization\n                    weight_decay=0.005\n                )\n            else:\n                optimizer = torch.optim.AdamW(\n                    model.classifier.parameters(),\n                    lr=args.learning_rate * 1.0,  # Full learning rate for reinitialization\n                    weight_decay=0.005\n                )\n            \n            # Reset scheduler\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer, mode='max', factor=0.5, patience=2, verbose=True\n            )\n            \n            stuck_epochs = 0  # Reset counter\n        \n        # Train for one epoch - choose appropriate training function based on mode\n        if use_embeddings:\n            train_loss, train_acc, stop_early = train_epoch_with_embeddings(\n                model.module if use_multi_gpu else model,\n                augmented_train_loader,\n                criterion,\n                optimizer,\n                device,\n                early_stopping=early_stopping,\n                epoch=epoch+1\n            )\n        else:\n            train_loss, train_acc, stop_early = train_epoch(\n                model,\n                train_loader,\n                criterion,\n                optimizer,\n                device,\n                early_stopping=early_stopping\n            )\n        \n        # Validate - choose appropriate validation function based on mode\n        if use_embeddings:\n            val_loss, accuracy_metrics, val_precision, val_recall, val_f1 = validate_with_embeddings(\n                model.module if use_multi_gpu else model,\n                val_embedding_loader,\n                criterion,\n                device\n            )\n        else:\n            val_loss, accuracy_metrics, val_precision, val_recall, val_f1 = validate(\n                model,\n                val_loader,\n                criterion,\n                device\n            )\n        \n        # Update scheduler based on F1 score\n        scheduler.step(val_f1)\n        \n        # Save metrics\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy (Exact Match): {train_acc:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}\")\n        print(f\"Val Accuracy (Exact Match): {accuracy_metrics['exact_match']:.4f}\")\n        print(f\"Val Accuracy (Partial Match): {accuracy_metrics['partial_match']:.4f}\")\n        print(f\"Val Accuracy (Jaccard): {accuracy_metrics['jaccard']:.4f}\")\n        print(f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}\")\n        \n        # Check for model improvement\n        improved = False\n        \n        # Save best model based on F1 score\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            improved = True\n            \n            # Save the model state_dict (handle DataParallel wrapper if needed)\n            if use_multi_gpu:\n                torch.save(model.module.state_dict(), model_path)\n            else:\n                torch.save(model.state_dict(), model_path)\n                \n            print(f\"Saved new best model to {model_path}\")\n            best_model_saved = True\n            stuck_epochs = 0  # Reset counter when we improve\n        else:\n            stuck_epochs += 1  # Increment counter when no improvement\n            print(f\"No improvement for {stuck_epochs} epochs. Best F1: {best_f1:.4f}\")\n        \n        # Always save a model for the first epoch if no model has been saved yet\n        # This ensures we have at least one model if early stopping occurs\n        if epoch == 0 and not best_model_saved:\n            if use_multi_gpu:\n                torch.save(model.module.state_dict(), model_path)\n            else:\n                torch.save(model.state_dict(), model_path)\n            print(f\"Saved initial model to {model_path} as baseline\")\n            best_model_saved = True\n            \n        # Check for early stopping\n        if stop_early:\n            print(\"Early stopping triggered. Terminating training.\")\n            break\n    \n    # Save training history\n    history = {\n        'train_losses': train_losses,\n        'val_losses': val_losses\n    }\n    with open(os.path.join(run_dir, 'training_history.json'), 'w') as f:\n        json.dump(history, f)\n    \n    # Load best model for final evaluation\n    print(\"\\n=== FINAL EVALUATION ===\")\n    best_model_path = os.path.join(run_dir, f'best_model_{args.text_column}_augmented.pt')\n    \n    # Handle loading for DataParallel model\n    if use_multi_gpu:\n        model.module.load_state_dict(torch.load(best_model_path))\n    else:\n        model.load_state_dict(torch.load(best_model_path))\n    \n    # Evaluate the model with default threshold\n    print(\"Final evaluation with best model:\")\n    if use_embeddings:\n        final_loss, final_acc_metrics, final_precision, final_recall, final_f1 = validate_with_embeddings(\n            model.module if use_multi_gpu else model,\n            val_embedding_loader,\n            criterion,\n            device\n        )\n    else:\n        final_loss, final_acc_metrics, final_precision, final_recall, final_f1 = validate(\n            model,\n            val_loader,\n            criterion,\n            device\n        )\n    \n    print(f\"Final Loss: {final_loss:.4f}\")\n    print(f\"Final Exact Match Accuracy: {final_acc_metrics['exact_match']:.4f}\")\n    print(f\"Final Partial Match Accuracy: {final_acc_metrics['partial_match']:.4f}\")\n    print(f\"Final Jaccard Similarity: {final_acc_metrics['jaccard']:.4f}\")\n    print(f\"Final Precision: {final_precision:.4f}\")\n    print(f\"Final Recall: {final_recall:.4f}\")\n    print(f\"Final F1 Score: {final_f1:.4f}\")\n    \n    # Update results dictionary with final metrics\n    results = {\n        'text_column': args.text_column,\n        'token_length_filter': args.token_length_filter,\n        'token_reduction_strategy': args.token_reduction_strategy,\n        'data_augmentation': {\n            'enabled': args.use_data_augmentation,\n            'method': 'SMOTE' if args.use_data_augmentation else None\n        },\n        'metrics': {\n            'exact_match': float(final_acc_metrics['exact_match']),\n            'partial_match': float(final_acc_metrics['partial_match']),\n            'jaccard': float(final_acc_metrics['jaccard']),\n            'precision': float(final_precision),\n            'recall': float(final_recall), \n            'f1': float(final_f1),\n        }\n    }\n    with open(os.path.join(run_dir, 'results.json'), 'w') as f:\n        json.dump(results, f, indent=4)\n    \n    print(f\"\\nTraining completed! Results saved to {run_dir}\")\n    \n    return {\n        'metrics': results['metrics'],\n        'model': model,\n        'label_encoder': mlb,\n        'results_dir': run_dir\n    }","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:37:59.985698Z","iopub.execute_input":"2025-04-28T08:37:59.986020Z","iopub.status.idle":"2025-04-28T08:38:00.126041Z","shell.execute_reply.started":"2025-04-28T08:37:59.985998Z","shell.execute_reply":"2025-04-28T08:38:00.125530Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Create parser and handle Jupyter/Colab environment by ignoring unknown args\n    parser = argparse.ArgumentParser(description='Train DeBERTa for multi-label classification')\n    \n    # Data parameters\n    parser.add_argument('--data_path', type=str, \n                        default=\"/kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json\",\n                        help='Path to the JSON data file')\n    parser.add_argument('--text_column', type=str, default='all_text_0.5',\n                        help='Column name with the text data to use for training (e.g., all_text, all_text_0.5)')\n    parser.add_argument('--results_dir', type=str, default='./results',\n                        help='Directory to save results')\n    \n    # Label filtering parameters\n    parser.add_argument('--min_label_freq', type=int, default=5,\n                        help='Minimum frequency for a label to be considered')\n    parser.add_argument('--max_label_len', type=int, default=5,\n                        help='Maximum number of labels per sample (default: 5)')\n    parser.add_argument('--min_label_comb_freq', type=int, default=2,\n                        help='Minimum frequency for a label combination')\n    \n    # Training parameters\n    parser.add_argument('--batch_size', type=int, default=16, help='Training batch size')\n    parser.add_argument('--epochs', type=int, default=200, help='Number of training epochs')\n    parser.add_argument('--learning_rate', type=float, default=2e-5, help='Learning rate')\n    parser.add_argument('--patience', type=int, default=100, help='Early stopping patience')\n    parser.add_argument('--use_class_weights', action='store_true', help='Use class weights for imbalanced data')\n    \n    # Token length parameters\n    parser.add_argument('--max_length', type=int, default=512, help='Maximum token length for model input')\n    \n    # Token length filtering parameters\n    parser.add_argument('--token_length_filter', type=str, choices=['2std', '3std', None], default='3std',\n                        help='Remove token length outliers based on standard deviation threshold')\n    parser.add_argument('--min_token_threshold', type=int, default=None,\n                        help='Minimum number of tokens required for a sample')\n    \n    # Token reduction parameters for handling long tokens\n    parser.add_argument('--token_reduction_strategy', type=str, \n                        choices=['simple', 'smart_truncation', 'extractive_summarization', 'hybrid'], \n                        default=None,\n                        help='Strategy to handle long tokens exceeding max_length: '\n                             'simple=simple truncation, '\n                             'smart_truncation=keep beginning and end, '\n                             'extractive_summarization=extract key sentences, '\n                             'hybrid=combine summarization and truncation')\n    \n    # Feature selection parameters\n    parser.add_argument('--feature_selection', action='store_true', #default=True, \n                        help='Enable hybrid feature selection')\n    parser.add_argument('--filter_k', type=int, default=20, \n                        help='Number of labels to retain after filter stage')\n    parser.add_argument('--final_k', type=int, default=15, \n                        help='Final number of labels to select')\n    parser.add_argument('--vectorizer', type=str, choices=['count', 'tfidf'], default='tfidf',\n                        help='Vectorizer to use for feature selection')\n    parser.add_argument('--wrapper_method', type=str, choices=['rf', 'lr'], default='rf',\n                        help='Wrapper method to use for feature selection (rf: Random Forest, lr: Logistic Regression)')\n    \n    # Data augmentation parameter\n    parser.add_argument('--use_data_augmentation', action='store_true', default=True,\n                        help='Enable data augmentation with SMOTE to balance class distribution')\n    \n    # Parse arguments, ignore unknown args for compatibility with Jupyter/Colab\n    args, unknown = parser.parse_known_args()\n    \n    # If the script is run directly, not imported\n    results = main(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:38:00.909236Z","iopub.execute_input":"2025-04-28T08:38:00.909511Z","iopub.status.idle":"2025-04-28T08:40:00.200303Z","shell.execute_reply.started":"2025-04-28T08:38:00.909492Z","shell.execute_reply":"2025-04-28T08:40:00.199613Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs: [0, 1]\nLoading data from /kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json...\nLoading tokenizer...\n\nApplying token length filtering...\n","output_type":"stream"},{"name":"stderr","text":"Calculating token lengths: 100%|| 1258/1258 [00:03<00:00, 373.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Token length statistics before filtering:\n  Mean: 379.62, Std Dev: 458.90\n  Min: 32, Max: 5493\n  25th percentile: 148.00\n  50th percentile (median): 245.00\n  75th percentile: 439.75\nApplied 3.0 std dev threshold: (1.00, 1756.33)\nRemoved 21 samples by std dev filtering (1.67% of data)\nTotal removed: 21 samples (1.67% of original data)\nRemaining: 1237 samples (98.33% of original data)\n\nToken length statistics after all filtering:\n  Mean: 336.26, Std Dev: 279.70\n  Min: 32, Max: 1725\n  25th percentile: 148.00\n  50th percentile (median): 241.00\n  75th percentile: 418.00\n\n=== DATA PREPROCESSING STATISTICS ===\nInitial dataset size: 1237\nDataset size after basic cleaning: 1237 (100.00% of original data)\n\nStep 4: Filtering infrequent labels (min frequency: 5)\nTotal unique labels before filtering: 39\nRemoved 19 infrequent labels (48.72% of labels)\nNumber of labels remaining: 20 (51.28% of labels)\nWarning: 15 samples (1.21%) now have no labels due to label frequency filtering\nRemoved 15 samples with no labels\nSamples remaining after label filtering: 1222 (98.79% of data)\n\nStep 5: Filtering infrequent label combinations (min frequency: 2)\nTotal unique label combinations before filtering: 108\nRemoved 57 infrequent label combinations (52.78% of combinations)\nNumber of label combinations remaining: 51 (47.22% of combinations)\nRemoved 57 samples with infrequent label combinations (4.66% of data)\nSamples remaining after combination filtering: 1165 (95.34% of data)\n\nStep 6: Filtering samples with too many labels (max labels per sample: 5)\nRemoved 21 samples with too many or zero labels (1.72% of data)\n\n=== FINAL PREPROCESSING RESULTS ===\nOriginal dataset size: 1237\nFinal dataset size: 1156 (93.45% of original data)\nTotal samples removed: 81 (6.55% of original data)\n\n=== FINAL DATASET STATISTICS ===\nInitial dataset size: 1258\nFinal dataset size: 1156\nTotal samples removed: 102 (8.11% of original data)\n\nLabel Distribution:\n0. area/kubelet: 352\n1. area/test: 297\n2. area/apiserver: 204\n3. area/cloudprovider: 178\n4. area/kubectl: 134\n5. area/provider/azure: 66\n6. area/dependency: 63\n7. area/code-generation: 47\n8. area/ipvs: 41\n9. area/kubeadm: 39\n10. area/kube-proxy: 27\n11. area/provider/gcp: 22\n12. area/e2e-test-framework: 17\n13. area/conformance: 16\n14. area/custom-resources: 15\n15. area/release-eng: 14\n16. area/security: 10\n17. area/etcd: 5\n18. area/provider/openstack: 5\n19. area/provider/vmware: 2\n\nLabel count per row distribution:\nLabel: 1, count: 870\nLabel: 2, count: 228\nLabel: 3, count: 24\nLabel: 4, count: 14\nLabel: 5, count: 20\n","output_type":"stream"},{"name":"stderr","text":"Calculating token lengths: 100%|| 1156/1156 [00:02<00:00, 504.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n=== TOKEN LENGTH SUMMARY ===\nMaximum token length: 1725\nMean token length: 325.18\nMedian token length: 231.50\nEncoding labels...\nLabel density: 0.0672\nAverage labels per sample: 1.34\nFeature selection enabled: False\nFeature selection disabled\nFeature selection disabled, using all labels\nTraining samples: 924, Validation samples: 232\nLoading tokenizer...\nUsing larger batch size of 32 for 2 GPUs\nInitializing model...\nModel wrapped in DataParallel\n\n=== APPLYING DATA AUGMENTATION WITH SMOTE ===\nExtracting embeddings for SMOTE augmentation...\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings: 100%|| 29/29 [00:58<00:00,  2.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Extracted embeddings shape: (924, 768)\nLabels shape: (924, 20)\nWarning: Could not access label encoder classes directly.\nWarning: None of the specified area labels were found in the dataset. Falling back to all labels.\n\nClass distribution before augmentation:\n  class_15: 0 samples (0.00%)\n  class_16: 0 samples (0.00%)\n  class_18: 4 samples (0.43%)\n  class_7: 5 samples (0.54%)\n  class_4: 6 samples (0.65%)\n  class_3: 11 samples (1.19%)\n  class_17: 14 samples (1.52%)\n  class_6: 16 samples (1.73%)\n  class_9: 22 samples (2.38%)\n  class_14: 22 samples (2.38%)\n  class_10: 24 samples (2.60%)\n  class_8: 29 samples (3.14%)\n  class_2: 42 samples (4.55%)\n  class_5: 57 samples (6.17%)\n  class_13: 63 samples (6.82%)\n  class_11: 107 samples (11.58%)\n  class_1: 139 samples (15.04%)\n  class_0: 162 samples (17.53%)\n  class_19: 270 samples (29.22%)\n  class_12: 277 samples (29.98%)\n\nApplying SMOTE augmentation for target labels...\n  Processing class_15: Positive samples 0/924 (0.00%)\n    Target: 110 samples (40% of max frequency)\n    Error applying SMOTE: The target 'y' needs to have more than 1 class. Got 1 class instead\n  Processing class_16: Positive samples 0/924 (0.00%)\n    Target: 110 samples (40% of max frequency)\n    Error applying SMOTE: The target 'y' needs to have more than 1 class. Got 1 class instead\n  Processing class_18: Positive samples 4/924 (0.43%)\n    Target: 110 samples (40% of max frequency)\n    Added 105 synthetic samples\n  Processing class_7: Positive samples 5/924 (0.54%)\n    Target: 110 samples (40% of max frequency)\n    Added 104 synthetic samples\n  Processing class_4: Positive samples 6/924 (0.65%)\n    Target: 110 samples (40% of max frequency)\n    Added 103 synthetic samples\n  Processing class_3: Positive samples 11/924 (1.19%)\n    Target: 110 samples (40% of max frequency)\n    Added 98 synthetic samples\n  Processing class_17: Positive samples 14/924 (1.52%)\n    Target: 110 samples (40% of max frequency)\n    Added 95 synthetic samples\n  Processing class_6: Positive samples 16/924 (1.73%)\n    Target: 110 samples (40% of max frequency)\n    Added 93 synthetic samples\n  Processing class_9: Positive samples 22/924 (2.38%)\n    Target: 110 samples (40% of max frequency)\n    Added 87 synthetic samples\n  Processing class_14: Positive samples 22/924 (2.38%)\n    Target: 110 samples (40% of max frequency)\n    Added 87 synthetic samples\n  Processing class_10: Positive samples 24/924 (2.60%)\n    Target: 110 samples (40% of max frequency)\n    Added 85 synthetic samples\n  Processing class_8: Positive samples 29/924 (3.14%)\n    Target: 83 samples (30% of max frequency)\n    Added 53 synthetic samples\n  Processing class_2: Positive samples 42/924 (4.55%)\n    Target: 83 samples (30% of max frequency)\n    Added 40 synthetic samples\n  Processing class_5: Positive samples 57/924 (6.17%)\n    Target: 83 samples (30% of max frequency)\n    Added 25 synthetic samples\n  Processing class_13: Positive samples 63/924 (6.82%)\n    Target: 83 samples (30% of max frequency)\n    Added 19 synthetic samples\n  Processing class_11: Positive samples 107/924 (11.58%)\n    Target: 55 samples (20% of max frequency)\n    Error applying SMOTE: The specified ratio required to remove samples from the minority class while trying to generate new samples. Please increase the ratio.\n  Skipping class_1: Already has 139 samples (>50% of max frequency)\n  Skipping class_0: Already has 162 samples (>50% of max frequency)\n  Skipping class_19: Already has 270 samples (>50% of max frequency)\n  Skipping class_12: Already has 277 samples (>50% of max frequency)\n\nClass distribution after augmentation:\n  class_15: 0  0 samples (0 added, 0.00%)\n  class_16: 0  0 samples (0 added, 0.00%)\n  class_18: 4  109 samples (105 added, 5.68%)\n  class_7: 5  109 samples (104 added, 5.68%)\n  class_4: 6  109 samples (103 added, 5.68%)\n  class_3: 11  109 samples (98 added, 5.68%)\n  class_17: 14  109 samples (95 added, 5.68%)\n  class_6: 16  109 samples (93 added, 5.68%)\n  class_9: 22  109 samples (87 added, 5.68%)\n  class_14: 22  109 samples (87 added, 5.68%)\n  class_10: 24  109 samples (85 added, 5.68%)\n  class_8: 29  82 samples (53 added, 4.28%)\n  class_2: 42  82 samples (40 added, 4.28%)\n  class_5: 57  82 samples (25 added, 4.28%)\n  class_13: 63  82 samples (19 added, 4.28%)\n  class_11: 107  107 samples (0 added, 5.58%)\n  class_1: 139  139 samples (0 added, 7.25%)\n  class_0: 162  162 samples (0 added, 8.45%)\n  class_19: 270  270 samples (0 added, 14.08%)\n  class_12: 277  277 samples (0 added, 14.44%)\nFinal augmented dataset size: 1918 samples (994 synthetic samples added)\n\nExtracting embeddings for validation set...\n","output_type":"stream"},{"name":"stderr","text":"Extracting validation embeddings: 100%|| 8/8 [00:13<00:00,  1.71s/it]\n","output_type":"stream"},{"name":"stdout","text":"Starting training for 200 epochs...\nTraining mode: Using pre-computed embeddings with augmentation\n\nEpoch 1/200\nUsing dynamic weighting: positive=4.70x, negative=0.53x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 405.63it/s]\n","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\nClassifier gradient norms: mean=0.601825, max=1.130166\nPositive prediction rate: 0.3865\nTrain Loss: 0.5315, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6738\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6458\nVal Accuracy (Jaccard): 0.1025\nVal Precision: 0.1059, Val Recall: 0.6458, Val F1: 0.1791\nSaved new best model to ./results/run_20250428_083800_all_text_0.5_augmented/best_model_all_text_0.5_augmented.pt\n\nEpoch 2/200\nUsing dynamic weighting: positive=4.40x, negative=0.55x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 423.02it/s]\n","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\nClassifier gradient norms: mean=0.620763, max=1.167662\nPositive prediction rate: 0.3646\nTrain Loss: 0.5227, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6576\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5328\nVal Accuracy (Jaccard): 0.0998\nVal Precision: 0.1042, Val Recall: 0.5328, Val F1: 0.1711\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 3/200\nUsing dynamic weighting: positive=4.10x, negative=0.57x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 417.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.621638, max=1.170756\nPositive prediction rate: 0.3256\nTrain Loss: 0.5140, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6418\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5328\nVal Accuracy (Jaccard): 0.0998\nVal Precision: 0.1042, Val Recall: 0.5328, Val F1: 0.1711\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 4/200\nUsing dynamic weighting: positive=3.80x, negative=0.60x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 412.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.653077, max=1.232497\nPositive prediction rate: 0.3053\nTrain Loss: 0.5055, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6265\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5328\nVal Accuracy (Jaccard): 0.0998\nVal Precision: 0.1042, Val Recall: 0.5328, Val F1: 0.1711\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 5/200\nUsing dynamic weighting: positive=3.50x, negative=0.62x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 429.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.600352, max=1.128990\nPositive prediction rate: 0.2880\nTrain Loss: 0.5004, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6189\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5328\nVal Accuracy (Jaccard): 0.0998\nVal Precision: 0.1042, Val Recall: 0.5328, Val F1: 0.1711\nNo improvement for 4 epochs. Best F1: 0.1791\n\nEpoch 6/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=3.20x, negative=0.65x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 451.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.801829, max=1.506977\nPositive prediction rate: 0.7262\nTrain Loss: 0.7501, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.9103\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.7673\nVal Accuracy (Jaccard): 0.0647\nVal Precision: 0.0649, Val Recall: 0.7673, Val F1: 0.1181\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 7/200\nUsing dynamic weighting: positive=2.90x, negative=0.68x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 471.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.773628, max=1.453209\nPositive prediction rate: 0.7141\nTrain Loss: 0.7243, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8708\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.7662\nVal Accuracy (Jaccard): 0.0646\nVal Precision: 0.0648, Val Recall: 0.7662, Val F1: 0.1179\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 8/200\nUsing dynamic weighting: positive=2.60x, negative=0.70x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 452.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.740976, max=1.390578\nPositive prediction rate: 0.6754\nTrain Loss: 0.7002, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8330\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6183\nVal Accuracy (Jaccard): 0.0598\nVal Precision: 0.0603, Val Recall: 0.6183, Val F1: 0.1084\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 9/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.30x, negative=0.72x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 399.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.797792, max=1.504830\nPositive prediction rate: 0.5279\nTrain Loss: 0.6724, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7824\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5843\nVal Accuracy (Jaccard): 0.0739\nVal Precision: 0.0749, Val Recall: 0.5843, Val F1: 0.1303\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 10/200\nUsing dynamic weighting: positive=2.00x, negative=0.75x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 440.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.754305, max=1.421458\nPositive prediction rate: 0.4882\nTrain Loss: 0.6481, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7484\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5800\nVal Accuracy (Jaccard): 0.0735\nVal Precision: 0.0746, Val Recall: 0.5800, Val F1: 0.1296\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 11/200\nUsing dynamic weighting: positive=2.00x, negative=0.78x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 458.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.717770, max=1.349571\nPositive prediction rate: 0.4722\nTrain Loss: 0.6392, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7156\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5671\nVal Accuracy (Jaccard): 0.0848\nVal Precision: 0.0863, Val Recall: 0.5671, Val F1: 0.1465\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 12/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 434.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.836737, max=1.572644\nPositive prediction rate: 0.7186\nTrain Loss: 0.7661, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8343\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.8772\nVal Accuracy (Jaccard): 0.0723\nVal Precision: 0.0725, Val Recall: 0.8772, Val F1: 0.1320\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 13/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 454.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.841459, max=1.586371\nPositive prediction rate: 0.6622\nTrain Loss: 0.7342, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7970\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.7384\nVal Accuracy (Jaccard): 0.0723\nVal Precision: 0.0725, Val Recall: 0.7384, Val F1: 0.1298\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 14/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 407.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.768737, max=1.444865\nPositive prediction rate: 0.5722\nTrain Loss: 0.7017, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7618\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6593\nVal Accuracy (Jaccard): 0.0748\nVal Precision: 0.0757, Val Recall: 0.6593, Val F1: 0.1333\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 15/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 409.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.716223, max=1.341949\nPositive prediction rate: 0.5349\nTrain Loss: 0.6833, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7489\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6626\nVal Accuracy (Jaccard): 0.0752\nVal Precision: 0.0760, Val Recall: 0.6626, Val F1: 0.1342\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 16/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 420.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.769155, max=1.448591\nPositive prediction rate: 0.5148\nTrain Loss: 0.6536, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7156\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6152\nVal Accuracy (Jaccard): 0.0773\nVal Precision: 0.0783, Val Recall: 0.6152, Val F1: 0.1364\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 17/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 449.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.704242, max=1.321923\nPositive prediction rate: 0.4810\nTrain Loss: 0.6260, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6841\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6152\nVal Accuracy (Jaccard): 0.0774\nVal Precision: 0.0784, Val Recall: 0.6152, Val F1: 0.1366\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 18/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 463.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.698601, max=1.311452\nPositive prediction rate: 0.3666\nTrain Loss: 0.6462, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6992\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4180\nVal Accuracy (Jaccard): 0.0776\nVal Precision: 0.0810, Val Recall: 0.4180, Val F1: 0.1331\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 19/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 446.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.672264, max=1.260926\nPositive prediction rate: 0.3323\nTrain Loss: 0.6195, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6691\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4180\nVal Accuracy (Jaccard): 0.0790\nVal Precision: 0.0826, Val Recall: 0.4180, Val F1: 0.1351\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 20/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 443.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.627841, max=1.175812\nPositive prediction rate: 0.3122\nTrain Loss: 0.5935, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6408\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4180\nVal Accuracy (Jaccard): 0.0790\nVal Precision: 0.0826, Val Recall: 0.4180, Val F1: 0.1351\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 21/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 455.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.791159, max=1.492409\nPositive prediction rate: 0.4780\nTrain Loss: 0.6600, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7125\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6585\nVal Accuracy (Jaccard): 0.0756\nVal Precision: 0.0772, Val Recall: 0.6585, Val F1: 0.1360\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 22/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 447.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.769426, max=1.451812\nPositive prediction rate: 0.4567\nTrain Loss: 0.6323, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6816\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6369\nVal Accuracy (Jaccard): 0.0813\nVal Precision: 0.0832, Val Recall: 0.6369, Val F1: 0.1447\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 23/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 446.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.715776, max=1.345625\nPositive prediction rate: 0.4125\nTrain Loss: 0.6078, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6523\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6348\nVal Accuracy (Jaccard): 0.0907\nVal Precision: 0.0930, Val Recall: 0.6348, Val F1: 0.1594\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 24/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 452.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.661799, max=1.245944\nPositive prediction rate: 0.4052\nTrain Loss: 0.5568, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6249\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2075\nVal Accuracy (Jaccard): 0.0311\nVal Precision: 0.0325, Val Recall: 0.2075, Val F1: 0.0550\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 25/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 444.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.663979, max=1.251063\nPositive prediction rate: 0.3322\nTrain Loss: 0.5312, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5972\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.1731\nVal Accuracy (Jaccard): 0.0329\nVal Precision: 0.0349, Val Recall: 0.1731, Val F1: 0.0565\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 26/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 446.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.625805, max=1.179444\nPositive prediction rate: 0.2789\nTrain Loss: 0.5090, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5714\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.1618\nVal Accuracy (Jaccard): 0.0347\nVal Precision: 0.0362, Val Recall: 0.1618, Val F1: 0.0582\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 27/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 458.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.755420, max=1.422198\nPositive prediction rate: 0.3878\nTrain Loss: 0.6439, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7167\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3136\nVal Accuracy (Jaccard): 0.0503\nVal Precision: 0.0517, Val Recall: 0.3136, Val F1: 0.0868\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 28/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 441.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.675394, max=1.266116\nPositive prediction rate: 0.3673\nTrain Loss: 0.6160, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6852\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2921\nVal Accuracy (Jaccard): 0.0494\nVal Precision: 0.0509, Val Recall: 0.2921, Val F1: 0.0846\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 29/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 447.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.702456, max=1.322510\nPositive prediction rate: 0.3327\nTrain Loss: 0.5888, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6556\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2007\nVal Accuracy (Jaccard): 0.0391\nVal Precision: 0.0409, Val Recall: 0.2007, Val F1: 0.0660\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 30/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 414.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.804497, max=1.511639\nPositive prediction rate: 0.5409\nTrain Loss: 0.7356, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8139\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3618\nVal Accuracy (Jaccard): 0.0434\nVal Precision: 0.0448, Val Recall: 0.3618, Val F1: 0.0782\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 31/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 472.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.835805, max=1.575427\nPositive prediction rate: 0.5027\nTrain Loss: 0.7059, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7790\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3618\nVal Accuracy (Jaccard): 0.0434\nVal Precision: 0.0448, Val Recall: 0.3618, Val F1: 0.0782\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 32/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 458.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.762460, max=1.432982\nPositive prediction rate: 0.4935\nTrain Loss: 0.6734, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7461\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3618\nVal Accuracy (Jaccard): 0.0434\nVal Precision: 0.0448, Val Recall: 0.3618, Val F1: 0.0782\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 33/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 444.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.645688, max=1.209179\nPositive prediction rate: 0.3723\nTrain Loss: 0.5937, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6550\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3195\nVal Accuracy (Jaccard): 0.0597\nVal Precision: 0.0625, Val Recall: 0.3195, Val F1: 0.1023\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 34/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 410.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.695829, max=1.311443\nPositive prediction rate: 0.3364\nTrain Loss: 0.5666, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6265\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3195\nVal Accuracy (Jaccard): 0.0597\nVal Precision: 0.0625, Val Recall: 0.3195, Val F1: 0.1023\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 35/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 458.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.637703, max=1.198423\nPositive prediction rate: 0.3249\nTrain Loss: 0.5428, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5996\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3195\nVal Accuracy (Jaccard): 0.0597\nVal Precision: 0.0625, Val Recall: 0.3195, Val F1: 0.1023\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 36/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 452.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.692690, max=1.299366\nPositive prediction rate: 0.3912\nTrain Loss: 0.6320, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6952\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5719\nVal Accuracy (Jaccard): 0.0988\nVal Precision: 0.1016, Val Recall: 0.5719, Val F1: 0.1683\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 37/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 434.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.685811, max=1.288655\nPositive prediction rate: 0.3455\nTrain Loss: 0.6046, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6649\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4299\nVal Accuracy (Jaccard): 0.0863\nVal Precision: 0.0905, Val Recall: 0.4299, Val F1: 0.1455\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 38/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 469.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.644434, max=1.209351\nPositive prediction rate: 0.3090\nTrain Loss: 0.5791, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6365\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4299\nVal Accuracy (Jaccard): 0.0863\nVal Precision: 0.0905, Val Recall: 0.4299, Val F1: 0.1455\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 39/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 452.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.734315, max=1.381313\nPositive prediction rate: 0.5609\nTrain Loss: 0.6317, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7210\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2794\nVal Accuracy (Jaccard): 0.0278\nVal Precision: 0.0285, Val Recall: 0.2794, Val F1: 0.0511\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 40/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 435.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.692907, max=1.301251\nPositive prediction rate: 0.4937\nTrain Loss: 0.6034, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6897\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2305\nVal Accuracy (Jaccard): 0.0304\nVal Precision: 0.0313, Val Recall: 0.2305, Val F1: 0.0543\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 41/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 437.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.622591, max=1.166014\nPositive prediction rate: 0.3923\nTrain Loss: 0.5775, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6602\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2154\nVal Accuracy (Jaccard): 0.0329\nVal Precision: 0.0339, Val Recall: 0.2154, Val F1: 0.0576\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 42/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 461.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.796362, max=1.498882\nPositive prediction rate: 0.4913\nTrain Loss: 0.7013, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7673\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6838\nVal Accuracy (Jaccard): 0.0869\nVal Precision: 0.0886, Val Recall: 0.6838, Val F1: 0.1545\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 43/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 459.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.719565, max=1.349274\nPositive prediction rate: 0.4543\nTrain Loss: 0.6714, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7349\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6838\nVal Accuracy (Jaccard): 0.0874\nVal Precision: 0.0891, Val Recall: 0.6838, Val F1: 0.1553\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 44/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 447.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.735851, max=1.383171\nPositive prediction rate: 0.4339\nTrain Loss: 0.6453, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7041\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5540\nVal Accuracy (Jaccard): 0.0789\nVal Precision: 0.0806, Val Recall: 0.5540, Val F1: 0.1387\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 45/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 456.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.703376, max=1.322378\nPositive prediction rate: 0.3919\nTrain Loss: 0.6018, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6790\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2001\nVal Accuracy (Jaccard): 0.0284\nVal Precision: 0.0296, Val Recall: 0.2001, Val F1: 0.0502\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 46/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 444.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.700027, max=1.317660\nPositive prediction rate: 0.3723\nTrain Loss: 0.5763, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6506\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.1987\nVal Accuracy (Jaccard): 0.0318\nVal Precision: 0.0333, Val Recall: 0.1987, Val F1: 0.0554\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 47/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 457.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.662445, max=1.247442\nPositive prediction rate: 0.3267\nTrain Loss: 0.5502, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6240\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.0599\nVal Accuracy (Jaccard): 0.0157\nVal Precision: 0.0181, Val Recall: 0.0599, Val F1: 0.0262\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 48/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 414.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.870200, max=1.638922\nPositive prediction rate: 0.5157\nTrain Loss: 0.8076, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.9003\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3147\nVal Accuracy (Jaccard): 0.0405\nVal Precision: 0.0418, Val Recall: 0.3147, Val F1: 0.0721\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 49/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 437.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.826476, max=1.554644\nPositive prediction rate: 0.4783\nTrain Loss: 0.7746, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8649\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3147\nVal Accuracy (Jaccard): 0.0448\nVal Precision: 0.0465, Val Recall: 0.3147, Val F1: 0.0789\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 50/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 468.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.768186, max=1.441424\nPositive prediction rate: 0.4466\nTrain Loss: 0.7458, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8311\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3147\nVal Accuracy (Jaccard): 0.0476\nVal Precision: 0.0493, Val Recall: 0.3147, Val F1: 0.0832\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 51/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 460.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.779146, max=1.462827\nPositive prediction rate: 0.5472\nTrain Loss: 0.7363, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8069\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6416\nVal Accuracy (Jaccard): 0.0774\nVal Precision: 0.0789, Val Recall: 0.6416, Val F1: 0.1380\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 52/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 464.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.779449, max=1.466506\nPositive prediction rate: 0.4997\nTrain Loss: 0.7031, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7734\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6416\nVal Accuracy (Jaccard): 0.0778\nVal Precision: 0.0793, Val Recall: 0.6416, Val F1: 0.1386\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 53/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 462.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.758377, max=1.425814\nPositive prediction rate: 0.4866\nTrain Loss: 0.6769, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7415\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6157\nVal Accuracy (Jaccard): 0.0821\nVal Precision: 0.0838, Val Recall: 0.6157, Val F1: 0.1447\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 54/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 459.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.681021, max=1.278856\nPositive prediction rate: 0.4456\nTrain Loss: 0.6155, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6902\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5768\nVal Accuracy (Jaccard): 0.0757\nVal Precision: 0.0776, Val Recall: 0.5768, Val F1: 0.1345\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 55/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 453.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.700914, max=1.320098\nPositive prediction rate: 0.4382\nTrain Loss: 0.5901, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6615\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5768\nVal Accuracy (Jaccard): 0.0757\nVal Precision: 0.0776, Val Recall: 0.5768, Val F1: 0.1345\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 56/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 463.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.723191, max=1.363719\nPositive prediction rate: 0.4269\nTrain Loss: 0.5654, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6346\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4578\nVal Accuracy (Jaccard): 0.0631\nVal Precision: 0.0651, Val Recall: 0.4578, Val F1: 0.1125\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 57/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 451.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.946079, max=1.785310\nPositive prediction rate: 0.6272\nTrain Loss: 0.8071, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.9081\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5004\nVal Accuracy (Jaccard): 0.0503\nVal Precision: 0.0514, Val Recall: 0.5004, Val F1: 0.0917\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 58/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 468.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.891954, max=1.681461\nPositive prediction rate: 0.5966\nTrain Loss: 0.7731, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8709\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4961\nVal Accuracy (Jaccard): 0.0499\nVal Precision: 0.0510, Val Recall: 0.4961, Val F1: 0.0910\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 59/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 462.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.795769, max=1.494174\nPositive prediction rate: 0.5722\nTrain Loss: 0.7408, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8356\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3767\nVal Accuracy (Jaccard): 0.0463\nVal Precision: 0.0479, Val Recall: 0.3767, Val F1: 0.0833\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 60/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 461.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.723176, max=1.361793\nPositive prediction rate: 0.4011\nTrain Loss: 0.6065, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6860\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2516\nVal Accuracy (Jaccard): 0.0366\nVal Precision: 0.0379, Val Recall: 0.2516, Val F1: 0.0648\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 61/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 447.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.690847, max=1.299893\nPositive prediction rate: 0.3702\nTrain Loss: 0.5804, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6573\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2386\nVal Accuracy (Jaccard): 0.0389\nVal Precision: 0.0406, Val Recall: 0.2386, Val F1: 0.0681\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 62/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 442.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.670997, max=1.264256\nPositive prediction rate: 0.3358\nTrain Loss: 0.5572, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6304\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2372\nVal Accuracy (Jaccard): 0.0444\nVal Precision: 0.0467, Val Recall: 0.2372, Val F1: 0.0765\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 63/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 369.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.823737, max=1.552160\nPositive prediction rate: 0.5057\nTrain Loss: 0.6925, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7590\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4383\nVal Accuracy (Jaccard): 0.0543\nVal Precision: 0.0556, Val Recall: 0.4383, Val F1: 0.0967\nNo improvement for 1 epochs. Best F1: 0.1791\n\nEpoch 64/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 389.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.763834, max=1.438068\nPositive prediction rate: 0.4780\nTrain Loss: 0.6629, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7267\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4383\nVal Accuracy (Jaccard): 0.0601\nVal Precision: 0.0617, Val Recall: 0.4383, Val F1: 0.1058\nNo improvement for 2 epochs. Best F1: 0.1791\n\nEpoch 65/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 416.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.716975, max=1.346852\nPositive prediction rate: 0.4478\nTrain Loss: 0.6363, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6961\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4361\nVal Accuracy (Jaccard): 0.0612\nVal Precision: 0.0629, Val Recall: 0.4361, Val F1: 0.1076\nNo improvement for 3 epochs. Best F1: 0.1791\n\nEpoch 66/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 450.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.724724, max=1.364385\nPositive prediction rate: 0.4203\nTrain Loss: 0.6184, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6690\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.7269\nVal Accuracy (Jaccard): 0.1049\nVal Precision: 0.1072, Val Recall: 0.7269, Val F1: 0.1839\nSaved new best model to ./results/run_20250428_083800_all_text_0.5_augmented/best_model_all_text_0.5_augmented.pt\n\nEpoch 67/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 448.65it/s]\n","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\nClassifier gradient norms: mean=0.688784, max=1.293393\nPositive prediction rate: 0.4047\nTrain Loss: 0.5911, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6388\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.7269\nVal Accuracy (Jaccard): 0.1049\nVal Precision: 0.1072, Val Recall: 0.7269, Val F1: 0.1839\nNo improvement for 1 epochs. Best F1: 0.1839\n\nEpoch 68/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 446.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.695005, max=1.309617\nPositive prediction rate: 0.3930\nTrain Loss: 0.5654, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6104\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.7054\nVal Accuracy (Jaccard): 0.1117\nVal Precision: 0.1145, Val Recall: 0.7054, Val F1: 0.1935\nSaved new best model to ./results/run_20250428_083800_all_text_0.5_augmented/best_model_all_text_0.5_augmented.pt\n\nEpoch 69/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 456.41it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\nClassifier gradient norms: mean=0.632519, max=1.189254\nPositive prediction rate: 0.3626\nTrain Loss: 0.5411, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5837\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6687\nVal Accuracy (Jaccard): 0.1247\nVal Precision: 0.1285, Val Recall: 0.6687, Val F1: 0.2114\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Saved new best model to ./results/run_20250428_083800_all_text_0.5_augmented/best_model_all_text_0.5_augmented.pt\n\nEpoch 70/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 446.78it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\nClassifier gradient norms: mean=0.637016, max=1.200244\nPositive prediction rate: 0.3275\nTrain Loss: 0.5178, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5587\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6580\nVal Accuracy (Jaccard): 0.1266\nVal Precision: 0.1307, Val Recall: 0.6580, Val F1: 0.2137\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Saved new best model to ./results/run_20250428_083800_all_text_0.5_augmented/best_model_all_text_0.5_augmented.pt\n\nEpoch 71/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 419.25it/s]\n","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\nClassifier gradient norms: mean=0.646124, max=1.219303\nPositive prediction rate: 0.3086\nTrain Loss: 0.4970, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5353\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6580\nVal Accuracy (Jaccard): 0.1266\nVal Precision: 0.1307, Val Recall: 0.6580, Val F1: 0.2137\nNo improvement for 1 epochs. Best F1: 0.2137\n\nEpoch 72/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 441.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.572363, max=1.076087\nPositive prediction rate: 0.2978\nTrain Loss: 0.4771, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5133\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6580\nVal Accuracy (Jaccard): 0.1266\nVal Precision: 0.1307, Val Recall: 0.6580, Val F1: 0.2137\nNo improvement for 2 epochs. Best F1: 0.2137\n\nEpoch 73/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 425.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.536747, max=1.008661\nPositive prediction rate: 0.2834\nTrain Loss: 0.4582, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.4926\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6580\nVal Accuracy (Jaccard): 0.1269\nVal Precision: 0.1310, Val Recall: 0.6580, Val F1: 0.2141\nSaved new best model to ./results/run_20250428_083800_all_text_0.5_augmented/best_model_all_text_0.5_augmented.pt\n\nEpoch 74/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 428.50it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\nClassifier gradient norms: mean=0.554645, max=1.045746\nPositive prediction rate: 0.2665\nTrain Loss: 0.4405, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.4733\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6429\nVal Accuracy (Jaccard): 0.1455\nVal Precision: 0.1516, Val Recall: 0.6429, Val F1: 0.2402\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Saved new best model to ./results/run_20250428_083800_all_text_0.5_augmented/best_model_all_text_0.5_augmented.pt\n\nEpoch 75/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 439.93it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\nClassifier gradient norms: mean=0.512112, max=0.963128\nPositive prediction rate: 0.2424\nTrain Loss: 0.4252, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.4553\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6429\nVal Accuracy (Jaccard): 0.1464\nVal Precision: 0.1526, Val Recall: 0.6429, Val F1: 0.2412\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Saved new best model to ./results/run_20250428_083800_all_text_0.5_augmented/best_model_all_text_0.5_augmented.pt\n\nEpoch 76/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 438.64it/s]\n","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\nClassifier gradient norms: mean=0.486113, max=0.914042\nPositive prediction rate: 0.2212\nTrain Loss: 0.4088, Train Accuracy (Exact Match): 0.0016\nVal Loss: 0.4384\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6386\nVal Accuracy (Jaccard): 0.1455\nVal Precision: 0.1517, Val Recall: 0.6386, Val F1: 0.2398\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 77/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 454.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.478630, max=0.900667\nPositive prediction rate: 0.2106\nTrain Loss: 0.3948, Train Accuracy (Exact Match): 0.0010\nVal Loss: 0.4226\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3555\nVal Accuracy (Jaccard): 0.1025\nVal Precision: 0.1106, Val Recall: 0.3555, Val F1: 0.1638\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 78/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 427.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.481866, max=0.909585\nPositive prediction rate: 0.1784\nTrain Loss: 0.3809, Train Accuracy (Exact Match): 0.0021\nVal Loss: 0.4079\nVal Accuracy (Exact Match): 0.0905\nVal Accuracy (Partial Match): 0.1463\nVal Accuracy (Jaccard): 0.1248\nVal Precision: 0.1480, Val Recall: 0.1463, Val F1: 0.1382\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 79/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 438.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.673261, max=1.265404\nPositive prediction rate: 0.3271\nTrain Loss: 0.5791, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6239\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6126\nVal Accuracy (Jaccard): 0.1241\nVal Precision: 0.1264, Val Recall: 0.6126, Val F1: 0.2050\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 80/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 415.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.643539, max=1.208494\nPositive prediction rate: 0.3143\nTrain Loss: 0.5543, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5953\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6126\nVal Accuracy (Jaccard): 0.1241\nVal Precision: 0.1264, Val Recall: 0.6126, Val F1: 0.2050\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 81/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 449.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.635583, max=1.194468\nPositive prediction rate: 0.3059\nTrain Loss: 0.5301, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5685\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6126\nVal Accuracy (Jaccard): 0.1241\nVal Precision: 0.1264, Val Recall: 0.6126, Val F1: 0.2050\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 82/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 442.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.710151, max=1.335069\nPositive prediction rate: 0.4177\nTrain Loss: 0.6095, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7002\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3568\nVal Accuracy (Jaccard): 0.0561\nVal Precision: 0.0585, Val Recall: 0.3568, Val F1: 0.0987\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 83/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 437.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.734845, max=1.386889\nPositive prediction rate: 0.3672\nTrain Loss: 0.5842, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6714\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3491\nVal Accuracy (Jaccard): 0.0581\nVal Precision: 0.0604, Val Recall: 0.3491, Val F1: 0.1014\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 84/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 433.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.668143, max=1.255799\nPositive prediction rate: 0.3232\nTrain Loss: 0.5598, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6444\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3276\nVal Accuracy (Jaccard): 0.0617\nVal Precision: 0.0647, Val Recall: 0.3276, Val F1: 0.1061\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 85/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 435.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.596805, max=1.121940\nPositive prediction rate: 0.3118\nTrain Loss: 0.4925, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5474\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2606\nVal Accuracy (Jaccard): 0.0498\nVal Precision: 0.0520, Val Recall: 0.2606, Val F1: 0.0852\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 86/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 441.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.580895, max=1.094850\nPositive prediction rate: 0.2457\nTrain Loss: 0.4706, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5242\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2110\nVal Accuracy (Jaccard): 0.0705\nVal Precision: 0.0761, Val Recall: 0.2110, Val F1: 0.1088\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 87/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 447.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.560159, max=1.056869\nPositive prediction rate: 0.1730\nTrain Loss: 0.4525, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5026\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.1571\nVal Accuracy (Jaccard): 0.0831\nVal Precision: 0.0948, Val Recall: 0.1571, Val F1: 0.1142\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 88/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 445.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.719164, max=1.353234\nPositive prediction rate: 0.4026\nTrain Loss: 0.6191, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6777\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4205\nVal Accuracy (Jaccard): 0.0709\nVal Precision: 0.0737, Val Recall: 0.4205, Val F1: 0.1221\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 89/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 447.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.671882, max=1.259828\nPositive prediction rate: 0.3624\nTrain Loss: 0.5919, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6474\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4205\nVal Accuracy (Jaccard): 0.0713\nVal Precision: 0.0742, Val Recall: 0.4205, Val F1: 0.1227\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 90/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 438.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.689435, max=1.298362\nPositive prediction rate: 0.3206\nTrain Loss: 0.5679, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6188\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3175\nVal Accuracy (Jaccard): 0.0841\nVal Precision: 0.0904, Val Recall: 0.3175, Val F1: 0.1367\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 91/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 451.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.787117, max=1.479841\nPositive prediction rate: 0.5693\nTrain Loss: 0.7094, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8009\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3061\nVal Accuracy (Jaccard): 0.0360\nVal Precision: 0.0371, Val Recall: 0.3061, Val F1: 0.0647\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 92/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 426.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.766909, max=1.443244\nPositive prediction rate: 0.5288\nTrain Loss: 0.6781, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7680\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3061\nVal Accuracy (Jaccard): 0.0361\nVal Precision: 0.0372, Val Recall: 0.3061, Val F1: 0.0648\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 93/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 433.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.734864, max=1.379543\nPositive prediction rate: 0.5095\nTrain Loss: 0.6512, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7366\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3047\nVal Accuracy (Jaccard): 0.0391\nVal Precision: 0.0404, Val Recall: 0.3047, Val F1: 0.0697\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 94/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 454.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.814998, max=1.532839\nPositive prediction rate: 0.6534\nTrain Loss: 0.7272, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8025\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.9231\nVal Accuracy (Jaccard): 0.0867\nVal Precision: 0.0869, Val Recall: 0.9231, Val F1: 0.1564\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 95/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 449.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.814750, max=1.534305\nPositive prediction rate: 0.5956\nTrain Loss: 0.6948, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7666\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5771\nVal Accuracy (Jaccard): 0.0623\nVal Precision: 0.0637, Val Recall: 0.5771, Val F1: 0.1129\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 96/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 435.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.753802, max=1.416437\nPositive prediction rate: 0.5188\nTrain Loss: 0.6651, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7326\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4214\nVal Accuracy (Jaccard): 0.0526\nVal Precision: 0.0541, Val Recall: 0.4214, Val F1: 0.0942\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 97/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 444.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.818484, max=1.542865\nPositive prediction rate: 0.5453\nTrain Loss: 0.6950, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7597\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.7389\nVal Accuracy (Jaccard): 0.0816\nVal Precision: 0.0828, Val Recall: 0.7389, Val F1: 0.1466\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 98/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 420.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.750048, max=1.410069\nPositive prediction rate: 0.4896\nTrain Loss: 0.6662, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7256\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6664\nVal Accuracy (Jaccard): 0.0845\nVal Precision: 0.0863, Val Recall: 0.6664, Val F1: 0.1504\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 99/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 418.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.704167, max=1.321763\nPositive prediction rate: 0.4486\nTrain Loss: 0.6357, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6934\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6664\nVal Accuracy (Jaccard): 0.0849\nVal Precision: 0.0867, Val Recall: 0.6664, Val F1: 0.1509\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 100/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 403.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.761709, max=1.437277\nPositive prediction rate: 0.4479\nTrain Loss: 0.6224, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7098\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4258\nVal Accuracy (Jaccard): 0.0624\nVal Precision: 0.0641, Val Recall: 0.4258, Val F1: 0.1096\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 101/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 455.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.675639, max=1.269912\nPositive prediction rate: 0.3959\nTrain Loss: 0.5955, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6806\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4042\nVal Accuracy (Jaccard): 0.0684\nVal Precision: 0.0708, Val Recall: 0.4042, Val F1: 0.1180\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 102/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 418.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.657554, max=1.234354\nPositive prediction rate: 0.3621\nTrain Loss: 0.5713, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6530\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3956\nVal Accuracy (Jaccard): 0.0746\nVal Precision: 0.0774, Val Recall: 0.3956, Val F1: 0.1264\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 103/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 457.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.744527, max=1.402711\nPositive prediction rate: 0.4847\nTrain Loss: 0.6175, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6676\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.8611\nVal Accuracy (Jaccard): 0.1033\nVal Precision: 0.1043, Val Recall: 0.8611, Val F1: 0.1831\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 104/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 465.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.703675, max=1.324682\nPositive prediction rate: 0.4653\nTrain Loss: 0.5912, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6377\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.8524\nVal Accuracy (Jaccard): 0.1053\nVal Precision: 0.1063, Val Recall: 0.8524, Val F1: 0.1859\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 105/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 461.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.669437, max=1.259205\nPositive prediction rate: 0.4043\nTrain Loss: 0.5654, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6097\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2677\nVal Accuracy (Jaccard): 0.0545\nVal Precision: 0.0572, Val Recall: 0.2677, Val F1: 0.0918\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 106/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 447.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.868970, max=1.637025\nPositive prediction rate: 0.5642\nTrain Loss: 0.7481, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8348\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4094\nVal Accuracy (Jaccard): 0.0408\nVal Precision: 0.0415, Val Recall: 0.4094, Val F1: 0.0746\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 107/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 439.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.830778, max=1.564356\nPositive prediction rate: 0.5440\nTrain Loss: 0.7185, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7994\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3965\nVal Accuracy (Jaccard): 0.0429\nVal Precision: 0.0437, Val Recall: 0.3965, Val F1: 0.0779\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 108/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 454.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.714137, max=1.338250\nPositive prediction rate: 0.4826\nTrain Loss: 0.6878, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7659\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3864\nVal Accuracy (Jaccard): 0.0469\nVal Precision: 0.0479, Val Recall: 0.3864, Val F1: 0.0841\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 109/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 463.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.806252, max=1.519674\nPositive prediction rate: 0.4283\nTrain Loss: 0.6803, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7644\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2670\nVal Accuracy (Jaccard): 0.0441\nVal Precision: 0.0463, Val Recall: 0.2670, Val F1: 0.0771\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 110/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 435.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.730845, max=1.372634\nPositive prediction rate: 0.4040\nTrain Loss: 0.6500, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7319\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2670\nVal Accuracy (Jaccard): 0.0500\nVal Precision: 0.0529, Val Recall: 0.2670, Val F1: 0.0860\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 111/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 432.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.738974, max=1.392048\nPositive prediction rate: 0.3651\nTrain Loss: 0.6221, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7014\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2670\nVal Accuracy (Jaccard): 0.0501\nVal Precision: 0.0530, Val Recall: 0.2670, Val F1: 0.0861\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 112/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 439.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.828987, max=1.560377\nPositive prediction rate: 0.6017\nTrain Loss: 0.7367, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8129\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5087\nVal Accuracy (Jaccard): 0.0551\nVal Precision: 0.0560, Val Recall: 0.5087, Val F1: 0.0991\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 113/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 455.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.814783, max=1.534754\nPositive prediction rate: 0.5803\nTrain Loss: 0.7049, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7769\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5087\nVal Accuracy (Jaccard): 0.0551\nVal Precision: 0.0560, Val Recall: 0.5087, Val F1: 0.0991\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 114/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 432.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.816570, max=1.540761\nPositive prediction rate: 0.5384\nTrain Loss: 0.6725, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7431\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4232\nVal Accuracy (Jaccard): 0.0510\nVal Precision: 0.0526, Val Recall: 0.4232, Val F1: 0.0917\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 115/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 422.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.876486, max=1.650595\nPositive prediction rate: 0.6809\nTrain Loss: 0.7753, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8600\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5871\nVal Accuracy (Jaccard): 0.0531\nVal Precision: 0.0540, Val Recall: 0.5871, Val F1: 0.0975\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 116/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 452.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.880210, max=1.660296\nPositive prediction rate: 0.6246\nTrain Loss: 0.7402, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8218\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5548\nVal Accuracy (Jaccard): 0.0535\nVal Precision: 0.0545, Val Recall: 0.5548, Val F1: 0.0977\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 117/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 458.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.818993, max=1.541919\nPositive prediction rate: 0.5584\nTrain Loss: 0.7086, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7856\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3665\nVal Accuracy (Jaccard): 0.0454\nVal Precision: 0.0469, Val Recall: 0.3665, Val F1: 0.0813\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 118/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 444.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.680931, max=1.280019\nPositive prediction rate: 0.4284\nTrain Loss: 0.6045, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6633\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5019\nVal Accuracy (Jaccard): 0.0704\nVal Precision: 0.0722, Val Recall: 0.5019, Val F1: 0.1243\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 119/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 454.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.679591, max=1.279124\nPositive prediction rate: 0.3970\nTrain Loss: 0.5789, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6345\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4976\nVal Accuracy (Jaccard): 0.0701\nVal Precision: 0.0720, Val Recall: 0.4976, Val F1: 0.1238\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 120/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 463.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.638071, max=1.198300\nPositive prediction rate: 0.3649\nTrain Loss: 0.5544, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6075\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4782\nVal Accuracy (Jaccard): 0.0877\nVal Precision: 0.0908, Val Recall: 0.4782, Val F1: 0.1500\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 121/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 436.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.748515, max=1.408553\nPositive prediction rate: 0.4393\nTrain Loss: 0.6322, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7271\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2616\nVal Accuracy (Jaccard): 0.0315\nVal Precision: 0.0322, Val Recall: 0.2616, Val F1: 0.0568\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 122/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 440.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.746371, max=1.407879\nPositive prediction rate: 0.3957\nTrain Loss: 0.6056, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6959\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2013\nVal Accuracy (Jaccard): 0.0322\nVal Precision: 0.0331, Val Recall: 0.2013, Val F1: 0.0562\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 123/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 443.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.676633, max=1.273818\nPositive prediction rate: 0.3286\nTrain Loss: 0.5792, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6667\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.1797\nVal Accuracy (Jaccard): 0.0388\nVal Precision: 0.0405, Val Recall: 0.1797, Val F1: 0.0651\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 124/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 431.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.619454, max=1.159958\nPositive prediction rate: 0.3193\nTrain Loss: 0.5698, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6226\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5165\nVal Accuracy (Jaccard): 0.0989\nVal Precision: 0.1027, Val Recall: 0.5165, Val F1: 0.1677\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 125/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 439.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.647797, max=1.219292\nPositive prediction rate: 0.3113\nTrain Loss: 0.5448, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5956\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5165\nVal Accuracy (Jaccard): 0.0989\nVal Precision: 0.1027, Val Recall: 0.5165, Val F1: 0.1677\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 126/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 433.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.662128, max=1.247775\nPositive prediction rate: 0.3048\nTrain Loss: 0.5217, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5703\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5165\nVal Accuracy (Jaccard): 0.0989\nVal Precision: 0.1027, Val Recall: 0.5165, Val F1: 0.1677\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 127/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 432.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.820066, max=1.544146\nPositive prediction rate: 0.6006\nTrain Loss: 0.7108, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7962\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5494\nVal Accuracy (Jaccard): 0.0549\nVal Precision: 0.0557, Val Recall: 0.5494, Val F1: 0.0998\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 128/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 443.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.794864, max=1.497379\nPositive prediction rate: 0.5508\nTrain Loss: 0.6793, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7611\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5192\nVal Accuracy (Jaccard): 0.0617\nVal Precision: 0.0629, Val Recall: 0.5192, Val F1: 0.1104\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 129/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 460.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.751937, max=1.413961\nPositive prediction rate: 0.5010\nTrain Loss: 0.6489, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7281\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4423\nVal Accuracy (Jaccard): 0.0555\nVal Precision: 0.0570, Val Recall: 0.4423, Val F1: 0.0995\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 130/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 449.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.883247, max=1.664846\nPositive prediction rate: 0.7004\nTrain Loss: 0.7677, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8557\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5935\nVal Accuracy (Jaccard): 0.0511\nVal Precision: 0.0520, Val Recall: 0.5935, Val F1: 0.0943\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 131/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 446.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.854980, max=1.610586\nPositive prediction rate: 0.6697\nTrain Loss: 0.7340, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8181\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5784\nVal Accuracy (Jaccard): 0.0536\nVal Precision: 0.0546, Val Recall: 0.5784, Val F1: 0.0983\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 132/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 453.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.803021, max=1.511733\nPositive prediction rate: 0.5943\nTrain Loss: 0.7031, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7824\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5654\nVal Accuracy (Jaccard): 0.0630\nVal Precision: 0.0643, Val Recall: 0.5654, Val F1: 0.1135\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 133/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 442.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.612934, max=1.146978\nPositive prediction rate: 0.3789\nTrain Loss: 0.5643, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6332\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2205\nVal Accuracy (Jaccard): 0.0389\nVal Precision: 0.0413, Val Recall: 0.2205, Val F1: 0.0681\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 134/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 453.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.654971, max=1.233157\nPositive prediction rate: 0.3493\nTrain Loss: 0.5394, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6057\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2205\nVal Accuracy (Jaccard): 0.0389\nVal Precision: 0.0413, Val Recall: 0.2205, Val F1: 0.0681\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 135/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 460.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.654679, max=1.235770\nPositive prediction rate: 0.3404\nTrain Loss: 0.5164, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5800\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2205\nVal Accuracy (Jaccard): 0.0389\nVal Precision: 0.0413, Val Recall: 0.2205, Val F1: 0.0681\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 136/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 456.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.741716, max=1.390948\nPositive prediction rate: 0.4768\nTrain Loss: 0.7086, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7696\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6222\nVal Accuracy (Jaccard): 0.0771\nVal Precision: 0.0787, Val Recall: 0.6222, Val F1: 0.1371\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 137/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 442.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.767937, max=1.445009\nPositive prediction rate: 0.4308\nTrain Loss: 0.6790, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7363\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6037\nVal Accuracy (Jaccard): 0.0898\nVal Precision: 0.0921, Val Recall: 0.6037, Val F1: 0.1567\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 138/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 470.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.735247, max=1.383166\nPositive prediction rate: 0.3972\nTrain Loss: 0.6510, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7049\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5951\nVal Accuracy (Jaccard): 0.0909\nVal Precision: 0.0934, Val Recall: 0.5951, Val F1: 0.1583\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 139/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 480.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.870916, max=1.639575\nPositive prediction rate: 0.6721\nTrain Loss: 0.7705, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8706\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3588\nVal Accuracy (Jaccard): 0.0333\nVal Precision: 0.0342, Val Recall: 0.3588, Val F1: 0.0614\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 140/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 461.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.871848, max=1.644108\nPositive prediction rate: 0.6148\nTrain Loss: 0.7382, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8333\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3351\nVal Accuracy (Jaccard): 0.0348\nVal Precision: 0.0358, Val Recall: 0.3351, Val F1: 0.0635\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 141/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 464.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.798408, max=1.502014\nPositive prediction rate: 0.5504\nTrain Loss: 0.7061, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7980\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2713\nVal Accuracy (Jaccard): 0.0333\nVal Precision: 0.0345, Val Recall: 0.2713, Val F1: 0.0601\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 142/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 475.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.723055, max=1.359282\nPositive prediction rate: 0.3853\nTrain Loss: 0.6516, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7139\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5300\nVal Accuracy (Jaccard): 0.0852\nVal Precision: 0.0887, Val Recall: 0.5300, Val F1: 0.1493\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 143/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 452.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.679667, max=1.276377\nPositive prediction rate: 0.3547\nTrain Loss: 0.6238, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6825\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5084\nVal Accuracy (Jaccard): 0.0918\nVal Precision: 0.0959, Val Recall: 0.5084, Val F1: 0.1583\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 144/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 438.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.667064, max=1.251871\nPositive prediction rate: 0.3198\nTrain Loss: 0.5967, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6530\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5041\nVal Accuracy (Jaccard): 0.0930\nVal Precision: 0.0970, Val Recall: 0.5041, Val F1: 0.1597\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 145/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 459.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.661738, max=1.245562\nPositive prediction rate: 0.3466\nTrain Loss: 0.5465, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6039\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3180\nVal Accuracy (Jaccard): 0.0528\nVal Precision: 0.0546, Val Recall: 0.3180, Val F1: 0.0913\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 146/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 461.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.611308, max=1.148841\nPositive prediction rate: 0.2963\nTrain Loss: 0.5220, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5766\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2521\nVal Accuracy (Jaccard): 0.0457\nVal Precision: 0.0476, Val Recall: 0.2521, Val F1: 0.0785\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 147/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 463.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.592629, max=1.114937\nPositive prediction rate: 0.2436\nTrain Loss: 0.4984, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5512\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.1702\nVal Accuracy (Jaccard): 0.0482\nVal Precision: 0.0514, Val Recall: 0.1702, Val F1: 0.0762\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 148/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 418.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.841520, max=1.586483\nPositive prediction rate: 0.5941\nTrain Loss: 0.7220, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8150\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3356\nVal Accuracy (Jaccard): 0.0330\nVal Precision: 0.0338, Val Recall: 0.3356, Val F1: 0.0603\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 149/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 428.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.804264, max=1.514976\nPositive prediction rate: 0.5620\nTrain Loss: 0.6909, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7797\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3356\nVal Accuracy (Jaccard): 0.0356\nVal Precision: 0.0364, Val Recall: 0.3356, Val F1: 0.0645\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 150/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 453.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.751491, max=1.412689\nPositive prediction rate: 0.4990\nTrain Loss: 0.6607, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7463\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2946\nVal Accuracy (Jaccard): 0.0416\nVal Precision: 0.0431, Val Recall: 0.2946, Val F1: 0.0734\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 151/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 462.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.738900, max=1.390059\nPositive prediction rate: 0.4915\nTrain Loss: 0.6301, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6998\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5610\nVal Accuracy (Jaccard): 0.0685\nVal Precision: 0.0702, Val Recall: 0.5610, Val F1: 0.1229\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 152/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 467.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.661682, max=1.239887\nPositive prediction rate: 0.4510\nTrain Loss: 0.5995, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6682\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4567\nVal Accuracy (Jaccard): 0.0652\nVal Precision: 0.0674, Val Recall: 0.4567, Val F1: 0.1151\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 153/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 456.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.660512, max=1.242454\nPositive prediction rate: 0.3914\nTrain Loss: 0.5737, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6384\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3841\nVal Accuracy (Jaccard): 0.0684\nVal Precision: 0.0712, Val Recall: 0.3841, Val F1: 0.1173\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 154/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 445.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.806711, max=1.522426\nPositive prediction rate: 0.5107\nTrain Loss: 0.6459, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7146\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4182\nVal Accuracy (Jaccard): 0.0488\nVal Precision: 0.0498, Val Recall: 0.4182, Val F1: 0.0871\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 155/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 459.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.732657, max=1.380195\nPositive prediction rate: 0.4964\nTrain Loss: 0.6177, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6835\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4139\nVal Accuracy (Jaccard): 0.0520\nVal Precision: 0.0533, Val Recall: 0.4139, Val F1: 0.0924\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 156/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 424.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.705895, max=1.328429\nPositive prediction rate: 0.4429\nTrain Loss: 0.5910, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6542\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3670\nVal Accuracy (Jaccard): 0.0638\nVal Precision: 0.0660, Val Recall: 0.3670, Val F1: 0.1092\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 157/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 450.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.657343, max=1.232692\nPositive prediction rate: 0.4370\nTrain Loss: 0.5854, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6548\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3445\nVal Accuracy (Jaccard): 0.0533\nVal Precision: 0.0556, Val Recall: 0.3445, Val F1: 0.0934\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 158/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 449.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.661966, max=1.245583\nPositive prediction rate: 0.3765\nTrain Loss: 0.5605, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6266\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2061\nVal Accuracy (Jaccard): 0.0385\nVal Precision: 0.0407, Val Recall: 0.2061, Val F1: 0.0664\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 159/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 470.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.628175, max=1.180478\nPositive prediction rate: 0.3254\nTrain Loss: 0.5369, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6000\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2018\nVal Accuracy (Jaccard): 0.0380\nVal Precision: 0.0402, Val Recall: 0.2018, Val F1: 0.0655\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 160/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 449.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.792636, max=1.490214\nPositive prediction rate: 0.6170\nTrain Loss: 0.7044, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7967\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5427\nVal Accuracy (Jaccard): 0.0574\nVal Precision: 0.0587, Val Recall: 0.5427, Val F1: 0.1043\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 161/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 470.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.821814, max=1.549747\nPositive prediction rate: 0.5620\nTrain Loss: 0.6746, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7625\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5254\nVal Accuracy (Jaccard): 0.0586\nVal Precision: 0.0600, Val Recall: 0.5254, Val F1: 0.1058\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 162/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 449.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.755467, max=1.421905\nPositive prediction rate: 0.5335\nTrain Loss: 0.6461, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7301\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5254\nVal Accuracy (Jaccard): 0.0586\nVal Precision: 0.0600, Val Recall: 0.5254, Val F1: 0.1058\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 163/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 467.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.803278, max=1.509117\nPositive prediction rate: 0.5416\nTrain Loss: 0.7578, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8622\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3009\nVal Accuracy (Jaccard): 0.0299\nVal Precision: 0.0306, Val Recall: 0.3009, Val F1: 0.0548\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 164/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 360.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.821969, max=1.546290\nPositive prediction rate: 0.5323\nTrain Loss: 0.7283, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.8275\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2923\nVal Accuracy (Jaccard): 0.0297\nVal Precision: 0.0304, Val Recall: 0.2923, Val F1: 0.0544\nNo improvement for 2 epochs. Best F1: 0.2412\n\nEpoch 165/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 390.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.779473, max=1.466130\nPositive prediction rate: 0.4876\nTrain Loss: 0.6976, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7946\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.2157\nVal Accuracy (Jaccard): 0.0289\nVal Precision: 0.0296, Val Recall: 0.2157, Val F1: 0.0515\nNo improvement for 3 epochs. Best F1: 0.2412\n\nEpoch 166/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 436.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.651648, max=1.225994\nPositive prediction rate: 0.3341\nTrain Loss: 0.5718, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6032\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6332\nVal Accuracy (Jaccard): 0.1392\nVal Precision: 0.1430, Val Recall: 0.6332, Val F1: 0.2264\nNo improvement for 1 epochs. Best F1: 0.2412\n\nEpoch 167/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 448.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.629475, max=1.183099\nPositive prediction rate: 0.2788\nTrain Loss: 0.5468, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5756\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6318\nVal Accuracy (Jaccard): 0.1529\nVal Precision: 0.1578, Val Recall: 0.6318, Val F1: 0.2457\nSaved new best model to ./results/run_20250428_083800_all_text_0.5_augmented/best_model_all_text_0.5_augmented.pt\n\nEpoch 168/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 432.85it/s]\n","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\nClassifier gradient norms: mean=0.632339, max=1.188222\nPositive prediction rate: 0.2573\nTrain Loss: 0.5238, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5499\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6318\nVal Accuracy (Jaccard): 0.1529\nVal Precision: 0.1578, Val Recall: 0.6318, Val F1: 0.2457\nNo improvement for 1 epochs. Best F1: 0.2457\n\nEpoch 169/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 471.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.599218, max=1.127002\nPositive prediction rate: 0.2460\nTrain Loss: 0.5018, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.5259\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6318\nVal Accuracy (Jaccard): 0.1529\nVal Precision: 0.1578, Val Recall: 0.6318, Val F1: 0.2457\nNo improvement for 2 epochs. Best F1: 0.2457\n\nEpoch 170/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 461.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.577753, max=1.087895\nPositive prediction rate: 0.2402\nTrain Loss: 0.4802, Train Accuracy (Exact Match): 0.0010\nVal Loss: 0.5034\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.6318\nVal Accuracy (Jaccard): 0.1529\nVal Precision: 0.1578, Val Recall: 0.6318, Val F1: 0.2457\nNo improvement for 3 epochs. Best F1: 0.2457\n\nEpoch 171/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 420.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.758417, max=1.425263\nPositive prediction rate: 0.5848\nTrain Loss: 0.6721, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7697\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3287\nVal Accuracy (Jaccard): 0.0342\nVal Precision: 0.0352, Val Recall: 0.3287, Val F1: 0.0624\nNo improvement for 1 epochs. Best F1: 0.2457\n\nEpoch 172/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 456.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.754521, max=1.420707\nPositive prediction rate: 0.5622\nTrain Loss: 0.6429, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7365\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3287\nVal Accuracy (Jaccard): 0.0342\nVal Precision: 0.0352, Val Recall: 0.3287, Val F1: 0.0624\nNo improvement for 2 epochs. Best F1: 0.2457\n\nEpoch 173/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 451.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.717909, max=1.351330\nPositive prediction rate: 0.5246\nTrain Loss: 0.6140, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7051\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3153\nVal Accuracy (Jaccard): 0.0383\nVal Precision: 0.0397, Val Recall: 0.3153, Val F1: 0.0692\nNo improvement for 3 epochs. Best F1: 0.2457\n\nEpoch 174/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 447.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.742534, max=1.394700\nPositive prediction rate: 0.4152\nTrain Loss: 0.6717, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7280\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4877\nVal Accuracy (Jaccard): 0.0686\nVal Precision: 0.0708, Val Recall: 0.4877, Val F1: 0.1218\nNo improvement for 1 epochs. Best F1: 0.2457\n\nEpoch 175/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 462.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.775894, max=1.463846\nPositive prediction rate: 0.3824\nTrain Loss: 0.6456, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6967\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4877\nVal Accuracy (Jaccard): 0.0767\nVal Precision: 0.0794, Val Recall: 0.4877, Val F1: 0.1344\nNo improvement for 2 epochs. Best F1: 0.2457\n\nEpoch 176/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 466.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.739497, max=1.393292\nPositive prediction rate: 0.3633\nTrain Loss: 0.6189, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6672\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.4877\nVal Accuracy (Jaccard): 0.0767\nVal Precision: 0.0794, Val Recall: 0.4877, Val F1: 0.1344\nNo improvement for 3 epochs. Best F1: 0.2457\n\nEpoch 177/200\nModel seems stuck. Reinitializing classifier layer...\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 449.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.770034, max=1.449005\nPositive prediction rate: 0.5497\nTrain Loss: 0.6640, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.7280\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.5004\nVal Accuracy (Jaccard): 0.0597\nVal Precision: 0.0610, Val Recall: 0.5004, Val F1: 0.1069\nNo improvement for 1 epochs. Best F1: 0.2457\n\nEpoch 178/200\nUsing dynamic weighting: positive=2.00x, negative=0.80x\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings):   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":" Gradients are flowing to classifier\n","output_type":"stream"},{"name":"stderr","text":"Training (embeddings): 100%|| 60/60 [00:00<00:00, 436.20it/s]\n/tmp/ipykernel_60/2576988714.py:1889: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.module.load_state_dict(torch.load(best_model_path))\n","output_type":"stream"},{"name":"stdout","text":"Classifier gradient norms: mean=0.711888, max=1.337139\nPositive prediction rate: 0.4580\nEarly stopping triggered\nTrain Loss: 0.6341, Train Accuracy (Exact Match): 0.0000\nVal Loss: 0.6956\nVal Accuracy (Exact Match): 0.0000\nVal Accuracy (Partial Match): 0.3713\nVal Accuracy (Jaccard): 0.0514\nVal Precision: 0.0528, Val Recall: 0.3713, Val F1: 0.0911\nNo improvement for 2 epochs. Best F1: 0.2457\nEarly stopping triggered. Terminating training.\n\n=== FINAL EVALUATION ===\nFinal evaluation with best model:\nFinal Loss: 0.5756\nFinal Exact Match Accuracy: 0.0000\nFinal Partial Match Accuracy: 0.6318\nFinal Jaccard Similarity: 0.1529\nFinal Precision: 0.1578\nFinal Recall: 0.6318\nFinal F1 Score: 0.2457\n\nTraining completed! Results saved to ./results/run_20250428_083800_all_text_0.5_augmented\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"Your problem with SMOTE for text classification is common. SMOTE works well for numerical features but struggles with high-dimensional text embeddings. Here's why:\n\nThe embedding space quality is critical - DeBERTa embeddings represent semantic relationships that SMOTE likely disrupts\nText data synthetic samples may not maintain linguistic coherence\nClass imbalance in NLP often requires different approaches than traditional oversampling","metadata":{}}]}