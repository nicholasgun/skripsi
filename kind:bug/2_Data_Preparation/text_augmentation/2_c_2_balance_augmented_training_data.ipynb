{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f30416f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "from collections import Counter\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090698a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('median/preprocessed data/preprocessed_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1809c150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution Count (Training Data):\n",
      "1. area/test: 309\n",
      "2. area/kubelet: 304\n",
      "3. area/apiserver: 196\n",
      "4. area/cloudprovider: 171\n",
      "5. area/kubectl: 133\n",
      "6. area/dependency: 80\n",
      "7. area/code-generation: 65\n",
      "8. area/provider/azure: 64\n",
      "9. area/ipvs: 39\n",
      "10. area/kubeadm: 33\n",
      "11. area/provider/gcp: 32\n",
      "12. area/api: 28\n",
      "13. area/e2e-test-framework: 28\n",
      "14. area/kube-proxy: 28\n",
      "15. area/release-eng: 28\n",
      "16. area/conformance: 28\n",
      "17. area/batch: 28\n",
      "18. area/deflake: 28\n",
      "19. area/network-policy: 28\n",
      "20. area/client-libraries: 28\n",
      "21. area/code-organization: 28\n",
      "22. area/security: 28\n",
      "23. area/etcd: 28\n",
      "24. area/custom-resources: 28\n",
      "25. area/provider/aws: 28\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of labels if it's a list of lists\n",
    "flattened_labels = []\n",
    "for label_list in train_data[\"labels\"]:\n",
    "    # Convert string representation of list to actual list\n",
    "    if isinstance(label_list, str):\n",
    "        # Remove brackets and split by comma\n",
    "        clean_label_list = label_list.strip('[]').replace(\"'\", \"\").split(', ')\n",
    "    else:\n",
    "        clean_label_list = label_list\n",
    "    \n",
    "    # Add each label to the flattened list\n",
    "    flattened_labels.extend(clean_label_list)\n",
    "\n",
    "# Print labels distribution count\n",
    "label_counts = Counter(flattened_labels)\n",
    "print(\"Label Distribution Count (Training Data):\")\n",
    "\n",
    "for i, (label, count) in enumerate(label_counts.most_common()):\n",
    "    print(f\"{i+1}. {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0165891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data size: 1268\n",
      "Balanced training data size: 609\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'labels' column contains lists of strings\n",
    "def parse_labels(label_list_str):\n",
    "    if isinstance(label_list_str, str):\n",
    "        try:\n",
    "            # Safely evaluate the string representation of the list\n",
    "            parsed_list = ast.literal_eval(label_list_str)\n",
    "            if isinstance(parsed_list, list):\n",
    "                # Ensure all elements are strings\n",
    "                return [str(item) for item in parsed_list]\n",
    "            else:\n",
    "                return [] # Return empty list if parsing results in non-list\n",
    "        except (ValueError, SyntaxError):\n",
    "             # Handle cases like '[]', \"['label1', 'label2']\" or malformed strings\n",
    "             clean_label_list_str = label_list_str.strip('[]').replace(\"'\", \"\").replace('\"', '')\n",
    "             if not clean_label_list_str: # Handle empty string case '[]'\n",
    "                 return []\n",
    "             clean_label_list = clean_label_list_str.split(',') # Split by comma first\n",
    "             # Clean whitespace and filter empty strings resulting from split\n",
    "             return [label.strip() for label in clean_label_list if label.strip()] \n",
    "    elif isinstance(label_list_str, list):\n",
    "         return [str(item) for item in label_list_str] # Ensure all elements are strings\n",
    "    else: # Handle other potential types like float (NaN) if there are missing values\n",
    "        return []\n",
    "\n",
    "# Apply parsing to create a reliable list representation\n",
    "train_data['parsed_labels'] = train_data['labels'].apply(parse_labels)\n",
    "\n",
    "# Recalculate flattened_labels and label_counts using the parsed column\n",
    "flattened_labels_parsed = [label for sublist in train_data['parsed_labels'] for label in sublist]\n",
    "label_counts_parsed = Counter(flattened_labels_parsed)\n",
    "\n",
    "# --- Undersampling Logic ---\n",
    "random.seed(42) # for reproducibility\n",
    "\n",
    "max_samples_per_label = 28\n",
    "oversampled_labels = {label for label, count in label_counts_parsed.items() if count > max_samples_per_label}\n",
    "\n",
    "indices_to_keep = set()\n",
    "indices_per_label = {label: [] for label in label_counts_parsed.keys()}\n",
    "\n",
    "# Group indices by label\n",
    "for index, row in train_data.iterrows():\n",
    "    for label in row['parsed_labels']:\n",
    "        if label in indices_per_label: # Ensure label exists in our count keys\n",
    "             indices_per_label[label].append(index)\n",
    "\n",
    "# Undersample for over-represented labels and collect indices\n",
    "for label, indices in indices_per_label.items():\n",
    "    unique_indices = list(set(indices)) # Ensure unique indices per label before sampling\n",
    "    if label in oversampled_labels:\n",
    "        if len(unique_indices) > max_samples_per_label:\n",
    "             indices_to_keep.update(random.sample(unique_indices, max_samples_per_label))\n",
    "        else:\n",
    "             # This case means the count was > 28 but unique rows are <= 28 (due to multi-label rows)\n",
    "             # Keep all unique rows containing this label\n",
    "             indices_to_keep.update(unique_indices)\n",
    "    else:\n",
    "        # Keep all unique indices for labels already at or below the threshold\n",
    "        indices_to_keep.update(unique_indices)\n",
    "\n",
    "\n",
    "# Create the balanced dataframe\n",
    "balanced_train_data = train_data.loc[sorted(list(indices_to_keep))].copy()\n",
    "\n",
    "# Optional: Drop the temporary parsed_labels column if not needed later\n",
    "# balanced_train_data = balanced_train_data.drop(columns=['parsed_labels'])\n",
    "\n",
    "print(f\"Original training data size: {len(train_data)}\")\n",
    "print(f\"Balanced training data size: {len(balanced_train_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution Count (Balanced Training Data):\n",
      "1. area/test: 96\n",
      "2. area/cloudprovider: 89\n",
      "3. area/apiserver: 76\n",
      "4. area/kubelet: 64\n",
      "5. area/dependency: 54\n",
      "6. area/kubectl: 54\n",
      "7. area/code-generation: 47\n",
      "8. area/ipvs: 33\n",
      "9. area/provider/azure: 32\n",
      "10. area/kubeadm: 29\n",
      "11. area/api: 28\n",
      "12. area/e2e-test-framework: 28\n",
      "13. area/kube-proxy: 28\n",
      "14. area/release-eng: 28\n",
      "15. area/provider/gcp: 28\n",
      "16. area/conformance: 28\n",
      "17. area/batch: 28\n",
      "18. area/deflake: 28\n",
      "19. area/network-policy: 28\n",
      "20. area/client-libraries: 28\n",
      "21. area/code-organization: 28\n",
      "22. area/security: 28\n",
      "23. area/etcd: 28\n",
      "24. area/custom-resources: 28\n",
      "25. area/provider/aws: 28\n",
      "\n",
      "All labels have <= 28 samples: False\n"
     ]
    }
   ],
   "source": [
    "# Verify Balanced Distribution\n",
    "# Ensure 'parsed_labels' column exists if it was dropped, otherwise re-parse or use 'labels'\n",
    "if 'parsed_labels' not in balanced_train_data.columns:\n",
    "     # Re-apply parsing if the column was dropped or doesn't exist\n",
    "     balanced_train_data['parsed_labels'] = balanced_train_data['labels'].apply(parse_labels)\n",
    "\n",
    "flattened_labels_balanced = [label for sublist in balanced_train_data['parsed_labels'] for label in sublist]\n",
    "label_counts_balanced = Counter(flattened_labels_balanced)\n",
    "\n",
    "print(\"\\nLabel Distribution Count (Balanced Training Data):\")\n",
    "# Sort by count descending for clarity\n",
    "for i, (label, count) in enumerate(label_counts_balanced.most_common()):\n",
    "    print(f\"{i+1}. {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e948249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the balanced data\n",
    "balanced_train_data.to_csv('median/preprocessed data/balanced_train_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r-reticulate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
