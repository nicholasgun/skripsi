{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-29T12:35:47.768177Z",
     "iopub.status.busy": "2025-04-29T12:35:47.767947Z",
     "iopub.status.idle": "2025-04-29T12:35:47.857824Z",
     "shell.execute_reply": "2025-04-29T12:35:47.856999Z",
     "shell.execute_reply.started": "2025-04-29T12:35:47.768160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import AdamW\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from transformers import DebertaTokenizer, DebertaModel\n",
    "from torch import nn\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, texts, labels, model, label_encoder=None, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: pandas Series or list of texts\n",
    "            labels: list of label lists\n",
    "            model: SBERT model\n",
    "            label_encoder: MultiLabelBinarizer instance (optional)\n",
    "            max_length: maximum sequence length\n",
    "        \"\"\"\n",
    "        # Convert texts to list if it's a pandas Series\n",
    "        self.texts = texts.tolist() if isinstance(texts, pd.Series) else texts\n",
    "        \n",
    "        # Initialize or use provided label encoder\n",
    "        if label_encoder is None:\n",
    "            self.label_encoder = MultiLabelBinarizer()\n",
    "            self.labels = self.label_encoder.fit_transform(labels)\n",
    "        else:\n",
    "            self.label_encoder = label_encoder\n",
    "            self.labels = self.label_encoder.transform(labels)\n",
    "        \n",
    "        self.model = model\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]  # Now a binary numpy array\n",
    "        \n",
    "        # Access tokenizer correctly, handling DataParallel wrapper\n",
    "        actual_model = self.model.module if isinstance(self.model, torch.nn.DataParallel) else self.model\n",
    "        tokenizer = actual_model._first_module().tokenizer\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class FineTunedSBERT(SentenceTransformer):\n",
    "    def __init__(self, model_name='all-mpnet-base-v2', num_labels=20):\n",
    "        super().__init__(model_name)\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for layer in self._first_module().auto_model.encoder.layer[-3:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        for param in self._first_module().auto_model.pooler.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        hidden_size = self._first_module().auto_model.config.hidden_size\n",
    "        self.classifier = torch.nn.Linear(hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self._first_module().auto_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A classifier model based on DeBERTa for multi-label classification.\n",
    "    \n",
    "    This model uses a pre-trained DeBERTa model as the encoder and adds a \n",
    "    classification head on top with sigmoid activation for multi-label output.\n",
    "    \n",
    "    Args:\n",
    "        num_labels (int): Number of classes in the multi-label classification task.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.deberta = DebertaModel.from_pretrained('microsoft/deberta-base')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels) # 768 is the hidden size for deberta-base\n",
    "        # Freeze all parameters in DeBERTa\n",
    "        for param in self.deberta.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze encoder parameters for fine-tuning\n",
    "        # Note: DeBERTa has a different architecture than BERT/RoBERTa\n",
    "        # We'll unfreeze the last 3 encoder layers\n",
    "        for layer in self.deberta.encoder.layer[-3:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Unlike BERT, DeBERTa doesn't have a pooler, so we need to take the last hidden state\n",
    "        # and either use the [CLS] token (first token) or do mean pooling\n",
    "        # Here we'll use the [CLS] token (first token) representation\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        # Return raw logits for BCEWithLogitsLoss (sigmoid will be applied in the loss function)\n",
    "        return self.classifier(cls_output)\n",
    "\n",
    "class DeBERTaIssueDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts.tolist() if isinstance(texts, pd.Series) else texts # Ensure texts is a list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "def prepare_data(df, text_column='all_text', min_label_freq=0, max_label_len=100):\n",
    "    if text_column in df.columns:\n",
    "        df = df[[text_column, 'labels']]\n",
    "        df = df[~df[text_column].apply(lambda x: x.startswith('nan') if isinstance(x, str) else False)]\n",
    "    else:\n",
    "        raise ValueError(f\"Text column '{text_column}' not found in the DataFrame\")\n",
    "    \n",
    "    df = df.dropna()\n",
    "    texts = df[text_column]\n",
    "    labels = df['labels'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    label_distribution = Counter([label for labels in labels for label in labels])\n",
    "    frequent_labels = [label for label, count in label_distribution.items() if count >= min_label_freq]\n",
    "    \n",
    "    filtered_labels = labels.apply(lambda x: [label for label in x if label in frequent_labels])\n",
    "    label_length = filtered_labels.apply(len)\n",
    "    length_mask = (label_length > 0) & (label_length <= max_label_len)\n",
    "    \n",
    "    texts = texts[length_mask].reset_index(drop=True)\n",
    "    filtered_labels = filtered_labels[length_mask].reset_index(drop=True)\n",
    "    \n",
    "    return texts, filtered_labels\n",
    "\n",
    "def get_embeddings(texts, model, batch_size=32):\n",
    "    # Determine the actual model and device, handling DataParallel\n",
    "    actual_model = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "    actual_model.eval() # Set the underlying model to evaluation mode\n",
    "    device = next(actual_model.parameters()).device\n",
    "\n",
    "    # Use the original model (potentially wrapped) for the forward pass if using multiple GPUs\n",
    "    # DataParallel handles the distribution automatically\n",
    "    inference_model = model if isinstance(model, torch.nn.DataParallel) else actual_model\n",
    "    if isinstance(inference_model, torch.nn.DataParallel):\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for embedding generation!\")\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "            batch_texts = texts[i:i + batch_size].tolist()\n",
    "            \n",
    "            # Use tokenizer from the actual_model\n",
    "            tokenizer = actual_model._first_module().tokenizer\n",
    "    \n",
    "            encoding = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=actual_model.max_seq_length, # Use actual model's max_seq_length\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            # Directly get embeddings from the base transformer model's output\n",
    "            # If using DataParallel (inference_model is wrapped), it will handle the multi-GPU forward pass\n",
    "            # We access the underlying module's auto_model to get the correct output structure\n",
    "            base_transformer = actual_model._first_module().auto_model\n",
    "            outputs = base_transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            pooled_output = outputs.pooler_output\n",
    "            \n",
    "            embeddings.append(pooled_output.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def calculate_similarities(test_embeddings, train_embeddings, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculate similarities between test and train embeddings using different metrics.\n",
    "    \n",
    "    Args:\n",
    "        test_embeddings: numpy array of shape (n_test, embedding_dim)\n",
    "        train_embeddings: numpy array of shape (n_train, embedding_dim)\n",
    "        similarity_metric: one of 'cosine', 'euclidean', 'jaccard'\n",
    "        \n",
    "    Returns:\n",
    "        similarities: numpy array of shape (n_test, n_train)\n",
    "    \"\"\"\n",
    "    if similarity_metric == 'cosine':\n",
    "        return cosine_similarity(test_embeddings, train_embeddings)\n",
    "    \n",
    "    elif similarity_metric == 'euclidean':\n",
    "        # Convert distances to similarities (higher is more similar)\n",
    "        distances = euclidean_distances(test_embeddings, train_embeddings)\n",
    "        max_dist = np.max(distances)\n",
    "        if max_dist == 0:\n",
    "            return np.ones_like(distances)\n",
    "        return 1 - (distances / max_dist)\n",
    "    \n",
    "    elif similarity_metric == 'jaccard':\n",
    "        # For Jaccard, we need to binarize the embeddings\n",
    "        # We'll use the median of each dimension as a threshold\n",
    "        binary_test = test_embeddings > np.median(test_embeddings, axis=0)\n",
    "        binary_train = train_embeddings > np.median(train_embeddings, axis=0)\n",
    "        \n",
    "        similarities = np.zeros((len(test_embeddings), len(train_embeddings)))\n",
    "        \n",
    "        for i in range(len(test_embeddings)):\n",
    "            for j in range(len(train_embeddings)):\n",
    "                intersection = np.logical_and(binary_test[i], binary_train[j]).sum()\n",
    "                union = np.logical_or(binary_test[i], binary_train[j]).sum()\n",
    "                similarities[i, j] = intersection / union if union > 0 else 0\n",
    "                \n",
    "        return similarities\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown similarity metric: {similarity_metric}\")\n",
    "\n",
    "def train_sbert_epoch(model, train_loader, criterion, optimizer, device, gradient_accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    if (i + 1) % gradient_accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate_sbert(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def calculate_label_based_metrics(test_labels, train_labels, similar_indices, k_values=[1, 3, 5]):\n",
    "    \"\"\"\n",
    "    Calculate precision@k, recall@k, F1@k and other metrics based on label matching.\n",
    "    \n",
    "    Args:\n",
    "        test_labels: list of label lists\n",
    "        train_labels: list of label lists\n",
    "        similar_indices: list of arrays containing indices of similar items\n",
    "        k_values: list of k values for metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for k in k_values:\n",
    "        metrics.update({\n",
    "            f'precision@{k}': [],\n",
    "            f'recall@{k}': [],\n",
    "            f'f1@{k}': []\n",
    "        })\n",
    "    \n",
    "    metrics.update({\n",
    "        'avg_label_overlap': [],\n",
    "        'total_matches': 0,\n",
    "        'total_test_samples': len(test_labels)\n",
    "    })\n",
    "    \n",
    "    for i, test_label_set in enumerate(test_labels):\n",
    "        test_labels_set = set(test_label_set)\n",
    "        if not test_labels_set:  # Skip empty test label sets\n",
    "            continue\n",
    "            \n",
    "        retrieved_indices = similar_indices[i]\n",
    "        \n",
    "        for k in k_values:\n",
    "            # Consider only top-k results\n",
    "            top_k_indices = retrieved_indices[:k]\n",
    "            \n",
    "            # Calculate true positives across all top-k recommendations\n",
    "            retrieved_labels_set = set()\n",
    "            for idx in top_k_indices:\n",
    "                retrieved_labels_set.update(train_labels[idx])\n",
    "            \n",
    "            true_positives = len(test_labels_set.intersection(retrieved_labels_set))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision = true_positives / len(retrieved_labels_set) if retrieved_labels_set else 0\n",
    "            recall = true_positives / len(test_labels_set) if test_labels_set else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            metrics[f'precision@{k}'].append(precision)\n",
    "            metrics[f'recall@{k}'].append(recall)\n",
    "            metrics[f'f1@{k}'].append(f1)\n",
    "        \n",
    "        # Calculate label overlap for all top-k results\n",
    "        label_overlaps = []\n",
    "        for j, idx in enumerate(retrieved_indices):\n",
    "            train_labels_set = set(train_labels[idx])\n",
    "            if test_labels_set & train_labels_set:  # If there's an intersection\n",
    "                overlap = len(test_labels_set & train_labels_set) / len(test_labels_set | train_labels_set)\n",
    "                label_overlaps.append(overlap)\n",
    "                \n",
    "                # Count if there's at least one match\n",
    "                if j == 0:  # Only count once per test sample\n",
    "                    metrics['total_matches'] += 1\n",
    "        \n",
    "        avg_overlap = np.mean(label_overlaps) if label_overlaps else 0\n",
    "        metrics['avg_label_overlap'].append(avg_overlap)\n",
    "    \n",
    "    # Calculate averages\n",
    "    for k in k_values:\n",
    "        metrics[f'avg_precision@{k}'] = np.mean(metrics[f'precision@{k}'])\n",
    "        metrics[f'avg_recall@{k}'] = np.mean(metrics[f'recall@{k}'])\n",
    "        metrics[f'avg_f1@{k}'] = np.mean(metrics[f'f1@{k}'])\n",
    "    \n",
    "    metrics['avg_label_overlap'] = np.mean(metrics['avg_label_overlap'])\n",
    "    metrics['match_rate'] = metrics['total_matches'] / metrics['total_test_samples']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def create_stratification_labels(labels_list, min_samples_per_label=2):\n",
    "    \"\"\"\n",
    "    Create stratification labels that ensure each label has enough samples.\n",
    "    Only considers labels that appear frequently enough for stratification.\n",
    "    \"\"\"\n",
    "    # Count label occurrences\n",
    "    label_counts = Counter([label for labels in labels_list for label in labels])\n",
    "    \n",
    "    # Keep only labels that appear frequently enough\n",
    "    frequent_labels = {label for label, count in label_counts.items() if count >= min_samples_per_label}\n",
    "    \n",
    "    # Create binary indicators only for frequent labels\n",
    "    stratification_indicators = []\n",
    "    for labels in labels_list:\n",
    "        # Create indicator only for frequent labels\n",
    "        indicator = tuple(sorted(label for label in labels if label in frequent_labels))\n",
    "        # If no frequent labels, use a special category\n",
    "        if not indicator:\n",
    "            indicator = ('rare_combination',)\n",
    "        stratification_indicators.append(indicator)\n",
    "    \n",
    "    return stratification_indicators\n",
    "\n",
    "def calculate_and_evaluate_similarity(test_embeddings, filtered_train_embeddings, test_labels, original_train_labels, original_index_map, k_values, run_dir):\n",
    "    \"\"\"\n",
    "    Calculates similarities between test and filtered train embeddings,\n",
    "    maps indices back to original, evaluates metrics, and saves plots.\n",
    "    \n",
    "    Args:\n",
    "        test_embeddings: numpy array (n_test, dim)\n",
    "        filtered_train_embeddings: numpy array (n_filtered_train, dim)\n",
    "        test_labels: list of lists (ground truth labels for test set)\n",
    "        original_train_labels: list of lists (ground truth labels for the *original* full training set)\n",
    "        original_index_map: dict mapping filtered train index -> original train index\n",
    "        k_values: list of k values for evaluation\n",
    "        run_dir: directory to save plots and results\n",
    "\n",
    "    Returns:\n",
    "        tuple: (results_dict, all_similar_indices_mapped, all_similarity_scores_mapped)\n",
    "            results_dict: Dictionary containing aggregated metrics for each similarity type.\n",
    "            all_similar_indices_mapped: Dict mapping metric -> list of lists of *original* train indices.\n",
    "            all_similarity_scores_mapped: Dict mapping metric -> list of lists of similarity scores.\n",
    "    \"\"\"\n",
    "    similarity_metrics = ['cosine', 'euclidean', 'jaccard']\n",
    "    results = {} # Store aggregated metrics per similarity type\n",
    "    all_similar_indices_mapped = {} # Store top-k original indices per metric\n",
    "    all_similarity_scores_mapped = {} # Store corresponding top-k scores per metric\n",
    "    max_k = max(k_values)\n",
    "\n",
    "    if filtered_train_embeddings.shape[0] == 0:\n",
    "        print(\"Warning: No filtered training embeddings to compare against. Skipping similarity calculation and evaluation.\")\n",
    "        # Return empty results\n",
    "        for metric in similarity_metrics:\n",
    "            results[metric] = {f'avg_precision@{k}': 0 for k in k_values}\n",
    "            results[metric].update({\n",
    "                f'avg_recall@{k}': 0 for k in k_values\n",
    "            })\n",
    "            results[metric].update({\n",
    "                f'avg_f1@{k}': 0 for k in k_values\n",
    "            })\n",
    "            results[metric]['match_rate'] = 0\n",
    "            results[metric]['avg_label_overlap'] = 0\n",
    "            all_similar_indices_mapped[metric] = [[] for _ in range(len(test_embeddings))]\n",
    "            all_similarity_scores_mapped[metric] = [[] for _ in range(len(test_embeddings))]\n",
    "        return results, all_similar_indices_mapped, all_similarity_scores_mapped\n",
    "\n",
    "    for metric in similarity_metrics:\n",
    "        print(f\"\\nCalculating {metric} similarities...\")\n",
    "        # Calculate full similarity matrix between test and the filtered training set\n",
    "        similarities = calculate_similarities(\n",
    "            test_embeddings, filtered_train_embeddings, metric\n",
    "        )\n",
    "        \n",
    "        # Get top-k indices and scores *within the filtered set*\n",
    "        # Argsort gives indices of the smallest values first, use [::-1] for descending order\n",
    "        num_filtered_train = filtered_train_embeddings.shape[0]\n",
    "        current_max_k = min(max_k, num_filtered_train)\n",
    "\n",
    "        # Get indices of top k scores for each test sample\n",
    "        top_k_indices_filtered = np.argsort(similarities, axis=1)[:, -current_max_k:][:, ::-1] \n",
    "        # Get the actual scores corresponding to these indices\n",
    "        top_k_scores = np.array([similarities[i, top_k_indices_filtered[i]] for i in range(len(test_embeddings))])\n",
    "\n",
    "        # Map filtered indices back to original training set indices\n",
    "        similar_original_indices_list = []\n",
    "        for filtered_indices_row in top_k_indices_filtered:\n",
    "            original_indices_row = [original_index_map[filt_idx] for filt_idx in filtered_indices_row]\n",
    "            similar_original_indices_list.append(original_indices_row)\n",
    "            \n",
    "        # Store mapped indices and scores for this metric\n",
    "        all_similar_indices_mapped[metric] = similar_original_indices_list\n",
    "        all_similarity_scores_mapped[metric] = top_k_scores.tolist() # Store as list\n",
    "\n",
    "        # Evaluate using original labels\n",
    "        print(f\"Evaluating metrics for {metric} similarity...\")\n",
    "        metrics = calculate_label_based_metrics(\n",
    "            test_labels, \n",
    "            original_train_labels, # Pass the original full set of train labels\n",
    "            similar_original_indices_list, # Pass the list of lists of mapped original indices\n",
    "            k_values\n",
    "        )\n",
    "\n",
    "        # Store aggregated metrics\n",
    "        results[metric] = metrics\n",
    "\n",
    "    # Create comparison plots (similar to before)\n",
    "    print(\"\\nGenerating comparison plots...\")\n",
    "    for k in k_values:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        metrics_to_plot = [f'avg_precision@{k}', f'avg_recall@{k}', f'avg_f1@{k}']\n",
    "        x = np.arange(len(metrics_to_plot))\n",
    "        width = 0.25\n",
    "\n",
    "        for i, metric in enumerate(similarity_metrics):\n",
    "            values = [results[metric].get(m, 0) for m in metrics_to_plot] # Use .get for safety if metric calculation failed\n",
    "            plt.bar(x + i*width, values, width, label=f'{metric.capitalize()} Similarity')\n",
    "\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(f'Comparison of Similarity Metrics at k={k} (Filtered Train Set)')\n",
    "        plt.xticks(x + width, [m.split('@')[0].replace(\"avg_\", \"\").capitalize() for m in metrics_to_plot])\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis='y')\n",
    "        plot_path = os.path.join(run_dir, f'similarity_metrics_comparison_k{k}_filtered_train.png')\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Saved comparison plot to {plot_path}\")\n",
    "        plt.close()\n",
    "\n",
    "    # Save comparison results dictionary\n",
    "    comparison_results_path = os.path.join(run_dir, 'similarity_metrics_comparison_filtered_train.json')\n",
    "    with open(comparison_results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"Saved comparison metrics to {comparison_results_path}\")\n",
    "\n",
    "    return results, all_similar_indices_mapped, all_similarity_scores_mapped\n",
    "\n",
    "def predict_labels_with_deberta(texts, model, tokenizer, mlb_deberta, device, batch_size, max_length, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict multi-labels for given texts using a trained DeBERTa model.\n",
    "\n",
    "    Args:\n",
    "        texts (pd.Series): Series of input texts.\n",
    "        model (nn.Module): Trained DeBERTaClassifier model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): DeBERTa tokenizer.\n",
    "        mlb_deberta (MultiLabelBinarizer): Fitted MultiLabelBinarizer corresponding to the DeBERTa model's labels.\n",
    "        device: Device to perform inference on.\n",
    "        batch_size (int): Batch size for prediction.\n",
    "        max_length (int): Maximum token length for tokenizer.\n",
    "        threshold (float): Threshold for converting sigmoid outputs to binary labels.\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: List of predicted label lists for each input text.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    dataset = DeBERTaIssueDataset(texts, tokenizer, max_length)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    all_preds_binary = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Predicting labels with DeBERTa\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            # Apply sigmoid and threshold\n",
    "            predictions_binary = (torch.sigmoid(outputs) >= threshold).cpu().numpy()\n",
    "            all_preds_binary.append(predictions_binary)\n",
    "\n",
    "    all_preds_binary = np.vstack(all_preds_binary)\n",
    "\n",
    "    # Convert binary predictions back to label names\n",
    "    predicted_labels = mlb_deberta.inverse_transform(all_preds_binary)\n",
    "    # Convert tuples back to lists\n",
    "    predicted_labels_list = [list(labels) for labels in predicted_labels]\n",
    "\n",
    "    return predicted_labels_list\n",
    "\n",
    "def main(args):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    results_dir = args.results_dir\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(results_dir, f\"run_{timestamp}_{args.text_column}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Loading data from {args.data_path}...\")\n",
    "    df = pd.read_json(args.data_path)\n",
    "    \n",
    "    if args.text_column not in df.columns:\n",
    "        available_columns = [col for col in df.columns if col.startswith('all_text')]\n",
    "        print(f\"Text column '{args.text_column}' not found. Available text columns: {available_columns}\")\n",
    "        if len(available_columns) == 0:\n",
    "            raise ValueError(\"No text columns found in the data\")\n",
    "        args.text_column = available_columns[0]\n",
    "        print(f\"Using '{args.text_column}' instead\")\n",
    "    \n",
    "    texts, filtered_labels_all = prepare_data(\n",
    "        df, \n",
    "        text_column=args.text_column,\n",
    "        min_label_freq=args.min_label_freq, \n",
    "        max_label_len=args.max_label_len\n",
    "    )\n",
    "    \n",
    "    # Create stratification indicators *before* split\n",
    "    print(\"\\nPreparing stratified split...\")\n",
    "    stratification_indicators = create_stratification_labels(filtered_labels_all)\n",
    "    \n",
    "    try:\n",
    "        # Try stratified split and convert to lists\n",
    "        train_indices, test_indices = train_test_split(\n",
    "            range(len(texts)),\n",
    "            test_size=0.1,\n",
    "            random_state=42,\n",
    "            stratify=stratification_indicators\n",
    "        )\n",
    "        \n",
    "        # Use indices to split both texts and labels\n",
    "        original_train_texts = texts.iloc[train_indices].reset_index(drop=True) # Keep original training data\n",
    "        test_texts = texts.iloc[test_indices].reset_index(drop=True)\n",
    "        original_train_labels = [filtered_labels_all[i] for i in train_indices] # Keep original training labels\n",
    "        test_labels = [filtered_labels_all[i] for i in test_indices]\n",
    "        \n",
    "        print(\"Successfully performed stratified split\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Could not perform stratified split ({str(e)})\")\n",
    "        print(\"Falling back to random split\")\n",
    "        \n",
    "        # Random split with indices\n",
    "        train_indices, test_indices = train_test_split(\n",
    "            range(len(texts)),\n",
    "            test_size=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Use indices to split both texts and labels\n",
    "        original_train_texts = texts.iloc[train_indices].reset_index(drop=True) # Keep original training data\n",
    "        test_texts = texts.iloc[test_indices].reset_index(drop=True)\n",
    "        original_train_labels = [filtered_labels_all[i] for i in train_indices] # Keep original training labels\n",
    "        test_labels = [filtered_labels_all[i] for i in test_indices]\n",
    "\n",
    "    print(f\"\\nOriginal Training Samples: {len(original_train_texts)}\")\n",
    "    print(f\"Test Samples: {len(test_texts)}\")\n",
    "\n",
    "    # --- DeBERTa Prediction FIRST ---\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "    print(\"\\nLoading DeBERTa model and resources for prediction...\")\n",
    "    deberta_tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "\n",
    "    with open(args.deberta_label_encoder_path, 'r') as f:\n",
    "        encoder_data = json.load(f)\n",
    "    mlb_deberta = MultiLabelBinarizer()\n",
    "    mlb_deberta.classes_ = np.array(encoder_data['classes'])\n",
    "    num_deberta_labels = len(mlb_deberta.classes_)\n",
    "\n",
    "    selected_deberta_labels = None\n",
    "    if args.deberta_selected_labels_path:\n",
    "        print(f\"Loading selected labels from {args.deberta_selected_labels_path}\")\n",
    "        with open(args.deberta_selected_labels_path, 'r') as f:\n",
    "            selected_data = json.load(f)\n",
    "        selected_deberta_labels = selected_data['selected_labels']\n",
    "        mlb_deberta.classes_ = np.array([lbl for lbl in selected_deberta_labels if lbl in mlb_deberta.classes_])\n",
    "        num_deberta_labels = len(mlb_deberta.classes_)\n",
    "        print(f\"Using {num_deberta_labels} selected labels for DeBERTa prediction.\")\n",
    "    else:\n",
    "        print(f\"Using all {num_deberta_labels} labels from the DeBERTa encoder.\")\n",
    "\n",
    "    deberta_model = DeBERTaClassifier(num_labels=num_deberta_labels).to(device)\n",
    "    deberta_model.load_state_dict(torch.load(args.deberta_model_path, map_location=device))\n",
    "    deberta_model.eval() \n",
    "\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for DeBERTa prediction!\")\n",
    "        deberta_model = torch.nn.DataParallel(deberta_model)\n",
    "\n",
    "    print(\"\\nPredicting labels for the test set using DeBERTa...\")\n",
    "    predicted_test_labels = predict_labels_with_deberta(\n",
    "        texts=test_texts, \n",
    "        model=deberta_model,\n",
    "        tokenizer=deberta_tokenizer,\n",
    "        mlb_deberta=mlb_deberta,\n",
    "        device=device,\n",
    "        batch_size=args.batch_size,\n",
    "        max_length=512,\n",
    "        threshold=args.deberta_threshold\n",
    "    )\n",
    "    print(f\"Finished predicting labels for {len(predicted_test_labels)} test samples.\")\n",
    "    \n",
    "    # Cleanup DeBERTa model to free memory if possible\n",
    "    del deberta_model\n",
    "    del deberta_tokenizer\n",
    "    del mlb_deberta\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    # --- End DeBERTa Prediction ---\n",
    "\n",
    "    # --- Filter Training Data based on ALL Predicted Test Labels ---\n",
    "    print(\"\\nFiltering training data based on globally predicted test labels...\")\n",
    "    all_predicted_labels_set = set(label for labels in predicted_test_labels for label in labels)\n",
    "    print(f\"Total unique labels predicted across test set: {len(all_predicted_labels_set)}\")\n",
    "\n",
    "    filtered_train_indices_original = [] # Store original indices\n",
    "    filtered_train_texts_list = []\n",
    "    filtered_train_labels_list = []\n",
    "\n",
    "    for i, (text, labels) in enumerate(zip(original_train_texts, original_train_labels)):\n",
    "        if any(label in all_predicted_labels_set for label in labels):\n",
    "            filtered_train_indices_original.append(i) # Store original index\n",
    "            filtered_train_texts_list.append(text)\n",
    "            filtered_train_labels_list.append(labels)\n",
    "\n",
    "    # Create filtered Series/lists\n",
    "    filtered_train_texts = pd.Series(filtered_train_texts_list)\n",
    "    filtered_train_labels = filtered_train_labels_list # Already a list\n",
    "    \n",
    "    # Map from filtered index (0 to N-1) back to original index\n",
    "    original_index_map = {filtered_idx: original_idx for filtered_idx, original_idx in enumerate(filtered_train_indices_original)}\n",
    "\n",
    "    print(f\"Filtered Training Samples: {len(filtered_train_texts)}\")\n",
    "    if len(filtered_train_texts) == 0:\n",
    "        print(\"Warning: Filtering resulted in zero training samples. Similarity search will be skipped.\")\n",
    "        # Optionally exit or handle this case appropriately\n",
    "        return {} \n",
    "    # --- End Training Data Filtering ---\n",
    "\n",
    "    # --- Initialize and Prepare SBERT Model ---\n",
    "    # Initialize label encoder *only* based on the filtered training labels now\n",
    "    print(\"\\nInitializing label encoder based on filtered training labels...\")\n",
    "    mlb_sbert = MultiLabelBinarizer()\n",
    "    mlb_sbert.fit(filtered_train_labels) # Fit only on labels that remain\n",
    "    print(f\"Number of unique SBERT labels (post-filtering): {len(mlb_sbert.classes_)}\")\n",
    "    \n",
    "    print(\"\\nInitializing SBERT model...\")\n",
    "    model = FineTunedSBERT('all-mpnet-base-v2', num_labels=len(mlb_sbert.classes_)) # Use count from filtered labels\n",
    "    model.max_seq_length = 512\n",
    "    model.use_fast_tokenizer = True\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Device already set\n",
    "\n",
    "    sbert_fine_tuned = False\n",
    "    if args.sbert_model_path:\n",
    "        if os.path.exists(args.sbert_model_path):\n",
    "            print(f\"Loading pre-trained SBERT model from {args.sbert_model_path}...\")\n",
    "            try:\n",
    "                # Adjust loading if the number of labels changed significantly due to filtering\n",
    "                # This might require careful handling depending on how the model was saved\n",
    "                # Option 1: Load weights except for the classifier head if sizes mismatch\n",
    "                pretrained_dict = torch.load(args.sbert_model_path, map_location='cpu')\n",
    "                model_dict = model.state_dict()\n",
    "                \n",
    "                # Filter out unnecessary keys and potentially size-mismatched classifier\n",
    "                pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].size() == v.size()}\n",
    "                \n",
    "                model_dict.update(pretrained_dict)\n",
    "                model.load_state_dict(model_dict, strict=False) # Use strict=False to ignore non-matching keys/sizes\n",
    "                print(\"Pre-trained SBERT model weights loaded (classifier may be reinitialized if size mismatched).\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error loading SBERT model: {e}. Check compatibility with filtered label set.\")\n",
    "                 print(\"Proceeding with base SBERT model.\")\n",
    "                 model = FineTunedSBERT('all-mpnet-base-v2', num_labels=len(mlb_sbert.classes_)) # Re-initialize if loading failed\n",
    "\n",
    "            model.eval() \n",
    "            sbert_fine_tuned = True \n",
    "        else:\n",
    "            print(f\"Warning: SBERT model path {args.sbert_model_path} not found. Proceeding without loading.\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for SBERT!\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    # --- End SBERT Model Prep ---\n",
    "\n",
    "    # --- Optional SBERT Fine-tuning (on FILTERED data) ---\n",
    "    best_loss = float('inf')\n",
    "    train_losses = []\n",
    "    test_losses = [] # Note: Test set isn't filtered, this validation might be less meaningful now\n",
    "    if args.training_epochs > 0 and not args.sbert_model_path:\n",
    "        # Create datasets using filtered training data and original test data\n",
    "        # The SBERT MultiLabelDataset needs the SBERT model instance passed to it\n",
    "        train_dataset = MultiLabelDataset(filtered_train_texts, filtered_train_labels, model, label_encoder=mlb_sbert)\n",
    "        # For validation, use the original test set but with the SBERT label encoder\n",
    "        # This evaluates how well the model generalizes to labels *after* being trained only on the filtered set\n",
    "        test_dataset = MultiLabelDataset(test_texts, test_labels, model, label_encoder=mlb_sbert) \n",
    "\n",
    "        effective_batch_size = args.batch_size // 4 if args.batch_size >= 4 else 1\n",
    "        gradient_accumulation_steps = 4 if args.batch_size >= 4 else 1\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=effective_batch_size, shuffle=True, num_workers=0)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=effective_batch_size, num_workers=0)\n",
    "        \n",
    "        criterion = BCEWithLogitsLoss()\n",
    "        # Ensure optimizer uses parameters from the potentially wrapped model\n",
    "        optimizer_params = model.module.parameters() if isinstance(model, torch.nn.DataParallel) else model.parameters()\n",
    "        optimizer = AdamW(optimizer_params, lr=2e-5)\n",
    "        \n",
    "        print(f\"\\nStarting SBERT fine-tuning on FILTERED data for {args.training_epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(args.training_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{args.training_epochs}\")\n",
    "            train_loss = train_sbert_epoch(model, train_loader, criterion, optimizer, device, gradient_accumulation_steps)\n",
    "            test_loss = validate_sbert(model, test_loader, criterion, device) # Validate on original test set\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            print(f\"Filtered Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Original Test Loss: {test_loss:.4f}\")\n",
    "            \n",
    "            if test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                model_to_save = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "                torch.save(model_to_save.state_dict(), os.path.join(run_dir, 'best_sbert_model_filtered_train.pt'))\n",
    "                print(\"Saved new best model (trained on filtered data)\")\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        sbert_fine_tuned = True \n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Filtered Train Loss')\n",
    "        plt.plot(test_losses, label='Original Test Loss')\n",
    "        plt.title('SBERT Training (Filtered Train) and Test Losses')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plot_path = os.path.join(run_dir, 'sbert_filtered_training_losses.png')\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Saved SBERT training loss plot to {plot_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        best_model_path = os.path.join(run_dir, 'best_sbert_model_filtered_train.pt')\n",
    "        if os.path.exists(best_model_path):\n",
    "            print(f\"Loading best SBERT model (trained on filtered data) from {best_model_path}\")\n",
    "            base_model = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "            base_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        else:\n",
    "            print(\"Warning: Best model file not found after training. Using the final state.\")\n",
    "        model.eval() \n",
    "    elif args.sbert_model_path:\n",
    "        print(\"Skipping SBERT fine-tuning as a pre-trained model was loaded.\")\n",
    "    else: \n",
    "        print(\"Skipping SBERT fine-tuning (training_epochs=0 and no pre-trained model path provided). Using base/loaded SBERT model.\")\n",
    "        model.eval()\n",
    "    # --- End SBERT Fine-tuning ---\n",
    "\n",
    "    # --- Generate Embeddings (Test and Filtered Train) ---\n",
    "    print(\"\\nGenerating embeddings with final SBERT model...\")\n",
    "    # Embed the original test texts\n",
    "    test_embeddings = get_embeddings(test_texts, model, batch_size=args.batch_size)\n",
    "    # Embed ONLY the filtered training texts\n",
    "    filtered_train_embeddings = get_embeddings(filtered_train_texts, model, batch_size=args.batch_size)\n",
    "    print(f\"Generated {test_embeddings.shape[0]} test embeddings.\")\n",
    "    print(f\"Generated {filtered_train_embeddings.shape[0]} filtered training embeddings.\")\n",
    "    # --- End Embedding Generation ---\n",
    "\n",
    "    # --- Compare Similarity Metrics (using filtered embeddings) ---\n",
    "    print(\"\\nCalculating similarities and evaluating metrics...\")\n",
    "    k_values = [1, 3, 5, 10] \n",
    "    \n",
    "    # Call the modified comparison function (needs implementation below)\n",
    "    # Pass original_train_labels and the original_index_map\n",
    "    similarity_comparison_results, all_similar_indices_mapped, all_similarity_scores_mapped = calculate_and_evaluate_similarity(\n",
    "        test_embeddings, filtered_train_embeddings, \n",
    "        test_labels, original_train_labels, # Pass ORIGINAL train labels for evaluation\n",
    "        original_index_map, # Pass the map\n",
    "        k_values, run_dir\n",
    "    )\n",
    "\n",
    "    # --- Save Detailed Results ---\n",
    "    all_similarity_details = {}\n",
    "    similarity_metrics = ['cosine', 'euclidean', 'jaccard'] \n",
    "\n",
    "    for metric in similarity_metrics:\n",
    "        print(f\"\\nProcessing results for {metric} similarity...\")\n",
    "        # Retrieve pre-calculated *mapped* indices and scores\n",
    "        # Note: calculate_and_evaluate_similarity should return indices mapped back to original\n",
    "        similar_original_indices = all_similar_indices_mapped[metric] \n",
    "        similarity_scores = all_similarity_scores_mapped[metric] # These scores correspond to the mapped indices\n",
    "\n",
    "        # Metrics are already calculated in calculate_and_evaluate_similarity\n",
    "        label_metrics = similarity_comparison_results[metric]\n",
    "\n",
    "        # Print metrics \n",
    "        print(f\"\\n{metric.capitalize()} Similarity Metrics (using filtered train set):\")\n",
    "        print(f\"Match Rate (at least one match): {label_metrics['match_rate']:.4f}\")\n",
    "        for k in k_values:\n",
    "            print(f\"Average Precision@{k}: {label_metrics[f'avg_precision@{k}']:.4f}\")\n",
    "            print(f\"Average Recall@{k}: {label_metrics[f'avg_recall@{k}']:.4f}\")\n",
    "            print(f\"Average F1@{k}: {label_metrics[f'avg_f1@{k}']:.4f}\")\n",
    "        print(f\"Average Label Overlap: {label_metrics['avg_label_overlap']:.4f}\")\n",
    "\n",
    "        # Generate and save detailed similarity results per test sample\n",
    "        metric_similarity_results = []\n",
    "        # Use args.top_k for the detailed results file length\n",
    "        for i, (original_indices_for_test, scores_for_test) in enumerate(zip(similar_original_indices, similarity_scores)):\n",
    "            test_sample = {\n",
    "                'test_text': test_texts.iloc[i],\n",
    "                'test_labels': test_labels[i],\n",
    "                'predicted_test_labels': predicted_test_labels[i], # Add predicted labels for context\n",
    "                'similar_requests': []\n",
    "            }\n",
    "            test_labels_set = set(test_labels[i])\n",
    "            \n",
    "            # Ensure we don't try to access more items than available\n",
    "            num_retrieved = min(args.top_k, len(original_indices_for_test))\n",
    "\n",
    "            for j in range(num_retrieved):\n",
    "                original_idx = original_indices_for_test[j]\n",
    "                score = scores_for_test[j]\n",
    "                \n",
    "                # Use original_idx to fetch from original training data\n",
    "                train_text = original_train_texts.iloc[original_idx]\n",
    "                train_label_list = original_train_labels[original_idx] \n",
    "                train_labels_set = set(train_label_list)\n",
    "                \n",
    "                matching_labels = list(test_labels_set & train_labels_set)\n",
    "                similar_request = {\n",
    "                    'rank': j + 1,\n",
    "                    'original_train_index': int(original_idx), # Store original index\n",
    "                    'text': train_text,\n",
    "                    'labels': train_label_list,\n",
    "                    'similarity_score': float(score),\n",
    "                    'matching_labels': matching_labels,\n",
    "                    'has_matching_label': len(matching_labels) > 0\n",
    "                }\n",
    "                test_sample['similar_requests'].append(similar_request)\n",
    "            metric_similarity_results.append(test_sample)\n",
    "\n",
    "        all_similarity_details[metric] = metric_similarity_results\n",
    "        with open(os.path.join(run_dir, f'{metric}_similarity_results_filtered_train.json'), 'w') as f:\n",
    "            json.dump(metric_similarity_results, f, indent=4)\n",
    "\n",
    "    # Prepare combined results dictionary\n",
    "    results = {\n",
    "        'text_column': args.text_column,\n",
    "        'filtering_info': {\n",
    "            'total_unique_predicted_labels': len(all_predicted_labels_set),\n",
    "            'original_training_samples': len(original_train_texts),\n",
    "            'filtered_training_samples': len(filtered_train_texts),\n",
    "        },\n",
    "        'similarity_comparison': similarity_comparison_results, # Metrics calculated by calculate_and_evaluate_similarity\n",
    "        'sbert_training_on_filtered': {\n",
    "            'performed': args.training_epochs > 0 and not args.sbert_model_path,\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'best_loss': float(best_loss) if args.training_epochs > 0 and 'best_loss' in locals() and best_loss != float('inf') else None\n",
    "        },\n",
    "        'deberta_info': {\n",
    "            'model_path': args.deberta_model_path,\n",
    "            'label_encoder_path': args.deberta_label_encoder_path,\n",
    "            'selected_labels_path': args.deberta_selected_labels_path,\n",
    "            'prediction_threshold': args.deberta_threshold,\n",
    "            'num_predicted_labels_used': num_deberta_labels\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(run_dir, 'all_metrics_results_filtered_train.json'), 'w') as f:\n",
    "        def convert_numpy(obj):\n",
    "            if isinstance(obj, np.integer): return int(obj)\n",
    "            elif isinstance(obj, np.floating): return float(obj)\n",
    "            elif isinstance(obj, np.ndarray): return obj.tolist()\n",
    "            elif isinstance(obj, dict): return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list): return [convert_numpy(i) for i in obj]\n",
    "            return obj\n",
    "        results_serializable = convert_numpy(results)\n",
    "        json.dump(results_serializable, f, indent=4)\n",
    "\n",
    "    print(f\"\\nAnalysis completed! Results saved to {run_dir}\")\n",
    "\n",
    "    # Return relevant data if needed downstream (adjust as necessary)\n",
    "    return {\n",
    "        'model': model,\n",
    "        'test_embeddings': test_embeddings,\n",
    "        'filtered_train_embeddings': filtered_train_embeddings,\n",
    "        'original_index_map': original_index_map,\n",
    "        'all_similar_indices_mapped': all_similar_indices_mapped,\n",
    "        'all_similarity_scores_mapped': all_similarity_scores_mapped,\n",
    "        'similarity_details': all_similarity_details,\n",
    "        'results_dir': run_dir\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:20:40.090097Z",
     "iopub.status.busy": "2025-04-22T10:20:40.089556Z",
     "iopub.status.idle": "2025-04-22T10:21:34.358298Z",
     "shell.execute_reply": "2025-04-22T10:21:34.357512Z",
     "shell.execute_reply.started": "2025-04-22T10:20:40.090072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json...\n",
      "\n",
      "Preparing stratified split...\n",
      "Warning: Could not perform stratified split (setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1222,) + inhomogeneous part.)\n",
      "Falling back to random split\n",
      "\n",
      "Original Training Samples: 1099\n",
      "Test Samples: 123\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "Loading DeBERTa model and resources for prediction...\n",
      "Loading selected labels from /kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/selected_labels.json\n",
      "Using 10 selected labels for DeBERTa prediction.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09189191597d453db93cdc01485fa019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056d6ed72c9745edae00b2e77f96821b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3846907644.py:666: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  deberta_model.load_state_dict(torch.load(args.deberta_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for DeBERTa prediction!\n",
      "\n",
      "Predicting labels for the test set using DeBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting labels with DeBERTa: 100%|| 4/4 [00:04<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting labels for 123 test samples.\n",
      "\n",
      "Filtering training data based on globally predicted test labels...\n",
      "Total unique labels predicted across test set: 9\n",
      "Filtered Training Samples: 860\n",
      "\n",
      "Initializing label encoder based on filtered training labels...\n",
      "Number of unique SBERT labels (post-filtering): 18\n",
      "\n",
      "Initializing SBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b4cf2e493f43db97de1964c21fef10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7eb410017244ae2bb741b3a1143a62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a298a8436c4599acbc5a7ef8ec4dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff510a8ddb244feb2054514ce528e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e333b2f29c4d9781b63f240b8555f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf694fd88fbe465eae546bf5ef0f91b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da2415120be4b088c9c8486c8a56c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3821fb876e0542fd9b5a993d00592f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d3597ce0bd4afba1125535adde3160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147ac79cf4404bbe9a61a9f0c1d36234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcdbce70bcd484faab8662165238f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for SBERT!\n",
      "Skipping SBERT fine-tuning (training_epochs=0 and no pre-trained model path provided). Using base/loaded SBERT model.\n",
      "\n",
      "Generating embeddings with final SBERT model...\n",
      "Using 2 GPUs for embedding generation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|| 4/4 [00:04<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for embedding generation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|| 27/27 [00:28<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 123 test embeddings.\n",
      "Generated 860 filtered training embeddings.\n",
      "\n",
      "Calculating similarities and evaluating metrics...\n",
      "\n",
      "Calculating cosine similarities...\n",
      "Evaluating metrics for cosine similarity...\n",
      "\n",
      "Calculating euclidean similarities...\n",
      "Evaluating metrics for euclidean similarity...\n",
      "\n",
      "Calculating jaccard similarities...\n",
      "Evaluating metrics for jaccard similarity...\n",
      "\n",
      "Generating comparison plots...\n",
      "Saved comparison plot to ./results/run_20250422_102040_all_text_0.5/similarity_metrics_comparison_k1_filtered_train.png\n",
      "Saved comparison plot to ./results/run_20250422_102040_all_text_0.5/similarity_metrics_comparison_k3_filtered_train.png\n",
      "Saved comparison plot to ./results/run_20250422_102040_all_text_0.5/similarity_metrics_comparison_k5_filtered_train.png\n",
      "Saved comparison plot to ./results/run_20250422_102040_all_text_0.5/similarity_metrics_comparison_k10_filtered_train.png\n",
      "Saved comparison metrics to ./results/run_20250422_102040_all_text_0.5/similarity_metrics_comparison_filtered_train.json\n",
      "\n",
      "Processing results for cosine similarity...\n",
      "\n",
      "Cosine Similarity Metrics (using filtered train set):\n",
      "Match Rate (at least one match): 0.7480\n",
      "Average Precision@1: 0.5978\n",
      "Average Recall@1: 0.6524\n",
      "Average F1@1: 0.5851\n",
      "Average Precision@3: 0.5012\n",
      "Average Recall@3: 0.8306\n",
      "Average F1@3: 0.5784\n",
      "Average Precision@5: 0.4132\n",
      "Average Recall@5: 0.8715\n",
      "Average F1@5: 0.5202\n",
      "Average Precision@10: 0.3326\n",
      "Average Recall@10: 0.9186\n",
      "Average F1@10: 0.4524\n",
      "Average Label Overlap: 0.6509\n",
      "\n",
      "Processing results for euclidean similarity...\n",
      "\n",
      "Euclidean Similarity Metrics (using filtered train set):\n",
      "Match Rate (at least one match): 0.7398\n",
      "Average Precision@1: 0.5984\n",
      "Average Recall@1: 0.6416\n",
      "Average F1@1: 0.5833\n",
      "Average Precision@3: 0.4867\n",
      "Average Recall@3: 0.8015\n",
      "Average F1@3: 0.5641\n",
      "Average Precision@5: 0.4173\n",
      "Average Recall@5: 0.8767\n",
      "Average F1@5: 0.5241\n",
      "Average Precision@10: 0.3355\n",
      "Average Recall@10: 0.9224\n",
      "Average F1@10: 0.4601\n",
      "Average Label Overlap: 0.6619\n",
      "\n",
      "Processing results for jaccard similarity...\n",
      "\n",
      "Jaccard Similarity Metrics (using filtered train set):\n",
      "Match Rate (at least one match): 0.7398\n",
      "Average Precision@1: 0.6136\n",
      "Average Recall@1: 0.6299\n",
      "Average F1@1: 0.5852\n",
      "Average Precision@3: 0.4976\n",
      "Average Recall@3: 0.8130\n",
      "Average F1@3: 0.5688\n",
      "Average Precision@5: 0.4176\n",
      "Average Recall@5: 0.8566\n",
      "Average F1@5: 0.5197\n",
      "Average Precision@10: 0.3233\n",
      "Average Recall@10: 0.9260\n",
      "Average F1@10: 0.4426\n",
      "Average Label Overlap: 0.6612\n",
      "\n",
      "Analysis completed! Results saved to ./results/run_20250422_102040_all_text_0.5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Compare change requests using SBERT embeddings and similarity metrics, with optional DeBERTa label filtering.')\n",
    "    \n",
    "    parser.add_argument('--data_path', type=str, \n",
    "                        default=\"/kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json\",\n",
    "                        help='Path to the JSON data file')\n",
    "    parser.add_argument('--text_column', type=str, default='all_text_0.5',\n",
    "                        help='Column name with the text data to use for training')\n",
    "    parser.add_argument('--results_dir', type=str, default='./results',\n",
    "                        help='Directory to save results')\n",
    "    \n",
    "    parser.add_argument('--min_label_freq', type=int, default=5,\n",
    "                        help='Minimum frequency for a label to be considered')\n",
    "    parser.add_argument('--max_label_len', type=int, default=5,\n",
    "                        help='Maximum number of labels per sample')\n",
    "    \n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='Batch size for generating embeddings and training/prediction')\n",
    "    parser.add_argument('--training_epochs', type=int, default=0,\n",
    "                        help='Number of epochs for SBERT fine-tuning (0 to skip if not loading model)')\n",
    "    parser.add_argument('--top_k', type=int, default=5,\n",
    "                        help='Number of similar requests to find for each test sample')\n",
    "    \n",
    "    # SBERT Model Loading (Optional)\n",
    "    parser.add_argument('--sbert_model_path', type=str, default=None,\n",
    "                        help='Optional path to load a pre-trained/fine-tuned SBERT model state_dict (.pt file). If provided, SBERT fine-tuning is skipped.')\n",
    "\n",
    "    # DeBERTa Filtering Settings\n",
    "    parser.add_argument('--deberta_model_path', type=str,\n",
    "                        default=\"/kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/best_model_all_text_0.5.pt\",\n",
    "                        help='Path to the trained DeBERTa model state_dict (.pt file). Uses default if not specified.')\n",
    "    parser.add_argument('--deberta_label_encoder_path', type=str,\n",
    "                        default='/kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/label_encoder.json',\n",
    "                        help='Path to the DeBERTa label encoder (.json file). Uses default if not specified.')\n",
    "    parser.add_argument('--deberta_selected_labels_path', type=str,\n",
    "                        default=\"/kaggle/input/deberta-bug-with-fs-10-lables-max-length-2/pytorch/default/2/selected_labels.json\",\n",
    "                        help='Optional path to the selected labels JSON file if feature selection was used for DeBERTa. Uses default if not specified.')\n",
    "    parser.add_argument('--deberta_threshold', type=float, default=0.5,\n",
    "                        help='Threshold for DeBERTa label prediction')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    results = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Model with Augmented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max 5 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T12:35:47.858975Z",
     "iopub.status.busy": "2025-04-29T12:35:47.858709Z",
     "iopub.status.idle": "2025-04-29T12:37:25.531791Z",
     "shell.execute_reply": "2025-04-29T12:37:25.530935Z",
     "shell.execute_reply.started": "2025-04-29T12:35:47.858956Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json...\n",
      "\n",
      "Preparing stratified split...\n",
      "Warning: Could not perform stratified split (setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1222,) + inhomogeneous part.)\n",
      "Falling back to random split\n",
      "\n",
      "Original Training Samples: 1099\n",
      "Test Samples: 123\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "Loading DeBERTa model and resources for prediction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03add1eb3e8644b2913c5b6f92aea39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7fd2e73ca448f48d7198e4448f6d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eef38e831804c35a2795c0b667e3aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f94c37f4e749dca58986e0464137b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all 25 labels from the DeBERTa encoder.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da032d1767bf455c81590468750966fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3846907644.py:666: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  deberta_model.load_state_dict(torch.load(args.deberta_model_path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b7c7bfb1014286ac06bdb901e58c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for DeBERTa prediction!\n",
      "\n",
      "Predicting labels for the test set using DeBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting labels with DeBERTa:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Predicting labels with DeBERTa:  25%|       | 1/4 [00:02<00:08,  2.98s/it]\u001b[A\n",
      "Predicting labels with DeBERTa:  50%|     | 2/4 [00:03<00:03,  1.78s/it]\u001b[A\n",
      "Predicting labels with DeBERTa:  75%|  | 3/4 [00:04<00:01,  1.38s/it]\u001b[A\n",
      "Predicting labels with DeBERTa: 100%|| 4/4 [00:05<00:00,  1.41s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting labels for 123 test samples.\n",
      "\n",
      "Filtering training data based on globally predicted test labels...\n",
      "Total unique labels predicted across test set: 15\n",
      "Filtered Training Samples: 1061\n",
      "\n",
      "Initializing label encoder based on filtered training labels...\n",
      "Number of unique SBERT labels (post-filtering): 19\n",
      "\n",
      "Initializing SBERT model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3824d7014a4e81ab9870f3936bedf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0f1ceba9084b8aa21dc4bdb9215c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9044f909bf4bef95e7072073e3b8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4e99bd8f39463491ed1dc2d3fda7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9c544ada4d4b0d9907a3918301df8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260650b401b748df878df6bb49998cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13cd2cd6abd494190785e3f9d3c44ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2a3dc0e1554b48a1562fda005979b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718eecae8b7d4b2897da803732881756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539ec2de30384b47858b25f9e1f202f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b24d62146964cd1b02096084d4db6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained SBERT model from /kaggle/input/sbert-bug-similaroty/pytorch/default/1/best_sbert_model (1).pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3846907644.py:744: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(args.sbert_model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained SBERT model weights loaded (classifier may be reinitialized if size mismatched).\n",
      "Using 2 GPUs for SBERT!\n",
      "Skipping SBERT fine-tuning as a pre-trained model was loaded.\n",
      "\n",
      "Generating embeddings with final SBERT model...\n",
      "Using 2 GPUs for embedding generation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|| 4/4 [00:04<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for embedding generation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|| 34/34 [00:35<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 123 test embeddings.\n",
      "Generated 1061 filtered training embeddings.\n",
      "\n",
      "Calculating similarities and evaluating metrics...\n",
      "\n",
      "Calculating cosine similarities...\n",
      "Evaluating metrics for cosine similarity...\n",
      "\n",
      "Calculating euclidean similarities...\n",
      "Evaluating metrics for euclidean similarity...\n",
      "\n",
      "Calculating jaccard similarities...\n",
      "Evaluating metrics for jaccard similarity...\n",
      "\n",
      "Generating comparison plots...\n",
      "Saved comparison plot to ./results/run_20250429_123547_all_text_0.5/similarity_metrics_comparison_k1_filtered_train.png\n",
      "Saved comparison plot to ./results/run_20250429_123547_all_text_0.5/similarity_metrics_comparison_k3_filtered_train.png\n",
      "Saved comparison plot to ./results/run_20250429_123547_all_text_0.5/similarity_metrics_comparison_k5_filtered_train.png\n",
      "Saved comparison plot to ./results/run_20250429_123547_all_text_0.5/similarity_metrics_comparison_k10_filtered_train.png\n",
      "Saved comparison metrics to ./results/run_20250429_123547_all_text_0.5/similarity_metrics_comparison_filtered_train.json\n",
      "\n",
      "Processing results for cosine similarity...\n",
      "\n",
      "Cosine Similarity Metrics (using filtered train set):\n",
      "Match Rate (at least one match): 0.8618\n",
      "Average Precision@1: 0.7820\n",
      "Average Recall@1: 0.7774\n",
      "Average F1@1: 0.7547\n",
      "Average Precision@3: 0.6932\n",
      "Average Recall@3: 0.8458\n",
      "Average F1@3: 0.7248\n",
      "Average Precision@5: 0.6520\n",
      "Average Recall@5: 0.8745\n",
      "Average F1@5: 0.7041\n",
      "Average Precision@10: 0.5934\n",
      "Average Recall@10: 0.9213\n",
      "Average F1@10: 0.6723\n",
      "Average Label Overlap: 0.7780\n",
      "\n",
      "Processing results for euclidean similarity...\n",
      "\n",
      "Euclidean Similarity Metrics (using filtered train set):\n",
      "Match Rate (at least one match): 0.8374\n",
      "Average Precision@1: 0.7607\n",
      "Average Recall@1: 0.7631\n",
      "Average F1@1: 0.7373\n",
      "Average Precision@3: 0.6798\n",
      "Average Recall@3: 0.8447\n",
      "Average F1@3: 0.7176\n",
      "Average Precision@5: 0.6407\n",
      "Average Recall@5: 0.8718\n",
      "Average F1@5: 0.6976\n",
      "Average Precision@10: 0.5887\n",
      "Average Recall@10: 0.9131\n",
      "Average F1@10: 0.6693\n",
      "Average Label Overlap: 0.7734\n",
      "\n",
      "Processing results for jaccard similarity...\n",
      "\n",
      "Jaccard Similarity Metrics (using filtered train set):\n",
      "Match Rate (at least one match): 0.8455\n",
      "Average Precision@1: 0.7686\n",
      "Average Recall@1: 0.7585\n",
      "Average F1@1: 0.7375\n",
      "Average Precision@3: 0.6723\n",
      "Average Recall@3: 0.8580\n",
      "Average F1@3: 0.7216\n",
      "Average Precision@5: 0.6192\n",
      "Average Recall@5: 0.8871\n",
      "Average F1@5: 0.6876\n",
      "Average Precision@10: 0.5773\n",
      "Average Recall@10: 0.9233\n",
      "Average F1@10: 0.6643\n",
      "Average Label Overlap: 0.7733\n",
      "\n",
      "Analysis completed! Results saved to ./results/run_20250429_123547_all_text_0.5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Compare change requests using SBERT embeddings and similarity metrics, with optional DeBERTa label filtering.')\n",
    "    \n",
    "    parser.add_argument('--data_path', type=str, \n",
    "                        default=\"/kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json\",\n",
    "                        help='Path to the JSON data file')\n",
    "    parser.add_argument('--text_column', type=str, default='all_text_0.5',\n",
    "                        help='Column name with the text data to use for training')\n",
    "    parser.add_argument('--results_dir', type=str, default='./results',\n",
    "                        help='Directory to save results')\n",
    "    \n",
    "    parser.add_argument('--min_label_freq', type=int, default=5,\n",
    "                        help='Minimum frequency for a label to be considered')\n",
    "    parser.add_argument('--max_label_len', type=int, default=5,\n",
    "                        help='Maximum number of labels per sample')\n",
    "    \n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='Batch size for generating embeddings and training/prediction')\n",
    "    parser.add_argument('--training_epochs', type=int, default=20,\n",
    "                        help='Number of epochs for SBERT fine-tuning (0 to skip if not loading model)')\n",
    "    parser.add_argument('--top_k', type=int, default=5,\n",
    "                        help='Number of similar requests to find for each test sample')\n",
    "    \n",
    "    # SBERT Model Loading (Optional)\n",
    "    parser.add_argument('--sbert_model_path', type=str, default='/kaggle/input/sbert-bug-similaroty/pytorch/default/1/best_sbert_model (1).pt',\n",
    "                        help='Optional path to load a pre-trained/fine-tuned SBERT model state_dict (.pt file). If provided, SBERT fine-tuning is skipped.')\n",
    "\n",
    "    # DeBERTa Filtering Settings\n",
    "    parser.add_argument('--deberta_model_path', type=str,\n",
    "                        default=\"/kaggle/input/deberta-with-augmented-data-5-labels/pytorch/default/1/best_model_all_text.pt\",\n",
    "                        help='Path to the trained DeBERTa model state_dict (.pt file). Uses default if not specified.')\n",
    "    parser.add_argument('--deberta_label_encoder_path', type=str,\n",
    "                        default='/kaggle/input/deberta-with-augmented-data-5-labels/pytorch/default/1/label_encoder-2.json',\n",
    "                        help='Path to the DeBERTa label encoder (.json file). Uses default if not specified.')\n",
    "    parser.add_argument('--deberta_selected_labels_path', type=str,\n",
    "                        help='Optional path to the selected labels JSON file if feature selection was used for DeBERTa. Uses default if not specified.')\n",
    "    parser.add_argument('--deberta_threshold', type=float, default=0.5,\n",
    "                        help='Threshold for DeBERTa label prediction')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    results = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6834080,
     "sourceId": 10981401,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 313850,
     "modelInstanceId": 293215,
     "sourceId": 351367,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 314025,
     "modelInstanceId": 293390,
     "sourceId": 351527,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 315673,
     "modelInstanceId": 295063,
     "sourceId": 353688,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 322814,
     "modelInstanceId": 302319,
     "sourceId": 364356,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
