{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10981401,"sourceType":"datasetVersion","datasetId":6834080},{"sourceId":351527,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":293390,"modelId":314025},{"sourceId":376249,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":310848,"modelId":331216}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport argparse\nimport os\nimport json\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.optim import AdamW\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom scipy.spatial.distance import pdist, squareform\nfrom transformers import DebertaTokenizer, DebertaModel\nfrom torch import nn\n\nclass MultiLabelDataset(Dataset):\n    def __init__(self, texts, labels, model, label_encoder=None, max_length=512):\n        \"\"\"\n        Args:\n            texts: pandas Series or list of texts\n            labels: list of label lists\n            model: SBERT model\n            label_encoder: MultiLabelBinarizer instance (optional)\n            max_length: maximum sequence length\n        \"\"\"\n        # Convert texts to list if it's a pandas Series\n        self.texts = texts.tolist() if isinstance(texts, pd.Series) else texts\n        \n        # Initialize or use provided label encoder\n        if label_encoder is None:\n            self.label_encoder = MultiLabelBinarizer()\n            self.labels = self.label_encoder.fit_transform(labels)\n        else:\n            self.label_encoder = label_encoder\n            self.labels = self.label_encoder.transform(labels)\n        \n        self.model = model\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]  # Now a binary numpy array\n        \n        # Access tokenizer correctly, handling DataParallel wrapper\n        actual_model = self.model.module if isinstance(self.model, torch.nn.DataParallel) else self.model\n        tokenizer = actual_model._first_module().tokenizer\n\n        encoding = tokenizer(\n            text,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': torch.tensor(label, dtype=torch.float)\n        }\n\nclass FineTunedSBERT(SentenceTransformer):\n    def __init__(self, model_name='all-mpnet-base-v2', num_labels=20):\n        super().__init__(model_name)\n        for param in self.parameters():\n            param.requires_grad = False\n        \n        for layer in self._first_module().auto_model.encoder.layer[-3:]:\n            for param in layer.parameters():\n                param.requires_grad = True\n        \n        for param in self._first_module().auto_model.pooler.parameters():\n            param.requires_grad = True\n            \n        hidden_size = self._first_module().auto_model.config.hidden_size\n        self.classifier = torch.nn.Linear(hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self._first_module().auto_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        logits = self.classifier(pooled_output)\n        return logits\n\nclass DeBERTaClassifier(nn.Module):\n    \"\"\"\n    A classifier model based on DeBERTa for multi-label classification.\n    \n    This model uses a pre-trained DeBERTa model as the encoder and adds a \n    classification head on top with sigmoid activation for multi-label output.\n    \n    Args:\n        num_labels (int): Number of classes in the multi-label classification task.\n    \"\"\"\n    def __init__(self, num_labels):\n        super().__init__()\n        self.deberta = DebertaModel.from_pretrained('microsoft/deberta-base')\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(768, num_labels) # 768 is the hidden size for deberta-base\n        # Freeze all parameters in DeBERTa\n        for param in self.deberta.parameters():\n            param.requires_grad = False\n        # Unfreeze encoder parameters for fine-tuning\n        # Note: DeBERTa has a different architecture than BERT/RoBERTa\n        # We'll unfreeze the last 3 encoder layers\n        for layer in self.deberta.encoder.layer[-3:]:\n            for param in layer.parameters():\n                param.requires_grad = True\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        # Unlike BERT, DeBERTa doesn't have a pooler, so we need to take the last hidden state\n        # and either use the [CLS] token (first token) or do mean pooling\n        # Here we'll use the [CLS] token (first token) representation\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        cls_output = self.dropout(cls_output)\n        # Return raw logits for BCEWithLogitsLoss (sigmoid will be applied in the loss function)\n        return self.classifier(cls_output)\n\nclass DeBERTaIssueDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_len):\n        self.texts = texts.tolist() if isinstance(texts, pd.Series) else texts # Ensure texts is a list\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten()\n        }\n\ndef prepare_data(df, text_column='all_text', min_label_freq=0, max_label_len=100):\n    if text_column in df.columns:\n        df = df[[text_column, 'labels']]\n        df = df[~df[text_column].apply(lambda x: x.startswith('nan') if isinstance(x, str) else False)]\n    else:\n        raise ValueError(f\"Text column '{text_column}' not found in the DataFrame\")\n    \n    df = df.dropna()\n    texts = df[text_column]\n    labels = df['labels'].apply(lambda x: x if isinstance(x, list) else [])\n\n    label_distribution = Counter([label for labels in labels for label in labels])\n    frequent_labels = [label for label, count in label_distribution.items() if count >= min_label_freq]\n    \n    filtered_labels = labels.apply(lambda x: [label for label in x if label in frequent_labels])\n    \n    # Filter out samples that have no labels after frequency filtering\n    # This ensures we only keep samples with at least one retained label\n    empty_labels_mask = filtered_labels.apply(len) > 0\n    \n    # Apply the empty labels mask first\n    filtered_labels = filtered_labels[empty_labels_mask].reset_index(drop=True)\n    texts = texts[empty_labels_mask].reset_index(drop=True)\n    \n    # Then apply the label length filter\n    label_length = filtered_labels.apply(len)\n    length_mask = (label_length > 0) & (label_length <= max_label_len)\n    \n    texts = texts[length_mask].reset_index(drop=True)\n    filtered_labels = filtered_labels[length_mask].reset_index(drop=True)\n    \n    return texts, filtered_labels\n\ndef get_embeddings(texts, model, batch_size=32):\n    # Determine the actual model and device, handling DataParallel\n    actual_model = model.module if isinstance(model, torch.nn.DataParallel) else model\n    actual_model.eval() # Set the underlying model to evaluation mode\n    device = next(actual_model.parameters()).device\n\n    # Use the original model (potentially wrapped) for the forward pass if using multiple GPUs\n    # DataParallel handles the distribution automatically\n    inference_model = model if isinstance(model, torch.nn.DataParallel) else actual_model\n    if isinstance(inference_model, torch.nn.DataParallel):\n        print(f\"Using {torch.cuda.device_count()} GPUs for embedding generation!\")\n\n    embeddings = []\n\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n            batch_texts = texts[i:i + batch_size].tolist()\n            \n            # Use tokenizer from the actual_model\n            tokenizer = actual_model._first_module().tokenizer\n    \n            encoding = tokenizer(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=actual_model.max_seq_length, # Use actual model's max_seq_length\n                return_tensors='pt'\n            )\n            \n            input_ids = encoding['input_ids'].to(device)\n            attention_mask = encoding['attention_mask'].to(device)\n            \n            # Directly get embeddings from the base transformer model's output\n            # If using DataParallel (inference_model is wrapped), it will handle the multi-GPU forward pass\n            # We access the underlying module's auto_model to get the correct output structure\n            base_transformer = actual_model._first_module().auto_model\n            outputs = base_transformer(input_ids=input_ids, attention_mask=attention_mask)\n\n            pooled_output = outputs.pooler_output\n            \n            embeddings.append(pooled_output.cpu().numpy())\n    \n    return np.vstack(embeddings)\n\ndef calculate_similarities(test_embeddings, train_embeddings, similarity_metric='cosine'):\n    \"\"\"\n    Calculate similarities between test and train embeddings using different metrics.\n    \n    Args:\n        test_embeddings: numpy array of shape (n_test, embedding_dim)\n        train_embeddings: numpy array of shape (n_train, embedding_dim)\n        similarity_metric: one of 'cosine', 'euclidean', 'jaccard'\n        \n    Returns:\n        similarities: numpy array of shape (n_test, n_train)\n    \"\"\"\n    if similarity_metric == 'cosine':\n        return cosine_similarity(test_embeddings, train_embeddings)\n    \n    elif similarity_metric == 'euclidean':\n        # Convert distances to similarities (higher is more similar)\n        distances = euclidean_distances(test_embeddings, train_embeddings)\n        max_dist = np.max(distances)\n        if max_dist == 0:\n            return np.ones_like(distances)\n        return 1 - (distances / max_dist)\n    \n    elif similarity_metric == 'jaccard':\n        # For Jaccard, we need to binarize the embeddings\n        # We'll use the median of each dimension as a threshold\n        binary_test = test_embeddings > np.median(test_embeddings, axis=0)\n        binary_train = train_embeddings > np.median(train_embeddings, axis=0)\n        \n        similarities = np.zeros((len(test_embeddings), len(train_embeddings)))\n        \n        for i in range(len(test_embeddings)):\n            for j in range(len(train_embeddings)):\n                intersection = np.logical_and(binary_test[i], binary_train[j]).sum()\n                union = np.logical_or(binary_test[i], binary_train[j]).sum()\n                similarities[i, j] = intersection / union if union > 0 else 0\n                \n        return similarities\n    \n    else:\n        raise ValueError(f\"Unknown similarity metric: {similarity_metric}\")\n\ndef train_sbert_epoch(model, train_loader, criterion, optimizer, device, gradient_accumulation_steps=4):\n    model.train()\n    total_loss = 0\n    optimizer.zero_grad()\n    \n    for i, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss = loss / gradient_accumulation_steps\n        loss.backward()\n        \n        if (i + 1) % gradient_accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        total_loss += loss.item() * gradient_accumulation_steps\n        \n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    \n    if (i + 1) % gradient_accumulation_steps != 0:\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    return total_loss / len(train_loader)\n\ndef validate_sbert(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            \n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n    \n    return total_loss / len(val_loader)\n\ndef calculate_label_based_metrics(test_labels, train_labels, similar_indices, k_values=[1, 3, 5]):\n    \"\"\"\n    Calculate precision@k, recall@k, F1@k and other metrics based on label matching.\n    \n    Args:\n        test_labels: list of label lists\n        train_labels: list of label lists\n        similar_indices: list of arrays containing indices of similar items\n        k_values: list of k values for metrics\n    \"\"\"\n    metrics = {}\n    for k in k_values:\n        metrics.update({\n            f'precision@{k}': [],\n            f'recall@{k}': [],\n            f'f1@{k}': []\n        })\n    \n    metrics.update({\n        'avg_label_overlap': [],\n        'total_matches': 0,\n        'total_test_samples': len(test_labels)\n    })\n    \n    for i, test_label_set in enumerate(test_labels):\n        test_labels_set = set(test_label_set)\n        if not test_labels_set:  # Skip empty test label sets\n            continue\n            \n        retrieved_indices = similar_indices[i]\n        \n        for k in k_values:\n            # Consider only top-k results\n            top_k_indices = retrieved_indices[:k]\n            \n            # Calculate true positives across all top-k recommendations\n            retrieved_labels_set = set()\n            for idx in top_k_indices:\n                retrieved_labels_set.update(train_labels[idx])\n            \n            true_positives = len(test_labels_set.intersection(retrieved_labels_set))\n            \n            # Calculate metrics\n            precision = true_positives / len(retrieved_labels_set) if retrieved_labels_set else 0\n            recall = true_positives / len(test_labels_set) if test_labels_set else 0\n            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n            \n            metrics[f'precision@{k}'].append(precision)\n            metrics[f'recall@{k}'].append(recall)\n            metrics[f'f1@{k}'].append(f1)\n        \n        # Calculate label overlap for all top-k results\n        label_overlaps = []\n        for j, idx in enumerate(retrieved_indices):\n            train_labels_set = set(train_labels[idx])\n            if test_labels_set & train_labels_set:  # If there's an intersection\n                overlap = len(test_labels_set & train_labels_set) / len(test_labels_set | train_labels_set)\n                label_overlaps.append(overlap)\n                \n                # Count if there's at least one match\n                if j == 0:  # Only count once per test sample\n                    metrics['total_matches'] += 1\n        \n        avg_overlap = np.mean(label_overlaps) if label_overlaps else 0\n        metrics['avg_label_overlap'].append(avg_overlap)\n    \n    # Calculate averages\n    for k in k_values:\n        metrics[f'avg_precision@{k}'] = np.mean(metrics[f'precision@{k}'])\n        metrics[f'avg_recall@{k}'] = np.mean(metrics[f'recall@{k}'])\n        metrics[f'avg_f1@{k}'] = np.mean(metrics[f'f1@{k}'])\n    \n    metrics['avg_label_overlap'] = np.mean(metrics['avg_label_overlap'])\n    metrics['match_rate'] = metrics['total_matches'] / metrics['total_test_samples']\n    \n    return metrics\n\ndef create_stratification_labels(labels_list, min_samples_per_label=2):\n    \"\"\"\n    Create stratification labels that ensure each label has enough samples.\n    Only considers labels that appear frequently enough for stratification.\n    \"\"\"\n    # Count label occurrences\n    label_counts = Counter([label for labels in labels_list for label in labels])\n    \n    # Keep only labels that appear frequently enough\n    frequent_labels = {label for label, count in label_counts.items() if count >= min_samples_per_label}\n    \n    # Create binary indicators only for frequent labels\n    stratification_indicators = []\n    for labels in labels_list:\n        # Create indicator only for frequent labels\n        indicator = tuple(sorted(label for label in labels if label in frequent_labels))\n        # If no frequent labels, use a special category\n        if not indicator:\n            indicator = ('rare_combination',)\n        stratification_indicators.append(indicator)\n    \n    return stratification_indicators\n\ndef calculate_and_evaluate_similarity(test_embeddings, filtered_train_embeddings, test_labels, original_train_labels, original_index_map, k_values, run_dir):\n    \"\"\"\n    Calculates similarities between test and filtered train embeddings,\n    maps indices back to original, evaluates metrics, and saves plots.\n    \n    Args:\n        test_embeddings: numpy array (n_test, dim)\n        filtered_train_embeddings: numpy array (n_filtered_train, dim)\n        test_labels: list of lists (ground truth labels for test set)\n        original_train_labels: list of lists (ground truth labels for the *original* full training set)\n        original_index_map: dict mapping filtered train index -> original train index\n        k_values: list of k values for evaluation\n        run_dir: directory to save plots and results\n\n    Returns:\n        tuple: (results_dict, all_similar_indices_mapped, all_similarity_scores_mapped)\n            results_dict: Dictionary containing aggregated metrics for each similarity type.\n            all_similar_indices_mapped: Dict mapping metric -> list of lists of *original* train indices.\n            all_similarity_scores_mapped: Dict mapping metric -> list of lists of similarity scores.\n    \"\"\"\n    similarity_metrics = ['cosine', 'euclidean', 'jaccard']\n    results = {} # Store aggregated metrics per similarity type\n    all_similar_indices_mapped = {} # Store top-k original indices per metric\n    all_similarity_scores_mapped = {} # Store corresponding top-k scores per metric\n    max_k = max(k_values)\n\n    if filtered_train_embeddings.shape[0] == 0:\n        print(\"Warning: No filtered training embeddings to compare against. Skipping similarity calculation and evaluation.\")\n        # Return empty results\n        for metric in similarity_metrics:\n            results[metric] = {f'avg_precision@{k}': 0 for k in k_values}\n            results[metric].update({\n                f'avg_recall@{k}': 0 for k in k_values\n            })\n            results[metric].update({\n                f'avg_f1@{k}': 0 for k in k_values\n            })\n            results[metric]['match_rate'] = 0\n            results[metric]['avg_label_overlap'] = 0\n            all_similar_indices_mapped[metric] = [[] for _ in range(len(test_embeddings))]\n            all_similarity_scores_mapped[metric] = [[] for _ in range(len(test_embeddings))]\n        return results, all_similar_indices_mapped, all_similarity_scores_mapped\n\n    for metric in similarity_metrics:\n        print(f\"\\nCalculating {metric} similarities...\")\n        # Calculate full similarity matrix between test and the filtered training set\n        similarities = calculate_similarities(\n            test_embeddings, filtered_train_embeddings, metric\n        )\n        \n        # Get top-k indices and scores *within the filtered set*\n        # Argsort gives indices of the smallest values first, use [::-1] for descending order\n        num_filtered_train = filtered_train_embeddings.shape[0]\n        current_max_k = min(max_k, num_filtered_train)\n\n        # Get indices of top k scores for each test sample\n        top_k_indices_filtered = np.argsort(similarities, axis=1)[:, -current_max_k:][:, ::-1] \n        # Get the actual scores corresponding to these indices\n        top_k_scores = np.array([similarities[i, top_k_indices_filtered[i]] for i in range(len(test_embeddings))])\n\n        # Map filtered indices back to original training set indices\n        similar_original_indices_list = []\n        for filtered_indices_row in top_k_indices_filtered:\n            original_indices_row = [original_index_map[filt_idx] for filt_idx in filtered_indices_row]\n            similar_original_indices_list.append(original_indices_row)\n            \n        # Store mapped indices and scores for this metric\n        all_similar_indices_mapped[metric] = similar_original_indices_list\n        all_similarity_scores_mapped[metric] = top_k_scores.tolist() # Store as list\n\n        # Evaluate using original labels\n        print(f\"Evaluating metrics for {metric} similarity...\")\n        metrics = calculate_label_based_metrics(\n            test_labels, \n            original_train_labels, # Pass the original full set of train labels\n            similar_original_indices_list, # Pass the list of lists of mapped original indices\n            k_values\n        )\n\n        # Store aggregated metrics\n        results[metric] = metrics\n\n    # Create comparison plots (similar to before)\n    print(\"\\nGenerating comparison plots...\")\n    for k in k_values:\n        plt.figure(figsize=(12, 8))\n        metrics_to_plot = [f'avg_precision@{k}', f'avg_recall@{k}', f'avg_f1@{k}']\n        x = np.arange(len(metrics_to_plot))\n        width = 0.25\n\n        for i, metric in enumerate(similarity_metrics):\n            values = [results[metric].get(m, 0) for m in metrics_to_plot] # Use .get for safety if metric calculation failed\n            plt.bar(x + i*width, values, width, label=f'{metric.capitalize()} Similarity')\n\n        plt.ylabel('Score')\n        plt.title(f'Comparison of Similarity Metrics at k={k} (Filtered Train Set)')\n        plt.xticks(x + width, [m.split('@')[0].replace(\"avg_\", \"\").capitalize() for m in metrics_to_plot])\n        plt.legend()\n        plt.grid(True, axis='y')\n        plot_path = os.path.join(run_dir, f'similarity_metrics_comparison_k{k}_filtered_train.png')\n        plt.savefig(plot_path)\n        print(f\"Saved comparison plot to {plot_path}\")\n        plt.close()\n\n    # Save comparison results dictionary\n    comparison_results_path = os.path.join(run_dir, 'similarity_metrics_comparison_filtered_train.json')\n    with open(comparison_results_path, 'w') as f:\n        json.dump(results, f, indent=4)\n    print(f\"Saved comparison metrics to {comparison_results_path}\")\n\n    return results, all_similar_indices_mapped, all_similarity_scores_mapped\n\ndef predict_labels_with_deberta(texts, model, tokenizer, mlb_deberta, device, batch_size, max_length, threshold=0.5, test_labels=None, selected_labels_set=None):\n    \"\"\"\n    Predict multi-labels for given texts using a trained DeBERTa model.\n\n    Args:\n        texts (pd.Series): Series of input texts.\n        model (nn.Module): Trained DeBERTaClassifier model.\n        tokenizer (transformers.PreTrainedTokenizer): DeBERTa tokenizer.\n        mlb_deberta (MultiLabelBinarizer): Fitted MultiLabelBinarizer corresponding to the DeBERTa model's labels.\n        device: Device to perform inference on.\n        batch_size (int): Batch size for prediction.\n        max_length (int): Maximum token length for tokenizer.\n        threshold (float): Threshold for converting sigmoid outputs to binary labels.\n        test_labels (list, optional): List of test labels if available, used for feature selection filtering.\n        selected_labels_set (set, optional): Set of selected label names if feature selection is enabled.\n\n    Returns:\n        list[list[str]]: List of predicted label lists for each input text.\n        pd.Series: Filtered text samples (if feature selection was applied)\n        list: Filtered test labels (if feature selection was applied and test_labels was provided)\n    \"\"\"\n    # If we have feature selection enabled and test labels available, filter samples\n    filtered_texts = texts\n    filtered_test_labels = test_labels\n    filtered_indices = None\n    \n    if selected_labels_set and test_labels:\n        print(\"\\nApplying feature selection filtering to test samples before prediction...\")\n        # Only keep test samples that have at least one label in the selected labels set\n        filtered_indices = []\n        filtered_texts_list = []\n        filtered_test_labels_list = []\n        \n        for i, (text, labels) in enumerate(zip(texts, test_labels)):\n            # Check if any of the sample's labels are in the selected labels set\n            matching_labels = [label for label in labels if label in selected_labels_set]\n            if matching_labels:  # Keep only samples with at least one matching selected label\n                filtered_indices.append(i)\n                filtered_texts_list.append(text)\n                filtered_test_labels_list.append(matching_labels)  # Keep only the matching labels\n                \n        # Convert to pandas Series for consistency\n        if filtered_texts_list:\n            filtered_texts = pd.Series(filtered_texts_list)\n            filtered_test_labels = filtered_test_labels_list\n            print(f\"Kept {len(filtered_texts)} test samples out of {len(texts)} after feature selection filtering\")\n        else:\n            print(\"Warning: No test samples have any labels matching the feature selection!\")\n            # Return empty results if no samples remain\n            return [], filtered_texts, filtered_test_labels\n    \n    model.eval() # Set model to evaluation mode\n    dataset = DeBERTaIssueDataset(filtered_texts, tokenizer, max_length)\n    loader = DataLoader(dataset, batch_size=batch_size)\n\n    all_preds_binary = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Predicting labels with DeBERTa\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n\n            outputs = model(input_ids, attention_mask)\n            # Apply sigmoid and threshold\n            predictions_binary = (torch.sigmoid(outputs) >= threshold).cpu().numpy()\n            all_preds_binary.append(predictions_binary)\n\n    all_preds_binary = np.vstack(all_preds_binary)\n\n    # Convert binary predictions back to label names\n    predicted_labels = mlb_deberta.inverse_transform(all_preds_binary)\n    \n    # Convert tuples to lists for consistency\n    predicted_labels = [list(labels) for labels in predicted_labels]\n    \n    # If we filtered the test samples, but we need to return predictions in the original order,\n    # we'd need to map the predictions back to their original positions. This is optional.\n    \n    return predicted_labels, filtered_texts, filtered_test_labels\n\ndef main(args):\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n    \n    np.random.seed(42)\n    torch.manual_seed(42)\n    \n    results_dir = args.results_dir\n    os.makedirs(results_dir, exist_ok=True)\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_dir = os.path.join(results_dir, f\"run_{timestamp}_{args.text_column}\")\n    os.makedirs(run_dir, exist_ok=True)\n    \n    print(f\"Loading data from {args.data_path}...\")\n    df = pd.read_json(args.data_path)\n    \n    if args.text_column not in df.columns:\n        available_columns = [col for col in df.columns if col.startswith('all_text')]\n        print(f\"Text column '{args.text_column}' not found. Available text columns: {available_columns}\")\n        if len(available_columns) == 0:\n            raise ValueError(\"No text columns found in the data\")\n        args.text_column = available_columns[0]\n        print(f\"Using '{args.text_column}' instead\")\n    \n    texts, filtered_labels_all = prepare_data(\n        df, \n        text_column=args.text_column,\n        min_label_freq=args.min_label_freq, \n        max_label_len=args.max_label_len\n    )\n    \n    # Create stratification indicators *before* split\n    print(\"\\nPreparing stratified split...\")\n    stratification_indicators = create_stratification_labels(filtered_labels_all)\n    \n    try:\n        # Try stratified split and convert to lists\n        train_indices, test_indices = train_test_split(\n            range(len(texts)),\n            test_size=0.1,\n            random_state=42,\n            stratify=stratification_indicators\n        )\n        \n        # Use indices to split both texts and labels\n        original_train_texts = texts.iloc[train_indices].reset_index(drop=True) # Keep original training data\n        test_texts = texts.iloc[test_indices].reset_index(drop=True)\n        original_train_labels = [filtered_labels_all[i] for i in train_indices] # Keep original training labels\n        test_labels = [filtered_labels_all[i] for i in test_indices]\n        \n        print(\"Successfully performed stratified split\")\n    except ValueError as e:\n        print(f\"Warning: Could not perform stratified split ({str(e)})\")\n        print(\"Falling back to random split\")\n        \n        # Random split with indices\n        train_indices, test_indices = train_test_split(\n            range(len(texts)),\n            test_size=0.1,\n            random_state=42\n        )\n        \n        # Use indices to split both texts and labels\n        original_train_texts = texts.iloc[train_indices].reset_index(drop=True) # Keep original training data\n        test_texts = texts.iloc[test_indices].reset_index(drop=True)\n        original_train_labels = [filtered_labels_all[i] for i in train_indices] # Keep original training labels\n        test_labels = [filtered_labels_all[i] for i in test_indices]\n\n    print(f\"\\nOriginal Training Samples: {len(original_train_texts)}\")\n    print(f\"Test Samples: {len(test_texts)}\")\n\n    # --- DeBERTa Prediction FIRST ---\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"\\nUsing device: {device}\")\n\n    print(\"\\nLoading DeBERTa model and resources for prediction...\")\n    deberta_tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n\n    with open(args.deberta_label_encoder_path, 'r') as f:\n        encoder_data = json.load(f)\n    mlb_deberta = MultiLabelBinarizer()\n    mlb_deberta.classes_ = np.array(encoder_data['classes'])\n    num_deberta_labels = len(mlb_deberta.classes_)\n\n    selected_deberta_labels = None\n    if args.deberta_selected_labels_path:\n        print(f\"Loading selected labels from {args.deberta_selected_labels_path}\")\n        with open(args.deberta_selected_labels_path, 'r') as f:\n            selected_data = json.load(f)\n        selected_deberta_labels = selected_data['selected_labels']\n        mlb_deberta.classes_ = np.array([lbl for lbl in selected_deberta_labels if lbl in mlb_deberta.classes_])\n        num_deberta_labels = len(mlb_deberta.classes_)\n        print(f\"Using {num_deberta_labels} selected labels for DeBERTa prediction.\")\n    else:\n        print(f\"Using all {num_deberta_labels} labels from the DeBERTa encoder.\")\n\n    deberta_model = DeBERTaClassifier(num_labels=num_deberta_labels).to(device)\n    deberta_model.load_state_dict(torch.load(args.deberta_model_path, map_location=device))\n    deberta_model.eval() \n\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs for DeBERTa prediction!\")\n        deberta_model = torch.nn.DataParallel(deberta_model)\n\n    # Create a set of selected labels for faster lookup\n    selected_labels_set = None\n    if selected_deberta_labels:\n        selected_labels_set = set(selected_deberta_labels)\n        print(f\"Creating selected labels set for feature selection filtering with {len(selected_labels_set)} labels\")\n    \n    print(\"\\nPredicting labels for the test set using DeBERTa...\")\n    predicted_test_labels, filtered_test_texts, filtered_test_labels = predict_labels_with_deberta(\n        texts=test_texts, \n        model=deberta_model,\n        tokenizer=deberta_tokenizer,\n        mlb_deberta=mlb_deberta,\n        device=device,\n        batch_size=args.batch_size,\n        max_length=512,\n        threshold=args.deberta_threshold,\n        test_labels=test_labels,\n        selected_labels_set=selected_labels_set\n    )\n    \n    # Update test_texts and test_labels if they were filtered during prediction\n    if filtered_test_texts is not None and len(filtered_test_texts) != len(test_texts):\n        test_texts = filtered_test_texts\n        test_labels = filtered_test_labels\n        print(f\"Using filtered test set with {len(test_texts)} samples after feature selection\")\n    \n    print(f\"Finished predicting labels for {len(predicted_test_labels)} test samples.\")\n    \n    # Cleanup DeBERTa model to free memory if possible\n    del deberta_model\n    del deberta_tokenizer\n    del mlb_deberta\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    # --- End DeBERTa Prediction ---\n\n    # --- Apply Feature Selection from DeBERTa model (if available) ---\n    # We already have selected_labels_set from earlier, so we'll reuse it\n    if selected_labels_set:\n        print(\"\\nApplying feature selection from DeBERTa model to training data...\")\n        print(f\"Number of selected labels from feature selection: {len(selected_labels_set)}\")\n        \n        # Filter original training data to only include samples with at least one selected label\n        fs_filtered_train_indices = []\n        fs_filtered_train_texts = []\n        fs_filtered_train_labels = []\n        \n        for i, (text, labels) in enumerate(zip(original_train_texts, original_train_labels)):\n            # Check if any of the sample's labels are in the selected labels set\n            matching_labels = [label for label in labels if label in selected_labels_set]\n            if matching_labels:  # Keep only samples with at least one matching selected label\n                fs_filtered_train_indices.append(i)\n                fs_filtered_train_texts.append(text)\n                fs_filtered_train_labels.append(matching_labels)  # Keep only the matching labels\n        \n        # Update the original training data to use the filtered set\n        original_train_texts = pd.Series(fs_filtered_train_texts)\n        original_train_labels = fs_filtered_train_labels\n        \n        # Calculate using filtered_test_texts if available, otherwise use test_texts\n        original_test_size = len(filtered_test_texts) if 'filtered_test_texts' in locals() else len(test_texts)\n        print(f\"After feature selection filtering: {len(original_train_texts)} samples retained out of the original {len(texts) - original_test_size}\")\n        print(f\"Removed {len(texts) - original_test_size - len(original_train_texts)} samples with no matching labels in feature selection\")\n        \n        if len(original_train_texts) == 0:\n            print(\"WARNING: No training samples retained after feature selection filtering. Cannot proceed.\")\n            return {}\n    \n    # --- Filter Training Data based on ALL Predicted Test Labels ---\n    print(\"\\nFiltering training data based on globally predicted test labels...\")\n    all_predicted_labels_set = set(label for labels in predicted_test_labels for label in labels)\n    # If we have feature selection, intersect with selected labels\n    if selected_labels_set:\n        all_predicted_labels_set = all_predicted_labels_set.intersection(selected_labels_set)\n        print(f\"Total unique labels predicted across test set (filtered by feature selection): {len(all_predicted_labels_set)}\")\n    else:\n        print(f\"Total unique labels predicted across test set: {len(all_predicted_labels_set)}\")\n\n    filtered_train_indices_original = [] # Store original indices\n    filtered_train_texts_list = []\n    filtered_train_labels_list = []\n\n    # Filter training samples to keep only those with at least one label matching the predicted test labels\n    retained_labels_count = 0\n    for i, (text, labels) in enumerate(zip(original_train_texts, original_train_labels)):\n        # First filter the labels to keep only those in all_predicted_labels_set\n        filtered_sample_labels = [label for label in labels if label in all_predicted_labels_set]\n        \n        # Only keep samples that have at least one label after filtering\n        if len(filtered_sample_labels) > 0:\n            filtered_train_indices_original.append(i) # Store original index\n            filtered_train_texts_list.append(text)\n            filtered_train_labels_list.append(filtered_sample_labels)  # Store only the filtered labels\n            retained_labels_count += len(filtered_sample_labels)\n\n    # Create filtered Series/lists\n    filtered_train_texts = pd.Series(filtered_train_texts_list)\n    filtered_train_labels = filtered_train_labels_list # Already a list\n    \n    print(f\"After filtering, retained {retained_labels_count} label instances that match the predicted test labels\")\n    \n    # Map from filtered index (0 to N-1) back to original index\n    original_index_map = {filtered_idx: original_idx for filtered_idx, original_idx in enumerate(filtered_train_indices_original)}\n\n    print(f\"Filtered Training Samples: {len(filtered_train_texts)} (out of {len(original_train_texts)} samples after feature selection)\")\n    if len(filtered_train_texts) == 0:\n        print(\"Warning: Filtering resulted in zero training samples. Similarity search will be skipped.\")\n        # Optionally exit or handle this case appropriately\n        return {} \n    # --- End Training Data Filtering ---\n\n    # --- Initialize and Prepare SBERT Model ---\n    # Initialize label encoder *only* based on the filtered training labels now\n    print(\"\\nInitializing label encoder based on filtered training labels...\")\n    mlb_sbert = MultiLabelBinarizer()\n    mlb_sbert.fit(filtered_train_labels) # Fit only on labels that remain\n    print(f\"Number of unique SBERT labels (post-filtering): {len(mlb_sbert.classes_)}\")\n    \n    print(\"\\nInitializing SBERT model...\")\n    model = FineTunedSBERT('all-mpnet-base-v2', num_labels=len(mlb_sbert.classes_)) # Use count from filtered labels\n    model.max_seq_length = 512\n    model.use_fast_tokenizer = True\n    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Device already set\n\n    sbert_fine_tuned = False\n    if args.sbert_model_path:\n        if os.path.exists(args.sbert_model_path):\n            print(f\"Loading pre-trained SBERT model from {args.sbert_model_path}...\")\n            try:\n                # Adjust loading if the number of labels changed significantly due to filtering\n                # This might require careful handling depending on how the model was saved\n                # Option 1: Load weights except for the classifier head if sizes mismatch\n                pretrained_dict = torch.load(args.sbert_model_path, map_location='cpu')\n                model_dict = model.state_dict()\n                \n                # Filter out unnecessary keys and potentially size-mismatched classifier\n                pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].size() == v.size()}\n                \n                model_dict.update(pretrained_dict)\n                model.load_state_dict(model_dict, strict=False) # Use strict=False to ignore non-matching keys/sizes\n                print(\"Pre-trained SBERT model weights loaded (classifier may be reinitialized if size mismatched).\")\n            except Exception as e:\n                 print(f\"Error loading SBERT model: {e}. Check compatibility with filtered label set.\")\n                 print(\"Proceeding with base SBERT model.\")\n                 model = FineTunedSBERT('all-mpnet-base-v2', num_labels=len(mlb_sbert.classes_)) # Re-initialize if loading failed\n\n            model.eval() \n            sbert_fine_tuned = True \n        else:\n            print(f\"Warning: SBERT model path {args.sbert_model_path} not found. Proceeding without loading.\")\n    \n    model = model.to(device)\n\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs for SBERT!\")\n        model = torch.nn.DataParallel(model)\n    # --- End SBERT Model Prep ---\n\n    # --- Optional SBERT Fine-tuning (on FILTERED data) ---\n    best_loss = float('inf')\n    train_losses = []\n    test_losses = [] # Note: Test set isn't filtered, this validation might be less meaningful now\n    if args.training_epochs > 0 and not args.sbert_model_path:\n        # Create datasets using filtered training data and original test data\n        # The SBERT MultiLabelDataset needs the SBERT model instance passed to it\n        train_dataset = MultiLabelDataset(filtered_train_texts, filtered_train_labels, model, label_encoder=mlb_sbert)\n        # For validation, use the original test set but with the SBERT label encoder\n        # This evaluates how well the model generalizes to labels *after* being trained only on the filtered set\n        test_dataset = MultiLabelDataset(test_texts, test_labels, model, label_encoder=mlb_sbert) \n\n        effective_batch_size = args.batch_size // 4 if args.batch_size >= 4 else 1\n        gradient_accumulation_steps = 4 if args.batch_size >= 4 else 1\n        \n        train_loader = DataLoader(train_dataset, batch_size=effective_batch_size, shuffle=True, num_workers=0)\n        test_loader = DataLoader(test_dataset, batch_size=effective_batch_size, num_workers=0)\n        \n        criterion = BCEWithLogitsLoss()\n        # Ensure optimizer uses parameters from the potentially wrapped model\n        optimizer_params = model.module.parameters() if isinstance(model, torch.nn.DataParallel) else model.parameters()\n        optimizer = AdamW(optimizer_params, lr=2e-5)\n        \n        print(f\"\\nStarting SBERT fine-tuning on FILTERED data for {args.training_epochs} epochs...\")\n        \n        for epoch in range(args.training_epochs):\n            print(f\"\\nEpoch {epoch+1}/{args.training_epochs}\")\n            train_loss = train_sbert_epoch(model, train_loader, criterion, optimizer, device, gradient_accumulation_steps)\n            test_loss = validate_sbert(model, test_loader, criterion, device) # Validate on original test set\n            \n            train_losses.append(train_loss)\n            test_losses.append(test_loss)\n            \n            print(f\"Filtered Train Loss: {train_loss:.4f}\")\n            print(f\"Original Test Loss: {test_loss:.4f}\")\n            \n            if test_loss < best_loss:\n                best_loss = test_loss\n                model_to_save = model.module if isinstance(model, torch.nn.DataParallel) else model\n                torch.save(model_to_save.state_dict(), os.path.join(run_dir, 'best_sbert_model_filtered_train.pt'))\n                print(\"Saved new best model (trained on filtered data)\")\n            \n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        \n        sbert_fine_tuned = True \n\n        plt.figure(figsize=(10, 5))\n        plt.plot(train_losses, label='Filtered Train Loss')\n        plt.plot(test_losses, label='Original Test Loss')\n        plt.title('SBERT Training (Filtered Train) and Test Losses')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        plot_path = os.path.join(run_dir, 'sbert_filtered_training_losses.png')\n        plt.savefig(plot_path)\n        print(f\"Saved SBERT training loss plot to {plot_path}\")\n        plt.close()\n        \n        best_model_path = os.path.join(run_dir, 'best_sbert_model_filtered_train.pt')\n        if os.path.exists(best_model_path):\n            print(f\"Loading best SBERT model (trained on filtered data) from {best_model_path}\")\n            base_model = model.module if isinstance(model, torch.nn.DataParallel) else model\n            base_model.load_state_dict(torch.load(best_model_path, map_location=device))\n        else:\n            print(\"Warning: Best model file not found after training. Using the final state.\")\n        model.eval() \n    elif args.sbert_model_path:\n        print(\"Skipping SBERT fine-tuning as a pre-trained model was loaded.\")\n    else: \n        print(\"Skipping SBERT fine-tuning (training_epochs=0 and no pre-trained model path provided). Using base/loaded SBERT model.\")\n        model.eval()\n    # --- End SBERT Fine-tuning ---\n\n    # --- Generate Embeddings (Test and Filtered Train) ---\n    print(\"\\nGenerating embeddings with final SBERT model...\")\n    # Embed the original test texts\n    test_embeddings = get_embeddings(test_texts, model, batch_size=args.batch_size)\n    # Embed ONLY the filtered training texts\n    filtered_train_embeddings = get_embeddings(filtered_train_texts, model, batch_size=args.batch_size)\n    print(f\"Generated {test_embeddings.shape[0]} test embeddings.\")\n    print(f\"Generated {filtered_train_embeddings.shape[0]} filtered training embeddings.\")\n    # --- End Embedding Generation ---\n\n    # --- Compare Similarity Metrics (using filtered embeddings) ---\n    print(\"\\nCalculating similarities and evaluating metrics...\")\n    k_values = [1, 3, 5, 10] \n    \n    # Call the modified comparison function (needs implementation below)\n    # Pass original_train_labels and the original_index_map\n    similarity_comparison_results, all_similar_indices_mapped, all_similarity_scores_mapped = calculate_and_evaluate_similarity(\n        test_embeddings, filtered_train_embeddings, \n        test_labels, original_train_labels, # Pass ORIGINAL train labels for evaluation\n        original_index_map, # Pass the map\n        k_values, run_dir\n    )\n\n    # --- Save Detailed Results ---\n    all_similarity_details = {}\n    similarity_metrics = ['cosine', 'euclidean', 'jaccard'] \n\n    for metric in similarity_metrics:\n        print(f\"\\nProcessing results for {metric} similarity...\")\n        # Retrieve pre-calculated *mapped* indices and scores\n        # Note: calculate_and_evaluate_similarity should return indices mapped back to original\n        similar_original_indices = all_similar_indices_mapped[metric] \n        similarity_scores = all_similarity_scores_mapped[metric] # These scores correspond to the mapped indices\n\n        # Metrics are already calculated in calculate_and_evaluate_similarity\n        label_metrics = similarity_comparison_results[metric]\n\n        # Print metrics \n        print(f\"\\n{metric.capitalize()} Similarity Metrics (using filtered train set):\")\n        print(f\"Match Rate (at least one match): {label_metrics['match_rate']:.4f}\")\n        for k in k_values:\n            print(f\"Average Precision@{k}: {label_metrics[f'avg_precision@{k}']:.4f}\")\n            print(f\"Average Recall@{k}: {label_metrics[f'avg_recall@{k}']:.4f}\")\n            print(f\"Average F1@{k}: {label_metrics[f'avg_f1@{k}']:.4f}\")\n        print(f\"Average Label Overlap: {label_metrics['avg_label_overlap']:.4f}\")\n\n        # Generate and save detailed similarity results per test sample\n        metric_similarity_results = []\n        # Use args.top_k for the detailed results file length\n        for i, (original_indices_for_test, scores_for_test) in enumerate(zip(similar_original_indices, similarity_scores)):\n            test_sample = {\n                'test_text': test_texts.iloc[i],\n                'test_labels': test_labels[i],\n                'predicted_test_labels': predicted_test_labels[i], # Add predicted labels for context\n                'similar_requests': []\n            }\n            test_labels_set = set(test_labels[i])\n            \n            # Ensure we don't try to access more items than available\n            num_retrieved = min(args.top_k, len(original_indices_for_test))\n\n            for j in range(num_retrieved):\n                original_idx = original_indices_for_test[j]\n                score = scores_for_test[j]\n                \n                # Use original_idx to fetch from original training data\n                train_text = original_train_texts.iloc[original_idx]\n                train_label_list = original_train_labels[original_idx] \n                train_labels_set = set(train_label_list)\n                \n                matching_labels = list(test_labels_set & train_labels_set)\n                similar_request = {\n                    'rank': j + 1,\n                    'original_train_index': int(original_idx), # Store original index\n                    'text': train_text,\n                    'labels': train_label_list,\n                    'similarity_score': float(score),\n                    'matching_labels': matching_labels,\n                    'has_matching_label': len(matching_labels) > 0\n                }\n                test_sample['similar_requests'].append(similar_request)\n            metric_similarity_results.append(test_sample)\n\n        all_similarity_details[metric] = metric_similarity_results\n        with open(os.path.join(run_dir, f'{metric}_similarity_results_filtered_train.json'), 'w') as f:\n            json.dump(metric_similarity_results, f, indent=4)\n\n    # Prepare combined results dictionary\n    results = {\n        'text_column': args.text_column,\n        'filtering_info': {\n            'total_unique_predicted_labels': len(all_predicted_labels_set),\n            'selected_labels_from_fs': len(selected_labels_set) if selected_labels_set else 0,\n            'original_test_samples': len(texts) - (len(texts) - len(test_texts)),\n            'filtered_test_samples': len(filtered_test_texts if 'filtered_test_texts' in locals() else test_texts),\n            'original_training_samples_before_fs': len(texts) - len(test_texts),\n            'original_training_samples_after_fs': len(original_train_texts),\n            'filtered_training_samples': len(filtered_train_texts),\n        },\n        'similarity_comparison': similarity_comparison_results, # Metrics calculated by calculate_and_evaluate_similarity\n        'sbert_training_on_filtered': {\n            'performed': args.training_epochs > 0 and not args.sbert_model_path,\n            'train_losses': train_losses,\n            'test_losses': test_losses,\n            'best_loss': float(best_loss) if args.training_epochs > 0 and 'best_loss' in locals() and best_loss != float('inf') else None\n        },\n        'deberta_info': {\n            'model_path': args.deberta_model_path,\n            'label_encoder_path': args.deberta_label_encoder_path,\n            'selected_labels_path': args.deberta_selected_labels_path,\n            'prediction_threshold': args.deberta_threshold,\n            'num_predicted_labels_used': num_deberta_labels\n        }\n    }\n\n    with open(os.path.join(run_dir, 'all_metrics_results_filtered_train.json'), 'w') as f:\n        def convert_numpy(obj):\n            if isinstance(obj, np.integer): return int(obj)\n            elif isinstance(obj, np.floating): return float(obj)\n            elif isinstance(obj, np.ndarray): return obj.tolist()\n            elif isinstance(obj, dict): return {k: convert_numpy(v) for k, v in obj.items()}\n            elif isinstance(obj, list): return [convert_numpy(i) for i in obj]\n            return obj\n        results_serializable = convert_numpy(results)\n        json.dump(results_serializable, f, indent=4)\n\n    print(f\"\\nAnalysis completed! Results saved to {run_dir}\")\n\n    # Return relevant data if needed downstream (adjust as necessary)\n    return {\n        'model': model,\n        'test_embeddings': test_embeddings,\n        'filtered_train_embeddings': filtered_train_embeddings,\n        'original_index_map': original_index_map,\n        'all_similar_indices_mapped': all_similar_indices_mapped,\n        'all_similarity_scores_mapped': all_similarity_scores_mapped,\n        'similarity_details': all_similarity_details,\n        'results_dir': run_dir\n    }","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T10:19:15.098968Z","iopub.execute_input":"2025-05-06T10:19:15.099274Z","iopub.status.idle":"2025-05-06T10:20:05.857153Z","shell.execute_reply.started":"2025-05-06T10:19:15.099253Z","shell.execute_reply":"2025-05-06T10:20:05.856581Z"}},"outputs":[{"name":"stderr","text":"2025-05-06 10:19:45.465319: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746526785.947139      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746526786.077624      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Compare change requests using SBERT embeddings and similarity metrics, with optional DeBERTa label filtering.')\n    \n    parser.add_argument('--data_path', type=str, \n                        default=\"/kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json\",\n                        help='Path to the JSON data file')\n    parser.add_argument('--text_column', type=str, default='all_text_0.5',\n                        help='Column name with the text data to use for training')\n    parser.add_argument('--results_dir', type=str, default='./results',\n                        help='Directory to save results')\n    \n    parser.add_argument('--min_label_freq', type=int, default=5,\n                        help='Minimum frequency for a label to be considered')\n    parser.add_argument('--max_label_len', type=int, default=5,\n                        help='Maximum number of labels per sample')\n    \n    parser.add_argument('--batch_size', type=int, default=32,\n                        help='Batch size for generating embeddings and training/prediction')\n    parser.add_argument('--training_epochs', type=int, default=0,\n                        help='Number of epochs for SBERT fine-tuning (0 to skip if not loading model)')\n    parser.add_argument('--top_k', type=int, default=10,\n                        help='Number of similar requests to find for each test sample')\n    \n    # SBERT Model Loading (Optional)\n    parser.add_argument('--sbert_model_path', type=str, default='/kaggle/input/sbert-bug-similaroty/pytorch/default/1/best_sbert_model (1).pt',\n                        help='Optional path to load a pre-trained/fine-tuned SBERT model state_dict (.pt file). If provided, SBERT fine-tuning is skipped.')\n\n    # DeBERTa Filtering Settings\n    parser.add_argument('--deberta_model_path', type=str,\n                        default='/kaggle/input/deberta-fs-10-labels-max-length-2/pytorch/default/1/best_model_all_text-2.pt',\n                        help='Path to the trained DeBERTa model state_dict (.pt file). Uses default if not specified.')\n    parser.add_argument('--deberta_label_encoder_path', type=str,\n                        default='/kaggle/input/deberta-fs-10-labels-max-length-2/pytorch/default/1/label_encoder.json',\n                        help='Path to the DeBERTa label encoder (.json file). Uses default if not specified.')\n    parser.add_argument('--deberta_selected_labels_path', type=str,\n                        default='/kaggle/input/deberta-fs-10-labels-max-length-2/pytorch/default/1/selected_labels.json',\n                        help='Optional path to the selected labels JSON file if feature selection was used for DeBERTa. Uses default if not specified.')\n    parser.add_argument('--deberta_threshold', type=float, default=0.5,\n                        help='Threshold for DeBERTa label prediction')\n\n    args, unknown = parser.parse_known_args()\n    results = main(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T10:20:05.858318Z","iopub.execute_input":"2025-05-06T10:20:05.858838Z","iopub.status.idle":"2025-05-06T10:21:05.512163Z","shell.execute_reply.started":"2025-05-06T10:20:05.858818Z","shell.execute_reply":"2025-05-06T10:21:05.511149Z"}},"outputs":[{"name":"stdout","text":"Loading data from /kaggle/input/kubernetes-final-bug-data-without-comments/cleaned_data_with_changed_files_no_comments.json...\n\nPreparing stratified split...\nWarning: Could not perform stratified split (setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1222,) + inhomogeneous part.)\nFalling back to random split\n\nOriginal Training Samples: 1099\nTest Samples: 123\n\nUsing device: cuda\n\nLoading DeBERTa model and resources for prediction...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8edea85daf68441ebc25d57c84dbb4d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72344aec7b3147569a4d6915051df19e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcc93addb99840b6ba25768c1e7d4e96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2786eecd59354f29810738371eea00e5"}},"metadata":{}},{"name":"stdout","text":"Loading selected labels from /kaggle/input/deberta-fs-10-labels-max-length-2/pytorch/default/1/selected_labels.json\nUsing 10 selected labels for DeBERTa prediction.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d2a26014bb1424d9674e45896937128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ce4257e16fa4f2da10b0371b4721cff"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/466160944.py:714: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  deberta_model.load_state_dict(torch.load(args.deberta_model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs for DeBERTa prediction!\nCreating selected labels set for feature selection filtering with 10 labels\n\nPredicting labels for the test set using DeBERTa...\n\nApplying feature selection filtering to test samples before prediction...\nKept 106 test samples out of 123 after feature selection filtering\n","output_type":"stream"},{"name":"stderr","text":"Predicting labels with DeBERTa: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]","output_type":"stream"},{"name":"stdout","text":"Using filtered test set with 106 samples after feature selection\nFinished predicting labels for 106 test samples.\n\nApplying feature selection from DeBERTa model to training data...\nNumber of selected labels from feature selection: 10\nAfter feature selection filtering: 839 samples retained out of the original 1116\nRemoved 277 samples with no matching labels in feature selection\n\nFiltering training data based on globally predicted test labels...\nTotal unique labels predicted across test set (filtered by feature selection): 10\nAfter filtering, retained 1068 label instances that match the predicted test labels\nFiltered Training Samples: 839 (out of 839 samples after feature selection)\n\nInitializing label encoder based on filtered training labels...\nNumber of unique SBERT labels (post-filtering): 10\n\nInitializing SBERT model...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01c1fcbdc3184b2c946113659ab02096"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af090eedf9d24afd99d1a01a61f1e5b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd44e6997de3474e89c36936d0a7febf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a7588cd622949a69fbfa7c2ecd18bff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e48bd5c04804457b8949d853ecf19f10"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a09187632b6e4ae8902ab92fb65f06d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f54612fca344240b1b15ebe29d093f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15c7174700d843c09b75d98d53372a5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce177ee5f6eb4021addaecac43e4a1a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6fab6f2b33f4967a8dea45bcb0c7239"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d7cee305c5540f186cd326164f30ed5"}},"metadata":{}},{"name":"stdout","text":"Loading pre-trained SBERT model from /kaggle/input/sbert-bug-similaroty/pytorch/default/1/best_sbert_model (1).pt...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/466160944.py:853: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  pretrained_dict = torch.load(args.sbert_model_path, map_location='cpu')\n","output_type":"stream"},{"name":"stdout","text":"Pre-trained SBERT model weights loaded (classifier may be reinitialized if size mismatched).\nUsing 2 GPUs for SBERT!\nSkipping SBERT fine-tuning as a pre-trained model was loaded.\n\nGenerating embeddings with final SBERT model...\nUsing 2 GPUs for embedding generation!\n","output_type":"stream"},{"name":"stderr","text":"Generating embeddings: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs for embedding generation!\n","output_type":"stream"},{"name":"stderr","text":"Generating embeddings: 100%|██████████| 27/27 [00:26<00:00,  1.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generated 106 test embeddings.\nGenerated 839 filtered training embeddings.\n\nCalculating similarities and evaluating metrics...\n\nCalculating cosine similarities...\nEvaluating metrics for cosine similarity...\n\nCalculating euclidean similarities...\nEvaluating metrics for euclidean similarity...\n\nCalculating jaccard similarities...\nEvaluating metrics for jaccard similarity...\n\nGenerating comparison plots...\nSaved comparison plot to ./results/run_20250506_102005_all_text_0.5/similarity_metrics_comparison_k1_filtered_train.png\nSaved comparison plot to ./results/run_20250506_102005_all_text_0.5/similarity_metrics_comparison_k3_filtered_train.png\nSaved comparison plot to ./results/run_20250506_102005_all_text_0.5/similarity_metrics_comparison_k5_filtered_train.png\nSaved comparison plot to ./results/run_20250506_102005_all_text_0.5/similarity_metrics_comparison_k10_filtered_train.png\nSaved comparison metrics to ./results/run_20250506_102005_all_text_0.5/similarity_metrics_comparison_filtered_train.json\n\nProcessing results for cosine similarity...\n\nCosine Similarity Metrics (using filtered train set):\nMatch Rate (at least one match): 0.8491\nAverage Precision@1: 0.8057\nAverage Recall@1: 0.8085\nAverage F1@1: 0.7931\nAverage Precision@3: 0.7932\nAverage Recall@3: 0.9288\nAverage F1@3: 0.8238\nAverage Precision@5: 0.7470\nAverage Recall@5: 0.9590\nAverage F1@5: 0.7995\nAverage Precision@10: 0.6750\nAverage Recall@10: 0.9708\nAverage F1@10: 0.7509\nAverage Label Overlap: 0.8805\n\nProcessing results for euclidean similarity...\n\nEuclidean Similarity Metrics (using filtered train set):\nMatch Rate (at least one match): 0.8396\nAverage Precision@1: 0.7962\nAverage Recall@1: 0.7967\nAverage F1@1: 0.7838\nAverage Precision@3: 0.7809\nAverage Recall@3: 0.8976\nAverage F1@3: 0.8092\nAverage Precision@5: 0.7401\nAverage Recall@5: 0.9354\nAverage F1@5: 0.7892\nAverage Precision@10: 0.6802\nAverage Recall@10: 0.9613\nAverage F1@10: 0.7530\nAverage Label Overlap: 0.8773\n\nProcessing results for jaccard similarity...\n\nJaccard Similarity Metrics (using filtered train set):\nMatch Rate (at least one match): 0.8396\nAverage Precision@1: 0.7910\nAverage Recall@1: 0.7825\nAverage F1@1: 0.7710\nAverage Precision@3: 0.7711\nAverage Recall@3: 0.9052\nAverage F1@3: 0.8033\nAverage Precision@5: 0.7328\nAverage Recall@5: 0.9264\nAverage F1@5: 0.7804\nAverage Precision@10: 0.6706\nAverage Recall@10: 0.9566\nAverage F1@10: 0.7430\nAverage Label Overlap: 0.8671\n\nAnalysis completed! Results saved to ./results/run_20250506_102005_all_text_0.5\n","output_type":"stream"}],"execution_count":2}]}